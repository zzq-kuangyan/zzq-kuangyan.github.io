{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[{"_id":"themes/maple/source/favicon.ico","path":"favicon.ico","modified":1,"renderable":1},{"_id":"themes/maple/source/images/avatar.jpg","path":"images/avatar.jpg","modified":1,"renderable":1},{"_id":"themes/maple/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/maple/source/css/highlight.styl","path":"css/highlight.styl","modified":1,"renderable":1},{"_id":"themes/maple/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/maple/source/lib/clipboard.min.js","path":"lib/clipboard.min.js","modified":1,"renderable":1},{"_id":"themes/maple/source/lib/iconify-icon.min.js","path":"lib/iconify-icon.min.js","modified":1,"renderable":1},{"_id":"themes/maple/source/lib/jquery.min.js","path":"lib/jquery.min.js","modified":1,"renderable":1},{"_id":"themes/maple/source/img/avatar.jpg","path":"img/avatar.jpg","modified":1,"renderable":1},{"_id":"themes/maple/source/js/main.js","path":"js/main.js","modified":1,"renderable":1},{"_id":"themes/maple/source/img/logo.svg","path":"img/logo.svg","modified":1,"renderable":1},{"_id":"themes/maple/source/lib/fancybox/fancybox.min.css","path":"lib/fancybox/fancybox.min.css","modified":1,"renderable":1},{"_id":"themes/maple/source/lib/fancybox/fancybox.umd.min.js","path":"lib/fancybox/fancybox.umd.min.js","modified":1,"renderable":1},{"_id":"themes/maple/source/lib/nprogress/nprogress.css","path":"lib/nprogress/nprogress.css","modified":1,"renderable":1},{"_id":"themes/maple/source/lib/nprogress/nprogress.js","path":"lib/nprogress/nprogress.js","modified":1,"renderable":1},{"_id":"themes/maple/source/lib/tocbot/tocbot.min.js","path":"lib/tocbot/tocbot.min.js","modified":1,"renderable":1},{"_id":"themes/maple/source/lib/tocbot/tocbot.min.css","path":"lib/tocbot/tocbot.min.css","modified":1,"renderable":1},{"_id":"source/img/1.png","path":"img/1.png","modified":1,"renderable":0},{"_id":"source/img/image-20230830173907526.png","path":"img/image-20230830173907526.png","modified":1,"renderable":0},{"_id":"source/img/2024-11-10-21.05.40.png","path":"img/2024-11-10-21.05.40.png","modified":1,"renderable":0},{"_id":"source/img/image-20240911233804427.png","path":"img/image-20240911233804427.png","modified":1,"renderable":0},{"_id":"source/img/image-20240911234004466.png","path":"img/image-20240911234004466.png","modified":1,"renderable":0},{"_id":"source/img/image-20240911234722046.png","path":"img/image-20240911234722046.png","modified":1,"renderable":0},{"_id":"source/img/image-20241005192256436.png","path":"img/image-20241005192256436.png","modified":1,"renderable":0},{"_id":"source/img/image-20241005193544038.png","path":"img/image-20241005193544038.png","modified":1,"renderable":0},{"_id":"source/img/image-20241006090215878.png","path":"img/image-20241006090215878.png","modified":1,"renderable":0},{"_id":"source/img/image-20241006123230093.png","path":"img/image-20241006123230093.png","modified":1,"renderable":0},{"_id":"source/img/image-20241006172059609.png","path":"img/image-20241006172059609.png","modified":1,"renderable":0},{"_id":"source/img/image-20241014020343184.png","path":"img/image-20241014020343184.png","modified":1,"renderable":0},{"_id":"source/img/image-20241110174645738.png","path":"img/image-20241110174645738.png","modified":1,"renderable":0},{"_id":"source/img/image-20250318002240060.png","path":"img/image-20250318002240060.png","modified":1,"renderable":0},{"_id":"source/img/image-20251012202236295.png","path":"img/image-20251012202236295.png","modified":1,"renderable":0},{"_id":"source/img/image-20251014212255259.png","path":"img/image-20251014212255259.png","modified":1,"renderable":0},{"_id":"source/img/image-20251014212502665.png","path":"img/image-20251014212502665.png","modified":1,"renderable":0},{"_id":"source/img/image-20251014213043601.png","path":"img/image-20251014213043601.png","modified":1,"renderable":0},{"_id":"source/img/image-20251014214735283.png","path":"img/image-20251014214735283.png","modified":1,"renderable":0},{"_id":"source/img/image-20251014213546574.png","path":"img/image-20251014213546574.png","modified":1,"renderable":0},{"_id":"source/img/image-20251014214810748.png","path":"img/image-20251014214810748.png","modified":1,"renderable":0},{"_id":"source/img/image-20251014215132441.png","path":"img/image-20251014215132441.png","modified":1,"renderable":0},{"_id":"source/img/image-20251014215956562.png","path":"img/image-20251014215956562.png","modified":1,"renderable":0},{"_id":"source/img/image-20251015144217715.png","path":"img/image-20251015144217715.png","modified":1,"renderable":0},{"_id":"source/img/image-20251015234446051.png","path":"img/image-20251015234446051.png","modified":1,"renderable":0},{"_id":"source/img/image-20251016210747455.png","path":"img/image-20251016210747455.png","modified":1,"renderable":0},{"_id":"source/img/截屏2024-11-10-21.26.40.png","path":"img/截屏2024-11-10-21.26.40.png","modified":1,"renderable":0},{"_id":"source/img/截屏2024-11-10-18.20.37.png","path":"img/截屏2024-11-10-18.20.37.png","modified":1,"renderable":0},{"_id":"source/img/截屏2024-11-10-21.30.24.png","path":"img/截屏2024-11-10-21.30.24.png","modified":1,"renderable":0},{"_id":"source/img/截屏2024-11-10-21.29.41.png","path":"img/截屏2024-11-10-21.29.41.png","modified":1,"renderable":0},{"_id":"source/img/截屏2024-11-10-21.41.09.png","path":"img/截屏2024-11-10-21.41.09.png","modified":1,"renderable":0}],"Cache":[{"_id":"themes/maple/.gitignore","hash":"67656f75e8113a4275ba5ef394b7662fab3ebea0","modified":1726055817407},{"_id":"themes/maple/.editorconfig","hash":"b16f01a7b04ad512e15ebb32c5786d432a536779","modified":1726055817403},{"_id":"themes/maple/README.md","hash":"1548880df88b582c523c0adec10b42805d3b3986","modified":1726055817427},{"_id":"themes/maple/_config.yml","hash":"b05144a2dfbe028697c319f1199899658e9a7181","modified":1726154552690},{"_id":"themes/maple/LICENSE","hash":"632b916dd7e4f5c11790ab808388cda6610210ed","modified":1726055817422},{"_id":"themes/maple/package.json","hash":"a400f031060d969c42cd4fdfa5b8d8df9a8766a0","modified":1726055817479},{"_id":"themes/maple/languages/default.yml","hash":"da38f00bb45a318f118db0d74df24a137351777e","modified":1726055817433},{"_id":"themes/maple/README.zh-CN.md","hash":"f5493199a39bcb94ab8ddc7d7f7fa044988e2240","modified":1726055817428},{"_id":"themes/maple/languages/de.yml","hash":"d45cea36c5c83d7d09afcd1c26fff4a4c513c25b","modified":1726055817431},{"_id":"themes/maple/languages/fr.yml","hash":"8cb0fe4b6913b4d5b662cdd0108a923c90025f85","modified":1726055817437},{"_id":"themes/maple/languages/ja.yml","hash":"3e2fedca096678c0c234ebffa4637828979296fa","modified":1726055817439},{"_id":"themes/maple/languages/es.yml","hash":"1e581098303181ed7927827d2388990f495cc873","modified":1726055817436},{"_id":"themes/maple/languages/en.yml","hash":"72066419d6682a017c97910921ade125f21b04cd","modified":1726055817435},{"_id":"themes/maple/languages/nl.yml","hash":"3d82ec703d0b3287739d7cb4750a715ae83bfcb3","modified":1726055817441},{"_id":"themes/maple/languages/no.yml","hash":"182bd9ea76313ec9dc769b5dd2845c0d1c56e3a0","modified":1726055817442},{"_id":"themes/maple/languages/ko.yml","hash":"11330316e3c1262474a2b496e40dbc29f93fe01b","modified":1726055817439},{"_id":"themes/maple/languages/ru.yml","hash":"50d08403ca204fba074f616b0d054a657f8d642f","modified":1726055817445},{"_id":"themes/maple/package-lock.json","hash":"adacc0bad40a537c1c68ce5654474d093871f55c","modified":1726055817478},{"_id":"themes/maple/languages/pt.yml","hash":"ae2c61b30e638f74f1a42c9ce39ac08d063b30f5","modified":1726055817444},{"_id":"themes/maple/languages/zh-TW.yml","hash":"d2cb9d5d8a4a80479547da0bb63e2feee0fffe47","modified":1726055817447},{"_id":"themes/maple/languages/zh-CN.yml","hash":"e41d1e0e3a9e15c30b7142491bed39dc50371e96","modified":1726055817446},{"_id":"themes/maple/scripts/echarts.js","hash":"2c5a1439a12b3c3ab3fd51b3572748a0ccd8667a","modified":1726055817481},{"_id":"themes/maple/layout/category.ejs","hash":"a2a5e2a2dbcec6adb32b890b9fbe52c9afa56845","modified":1726055817473},{"_id":"themes/maple/layout/archive.ejs","hash":"eaa27f97ddac9f3604d635fcfc6d624dc0ef5dcc","modified":1726055817472},{"_id":"themes/maple/layout/index.ejs","hash":"d4bd174f3b8136034df484fa26ec10eae7ad7432","modified":1726055817474},{"_id":"themes/maple/scripts/wordcount.js","hash":"d1c45c18bc63c144f02f3400c21b179534ae99ce","modified":1726055817483},{"_id":"themes/maple/layout/layout.ejs","hash":"83092233a75ce3d5a9b60b4b3776ce15effe7e72","modified":1726055817475},{"_id":"themes/maple/scripts/mermaid.js","hash":"778f3daf90edab9bd5c007cfb131e69e3e65709f","modified":1726055817482},{"_id":"themes/maple/layout/_partial/after-footer.ejs","hash":"0978f5a247ef2e6c17cb368015a5a90cdd1a32ea","modified":1726055817451},{"_id":"themes/maple/layout/post.ejs","hash":"67f2db7c08f3208cd87db2ec793898b29640f874","modified":1726055817476},{"_id":"themes/maple/layout/tag.ejs","hash":"e2dac7c1488f863735487dbe68f95b66c31fbde8","modified":1726055817477},{"_id":"themes/maple/layout/_plugins/baidu-analytics.ejs","hash":"5d651a50ab521334b3cc0bff34ce3de3e76ac750","modified":1726055817458},{"_id":"themes/maple/source/favicon.ico","hash":"8ede8a7e8edefbd78bb978d781a502f696c32be9","modified":1726055817509},{"_id":"themes/maple/layout/_partial/footer.ejs","hash":"d5a5fc05cc649584fc5cb24fa98e9fd510dc0f2e","modified":1726055817451},{"_id":"themes/maple/layout/_partial/paginator.ejs","hash":"7c64dd99baf13eb3c1b25435f3617635f351dbad","modified":1726055817455},{"_id":"themes/maple/layout/_partial/header.ejs","hash":"181adc8339d2e0df5b141f8abbdc76754be8fff9","modified":1726055817454},{"_id":"themes/maple/layout/_partial/toc.ejs","hash":"41a8f1ebc2d7e304439d656e4d99ec28caf2ede5","modified":1726055817456},{"_id":"themes/maple/layout/_partial/head.ejs","hash":"63e151a0e97f94559d4bf0cb8419a06055842c2f","modified":1726055817453},{"_id":"themes/maple/layout/_plugins/busuanzi.ejs","hash":"a6130b1c137b5a2af1cd2e9d6bf7eeabe780db0a","modified":1726055817459},{"_id":"themes/maple/layout/_plugins/google-analytics.ejs","hash":"a8a6e445d6016bfd8ae657d33c213ec828612184","modified":1726055817464},{"_id":"themes/maple/layout/_plugins/fancybox.ejs","hash":"4ef371acbfc1972e020b1f91ec2a17c738d2d42b","modified":1726055817462},{"_id":"themes/maple/layout/_plugins/giscus.ejs","hash":"d6f97ceb3ccfcedd61ac7890c5ca46f252f73802","modified":1726055817463},{"_id":"themes/maple/layout/_plugins/disqusjs.ejs","hash":"5f9b22b65eb828e3af178379c3d5be56d93a7aaa","modified":1726055817460},{"_id":"themes/maple/layout/_plugins/mermaid.ejs","hash":"804c36256f9232a1fe6f308e06ae89642fe0eb1b","modified":1726055817467},{"_id":"themes/maple/layout/_plugins/maple.ejs","hash":"68ce69e63ead68d7b6e742b114ced2dcd4aacd98","modified":1726055817465},{"_id":"themes/maple/layout/_plugins/nprogress.ejs","hash":"b9696030ae715421088dc0c62d2b705efe9c3505","modified":1726055817469},{"_id":"themes/maple/layout/_plugins/mathjax.ejs","hash":"c358356a7595b5e1fe1d65aee10616eec8eb00e7","modified":1726055817466},{"_id":"themes/maple/layout/_plugins/tocbot.ejs","hash":"e17e921d40c71405a37ee3c782b7584f8b339653","modified":1726055817471},{"_id":"themes/maple/layout/_plugins/tailwindcss.ejs","hash":"3bc366434255a72d0447c88af03abc1c34c128a7","modified":1726055817469},{"_id":"themes/maple/source/images/logo.svg","hash":"1b7a73d948e593dcec7549d63e5ac60ad6db6a8f","modified":1726055817518},{"_id":"themes/maple/layout/_plugins/theme.ejs","hash":"ce1aae063cfc761d9b4f1d3317dad83ea3bee531","modified":1726055817470},{"_id":"themes/maple/source/css/highlight.styl","hash":"83ae52cb1af7948443dead20cee63be9ac17acd3","modified":1726055817493},{"_id":"themes/maple/source/css/main.styl","hash":"8f94a8f6ffff1ebf9bbd72be9d34d996fff2aca4","modified":1726055817507},{"_id":"themes/maple/source/lib/clipboard.min.js","hash":"91d8fe48e42d7d985918d4f244f6e7e3cc5adc2d","modified":1726055817530},{"_id":"themes/maple/source/lib/iconify-icon.min.js","hash":"014e6de8308a10051823d39ba49834ff18e4dba7","modified":1726055817535},{"_id":"themes/maple/source/img/logo.svg","hash":"1de387a7b22ee87bd509ded9230d647b7ec599b3","modified":1726055817526},{"_id":"themes/maple/source/js/main.js","hash":"d71026032c622228b1390c2dde719c1baffa2693","modified":1726055817527},{"_id":"themes/maple/source/lib/fancybox/fancybox.min.css","hash":"faa1beb3cde9b3abf714bf1b8410eb71199e798d","modified":1726055817532},{"_id":"themes/maple/source/lib/nprogress/nprogress.js","hash":"e99cc7e1e3245a9d00882f9a370dd6dd105cb477","modified":1726055817539},{"_id":"themes/maple/source/lib/tocbot/tocbot.min.css","hash":"febfeedac017dce90bbbaece9d775a7ec9c7c37f","modified":1726055817542},{"_id":"themes/maple/source/lib/nprogress/nprogress.css","hash":"6737420c595d5b5609a70f76cfb641a63d522e43","modified":1726055817538},{"_id":"source/_drafts/test.md","hash":"5e2366d84f43370ad3d5ac34576f152d419a7bfd","modified":1726057508537},{"_id":"themes/maple/source/lib/tocbot/tocbot.min.js","hash":"30be41afb1f21ae821a143d74beab47094bc87a3","modified":1726055817543},{"_id":"source/_posts/.DS_Store","hash":"b1f24e7f3dc996a7a6869725e75e5b3839df32ef","modified":1731299905890},{"_id":"source/.DS_Store","hash":"de0270ea3f6f77a64ea89d4b19042dcb35bf6481","modified":1731299905889},{"_id":"source/_posts/.~程序 内存 进程 线程 浏览器和JavaScript和事件循环.md","hash":"f46a1f54beb908543ff6b25ab37beb6162c6d352","modified":1726096877837},{"_id":"source/img/.DS_Store","hash":"5c5bab4a0dee17c8d76dc0ef121ffb6bc9d58deb","modified":1731299905893},{"_id":"source/_posts/1环.md","hash":"c8141aaadd046fb0428e4a3c9523ebd38ecd3ec9","modified":1726097211118},{"_id":"source/img/image-20240911233804427.png","hash":"015d6bb31fc40514cd1075df87c9961a4279f495","modified":1726069084448},{"_id":"source/img/image-20241005192256436.png","hash":"30c6cedc44be41b6158aa989c4b544ae026f0ee5","modified":1728127376449},{"_id":"source/img/image-20251014212502665.png","hash":"118bae8072929ec74984aa6e8392c678d0a62d5c","modified":1760448302683},{"_id":"source/img/image-20251014214735283.png","hash":"f4f5a6a377dbce8416cc5c8bd5f79873759754f0","modified":1760449655307},{"_id":"source/img/image-20251014213043601.png","hash":"48a750a39ca3c9c1f822ee4d3ac8c11463642cb3","modified":1760448643623},{"_id":"source/img/image-20251014214810748.png","hash":"f9cf6889e0434324b499536cb90366be613b480c","modified":1760449690775},{"_id":"source/img/image-20251014213546574.png","hash":"6c8454f74cba07966d522d8b03ed38c7674be6d7","modified":1760448946594},{"_id":"source/img/image-20251014215132441.png","hash":"1b4471707755684e576ff1b5208c6d578bd94ec6","modified":1760449892454},{"_id":"source/img/image-20251014215956562.png","hash":"f6a9f147b72ff791df89bfd5b492fca3c552aacc","modified":1760450396572},{"_id":"source/img/截屏2024-11-10-21.30.24.png","hash":"83155e056a45b202629516221eda0c51cdd1c687","modified":1731299905905},{"_id":"source/category/index.md","hash":"557d4fff656877dd0c28fd8294675eb9a39d1700","modified":1726119254724},{"_id":"source/img/截屏2024-11-10-18.20.37.png","hash":"5e0ee143dd7b3517fa4e6916c764682e31372c32","modified":1731299905902},{"_id":"source/tag/index.md","hash":"78c69fcf49512478babfbda784d6e5626246a1a7","modified":1726119303615},{"_id":"source/_posts/BigData/关系型数据库设计规范.md","hash":"254ed015361b786cc90711942e31ca5f53e17567","modified":1731299905892},{"_id":"source/_posts/BigData/数据仓库-分层结构.md","hash":"0f91ca37083e361c321f9479ee9bd8c8ab684252","modified":1728128256160},{"_id":"source/_posts/BigData/.DS_Store","hash":"874840def052ccc0e68944a473cceb7e0368c8d0","modified":1731299905892},{"_id":"source/_posts/hack/新建 文本.md","hash":"09afdd11854ed0d07d57cc0be9b3d284d3fa53b3","modified":1728216818864},{"_id":"source/_posts/BigData/大数据之HDFS.md","hash":"54b6b3354bdf13751be460b548f75d672a2fdc12","modified":1730465600506},{"_id":"source/_posts/DeepLearning/DeepLearningDay1.md","hash":"4233457f861eba4b344bbd9b4c772e6d715ca2f1","modified":1760274039793},{"_id":"source/_posts/电商数仓6.0/优惠券维度表.md","hash":"da1b1828d03383063f14e393984320fe13962b42","modified":1728842854832},{"_id":"source/_posts/DeepLearning/DeepLearningDay2.md","hash":"9e47408b37b3562fabfb9b5cb8635fbd78e5c0e6","modified":1760510659929},{"_id":"source/_posts/DeepLearning/DeepLearningDay3_2.md","hash":"488128f00d8b8e5f02d2b1d715b16f7f06ef9689","modified":1760623989728},{"_id":"source/_posts/DeepLearning/DeepLearningDay3_1.md","hash":"7e959b733d2254f8a92aace0eba5ff1cda9ddd76","modified":1760624411982},{"_id":"source/_posts/BigData/Hive on Spark/新建 文本文档.md","hash":"1c57d1093e4ec7ffc99b07fbd38f0d88d4b206ee","modified":1728206470909},{"_id":"source/_posts/BigData/DataX/20240911.md","hash":"f5fce3fd38afc11985e75b910c4a1764ac1b506b","modified":1726122393531},{"_id":"themes/maple/source/lib/jquery.min.js","hash":"f694238d616f579a0690001f37984af430c19963","modified":1726055817537},{"_id":"themes/maple/source/lib/fancybox/fancybox.umd.min.js","hash":"91f17b14286c64506ed5e214d7e31209d34940c0","modified":1726055817534},{"_id":"source/img/2024-11-10-21.05.40.png","hash":"816a3a2cb9f5ba51db826724fc70702dc10b2f92","modified":1731299905894},{"_id":"source/img/image-20241014020343184.png","hash":"1b565b422fbf384b4175181c110737072b191dce","modified":1728842623305},{"_id":"source/img/image-20251014212255259.png","hash":"6b9cef82efc11ce1c5711853ccde65916196231d","modified":1760448175286},{"_id":"source/img/截屏2024-11-10-21.29.41.png","hash":"d155786cbdc7b2d6601607badd58de1e6c4053d9","modified":1731299905904},{"_id":"source/img/截屏2024-11-10-21.41.09.png","hash":"dc7042925dc80e96b36b835149040d15950c5763","modified":1731299905906},{"_id":"source/img/image-20240911234004466.png","hash":"1ad9ca5d474b06c2e180918db7ea590ac19cb954","modified":1726069204550},{"_id":"source/img/image-20251012202236295.png","hash":"8b210b4b574d06fbfa3e363727e99114932f95cf","modified":1760271756324},{"_id":"source/img/image-20251015144217715.png","hash":"b76ef5657018b34b38e03a4640feda0e554d9096","modified":1760510537754},{"_id":"source/img/image-20251015234446051.png","hash":"b0ad32a1f54b323f432e3d4d9b2032fd617813ca","modified":1760543086109},{"_id":"source/img/image-20251016210747455.png","hash":"a41b8815d6d9772a26872a7b77755250a49d6103","modified":1760620067490},{"_id":"source/img/截屏2024-11-10-21.26.40.png","hash":"889736b0f72b29351fa83c2f6d7bb08425171c99","modified":1731299905903},{"_id":"source/img/image-20241006123230093.png","hash":"f5ea508dfbb8fca5ab665f7b3a3fa7fd647a820e","modified":1728189150143},{"_id":"source/img/image-20240911234722046.png","hash":"83d07f44f0a1e68bd72795daecc7b9b9e7e78d41","modified":1726069642144},{"_id":"source/img/image-20241005193544038.png","hash":"3caef973e87bf036e4a52e9c12ba000436f5a9d5","modified":1728128144117},{"_id":"source/img/image-20241006172059609.png","hash":"3a97157a992e0edfb82740f58fef4c33a6804fd8","modified":1728206459691},{"_id":"source/img/image-20250318002240060.png","hash":"23132992d7d823a92c4f64138142a56f2072f424","modified":1742228560125},{"_id":"source/img/image-20230830173907526.png","hash":"3775c2657716bd9c0361b5954f415d8b6477ef79","modified":1758096919507},{"_id":"source/img/1.png","hash":"5556c012d36b1f21b38ef7e7f378a51474aecc10","modified":1726066161899},{"_id":"source/img/image-20241006090215878.png","hash":"2d80fb19e9c2a50c1dbf011967e6288ac3b07b89","modified":1728176536252},{"_id":"themes/maple/source/images/avatar.jpg","hash":"0eb2de59901a55c792898ddf5d1da3ed96f2e2d8","modified":1726055817517},{"_id":"themes/maple/source/img/avatar.jpg","hash":"0eb2de59901a55c792898ddf5d1da3ed96f2e2d8","modified":1726055817525},{"_id":"source/img/image-20241110174645738.png","hash":"947f6697d0e552f627485645a6708c867a45edfe","modified":1731299905900},{"_id":"public/tag/index.html","hash":"1594a65f44043627361985953c44c2506047e54c","modified":1760667312425},{"_id":"public/category/index.html","hash":"65c0eb37edf5aa46c7f1244d3992a6849643cd86","modified":1760667312425},{"_id":"public/2025/10/15/DeepLearning/DeepLearningDay3_1/index.html","hash":"4a009b53ee6df2ad24a90b7200a233ff976ed168","modified":1760667312425},{"_id":"public/2025/10/15/DeepLearning/DeepLearningDay3_2/index.html","hash":"06ddca8425f73fa6bc97d51d9b45c902e7a811db","modified":1760667312425},{"_id":"public/2025/10/13/DeepLearning/DeepLearningDay2/index.html","hash":"66b3c83dc32759e5493027aa6b26e13b3b08773b","modified":1760667312425},{"_id":"public/2025/10/12/DeepLearning/DeepLearningDay1/index.html","hash":"d7f6384c48ce835e8db6f4139624a66c392a5c6c","modified":1760667312425},{"_id":"public/2024/11/11/BigData/关系型数据库设计规范/index.html","hash":"20276c1df796158c522b5c4140ffdacadfd2e9bf","modified":1760667312425},{"_id":"public/2024/11/01/BigData/大数据之HDFS/index.html","hash":"b3900e82de5cb1176797c8f24b4c8ac49743464f","modified":1760667312425},{"_id":"public/2024/10/14/电商数仓6.0/优惠券维度表/index.html","hash":"05779fc5cd10fabaafef3199c9b0dc595db31d17","modified":1760667312425},{"_id":"public/2024/10/06/hack/新建 文本/index.html","hash":"b28c0cdbd2bb0624716f18b32dfe4131ca3eff3c","modified":1760667312425},{"_id":"public/2024/10/02/BigData/Hive on Spark/新建 文本文档/index.html","hash":"74f159f9f77f0eecb99440ee449b7dd6aba93cc9","modified":1760667312425},{"_id":"public/2024/09/16/BigData/数据仓库-分层结构/index.html","hash":"c828ac385e23665401c1f8754cc04b1f698681b9","modified":1760667312425},{"_id":"public/archives/index.html","hash":"ec0ea31abdcc69944bbb3867f92a16d6f908d537","modified":1760667312425},{"_id":"public/2024/09/12/1环/index.html","hash":"63b4345bdec63e0cc2e6a81833ec40f9be95843f","modified":1760667312425},{"_id":"public/2024/09/11/BigData/DataX/20240911/index.html","hash":"8f4ba27740e65a75cdb572e52387f2c88690f170","modified":1760667312425},{"_id":"public/archives/page/2/index.html","hash":"8ac0eff283343a45d918f9c6e5c34aaaf91d3590","modified":1760667312425},{"_id":"public/archives/2024/index.html","hash":"ea1b754b1d02a59fdc98a712700be197e5147356","modified":1760667312425},{"_id":"public/archives/2024/09/index.html","hash":"077c0050cbaf641609ffb3078abdb325b0093e30","modified":1760667312425},{"_id":"public/archives/2024/10/index.html","hash":"15bd0259c13e0d5751df1c6c3fb3264cf86b622b","modified":1760667312425},{"_id":"public/archives/2024/11/index.html","hash":"71c6ab0a26be112d9f47922927ee24656be6f115","modified":1760667312425},{"_id":"public/archives/2025/index.html","hash":"1d771772a53eb4a5d7165ab80c66e1f6804e4824","modified":1760667312425},{"_id":"public/categories/大数据/index.html","hash":"45928fbfc1133ccb580fd3bbe55066ed72a299b0","modified":1760667312425},{"_id":"public/archives/2025/10/index.html","hash":"a49ad2e8aa21aa878474f429836ed06d904e4f1d","modified":1760667312425},{"_id":"public/categories/DeepLearning/index.html","hash":"10f03de43bc1bd8bff5ca08126a2a7a81476047f","modified":1760667312425},{"_id":"public/categories/hack/index.html","hash":"5eab361253ace6d3c6e7c40491030311ef09cf0d","modified":1760667312425},{"_id":"public/categories/电商数仓6-0/index.html","hash":"0f31499547a3026195495ead654588e534916ca6","modified":1760667312425},{"_id":"public/categories/DataX/index.html","hash":"f82a34ab8c2ccee59a93855e0f3a54ac2a81b139","modified":1760667312425},{"_id":"public/categories/Hive-on-Spark/index.html","hash":"1217571ee7563380d74bbdd43173cf9c217cbb41","modified":1760667312425},{"_id":"public/tags/DataBase/index.html","hash":"e7cb0e5af153f4090edb40b1b09bc1b72cec7a11","modified":1760667312425},{"_id":"public/page/2/index.html","hash":"e0a489fd5c023280cd772bd9fe993f14edcf005d","modified":1760667312425},{"_id":"public/tags/大数据/index.html","hash":"4619f77c02a5688d98c664c7122c435bba4286c3","modified":1760667312425},{"_id":"public/index.html","hash":"5744c6c0bbb0efc9545a62d944b58bb201dd342c","modified":1760667312425},{"_id":"public/tags/HDFS/index.html","hash":"862aa049972ed7c3f7f49fa50293d5b06f351219","modified":1760667312425},{"_id":"public/tags/数据仓库/index.html","hash":"d8af917206dd9e3c2ecf9e3cc597db538492c457","modified":1760667312425},{"_id":"public/tags/hack/index.html","hash":"1467b48fcbbf5a81607062da7971822345eb69e1","modified":1760667312425},{"_id":"public/tags/apple-music/index.html","hash":"77f6836435174f2d7a8df734fbdf18aca9ddcc36","modified":1760667312425},{"_id":"public/tags/电商数仓6-0/index.html","hash":"8f1d27ebe809a08ade7d588d5da5084d760a71d2","modified":1760667312425},{"_id":"public/tags/DataX/index.html","hash":"f136f18bc65d5e2ba8467da6943080a3dc293983","modified":1760667312425},{"_id":"public/tags/spark/index.html","hash":"0ca14d6e39a1e87eaaf9080a3c82514b0c1b117a","modified":1760667312425},{"_id":"public/favicon.ico","hash":"8ede8a7e8edefbd78bb978d781a502f696c32be9","modified":1760667312425},{"_id":"public/images/logo.svg","hash":"1b7a73d948e593dcec7549d63e5ac60ad6db6a8f","modified":1760667312425},{"_id":"public/img/logo.svg","hash":"1de387a7b22ee87bd509ded9230d647b7ec599b3","modified":1760667312425},{"_id":"public/img/image-20240911233804427.png","hash":"015d6bb31fc40514cd1075df87c9961a4279f495","modified":1760667312425},{"_id":"public/img/image-20241005192256436.png","hash":"30c6cedc44be41b6158aa989c4b544ae026f0ee5","modified":1760667312425},{"_id":"public/css/highlight.css","hash":"58217c19313bd9b12b768a4fcd09e66a980fea2c","modified":1760667312425},{"_id":"public/css/main.css","hash":"5a1af8c15ba4385840892d78884e7811defe4380","modified":1760667312425},{"_id":"public/lib/clipboard.min.js","hash":"f48e9bfeca83e5057cc751e8c44fc07e9d976c06","modified":1760667312425},{"_id":"public/lib/jquery.min.js","hash":"69bb69e25ca7d5ef0935317584e6153f3fd9a88c","modified":1760667312425},{"_id":"public/lib/iconify-icon.min.js","hash":"7526cf2b54b9e657f377083129cc00c5aa4dc110","modified":1760667312425},{"_id":"public/js/main.js","hash":"c06a89f631ac37cc3acde2ce0979f6ebf6f6e6d6","modified":1760667312425},{"_id":"public/img/image-20251014214735283.png","hash":"f4f5a6a377dbce8416cc5c8bd5f79873759754f0","modified":1760667312425},{"_id":"public/img/image-20251014212502665.png","hash":"118bae8072929ec74984aa6e8392c678d0a62d5c","modified":1760667312425},{"_id":"public/img/image-20251014213043601.png","hash":"48a750a39ca3c9c1f822ee4d3ac8c11463642cb3","modified":1760667312425},{"_id":"public/img/image-20251014213546574.png","hash":"6c8454f74cba07966d522d8b03ed38c7674be6d7","modified":1760667312425},{"_id":"public/img/image-20251014215132441.png","hash":"1b4471707755684e576ff1b5208c6d578bd94ec6","modified":1760667312425},{"_id":"public/img/image-20251014214810748.png","hash":"f9cf6889e0434324b499536cb90366be613b480c","modified":1760667312425},{"_id":"public/img/image-20251014215956562.png","hash":"f6a9f147b72ff791df89bfd5b492fca3c552aacc","modified":1760667312425},{"_id":"public/img/截屏2024-11-10-18.20.37.png","hash":"5e0ee143dd7b3517fa4e6916c764682e31372c32","modified":1760667312425},{"_id":"public/img/截屏2024-11-10-21.30.24.png","hash":"83155e056a45b202629516221eda0c51cdd1c687","modified":1760667312425},{"_id":"public/img/2024-11-10-21.05.40.png","hash":"816a3a2cb9f5ba51db826724fc70702dc10b2f92","modified":1760667312425},{"_id":"public/lib/fancybox/fancybox.min.css","hash":"1564bb6a6b930a61875610c05001c4f7bfe9939a","modified":1760667312425},{"_id":"public/lib/nprogress/nprogress.css","hash":"d8a2825a6fb35a3eeb3743e09184c18be9dbfcdc","modified":1760667312425},{"_id":"public/lib/nprogress/nprogress.js","hash":"e328b676f7e93bd78cd5d31cd0899cba2a423097","modified":1760667312425},{"_id":"public/lib/fancybox/fancybox.umd.min.js","hash":"e766e468e4f017b51a643648f6b4f05187c41d6b","modified":1760667312425},{"_id":"public/lib/tocbot/tocbot.min.js","hash":"4f1b40a6818fe6e955f2ce7de3b79aec4dcd0a7c","modified":1760667312425},{"_id":"public/lib/tocbot/tocbot.min.css","hash":"0029d92ce7f1a59c7c57feb6d5f80649daba3db8","modified":1760667312425},{"_id":"public/img/image-20241014020343184.png","hash":"1b565b422fbf384b4175181c110737072b191dce","modified":1760667312425},{"_id":"public/img/image-20251014212255259.png","hash":"6b9cef82efc11ce1c5711853ccde65916196231d","modified":1760667312425},{"_id":"public/img/截屏2024-11-10-21.29.41.png","hash":"d155786cbdc7b2d6601607badd58de1e6c4053d9","modified":1760667312425},{"_id":"public/img/image-20240911234004466.png","hash":"1ad9ca5d474b06c2e180918db7ea590ac19cb954","modified":1760667312425},{"_id":"public/img/截屏2024-11-10-21.41.09.png","hash":"dc7042925dc80e96b36b835149040d15950c5763","modified":1760667312425},{"_id":"public/img/image-20251012202236295.png","hash":"8b210b4b574d06fbfa3e363727e99114932f95cf","modified":1760667312425},{"_id":"public/img/截屏2024-11-10-21.26.40.png","hash":"889736b0f72b29351fa83c2f6d7bb08425171c99","modified":1760667312425},{"_id":"public/img/image-20251015144217715.png","hash":"b76ef5657018b34b38e03a4640feda0e554d9096","modified":1760667312425},{"_id":"public/img/image-20251016210747455.png","hash":"a41b8815d6d9772a26872a7b77755250a49d6103","modified":1760667312425},{"_id":"public/img/image-20251015234446051.png","hash":"b0ad32a1f54b323f432e3d4d9b2032fd617813ca","modified":1760667312425},{"_id":"public/img/image-20241006123230093.png","hash":"f5ea508dfbb8fca5ab665f7b3a3fa7fd647a820e","modified":1760667312425},{"_id":"public/img/image-20240911234722046.png","hash":"83d07f44f0a1e68bd72795daecc7b9b9e7e78d41","modified":1760667312425},{"_id":"public/img/image-20241005193544038.png","hash":"3caef973e87bf036e4a52e9c12ba000436f5a9d5","modified":1760667312425},{"_id":"public/img/image-20241006172059609.png","hash":"3a97157a992e0edfb82740f58fef4c33a6804fd8","modified":1760667312425},{"_id":"public/img/image-20250318002240060.png","hash":"23132992d7d823a92c4f64138142a56f2072f424","modified":1760667312425},{"_id":"public/img/image-20230830173907526.png","hash":"3775c2657716bd9c0361b5954f415d8b6477ef79","modified":1760667312425},{"_id":"public/img/1.png","hash":"5556c012d36b1f21b38ef7e7f378a51474aecc10","modified":1760667312425},{"_id":"public/img/image-20241006090215878.png","hash":"2d80fb19e9c2a50c1dbf011967e6288ac3b07b89","modified":1760667312425},{"_id":"public/images/avatar.jpg","hash":"0eb2de59901a55c792898ddf5d1da3ed96f2e2d8","modified":1760667312425},{"_id":"public/img/avatar.jpg","hash":"0eb2de59901a55c792898ddf5d1da3ed96f2e2d8","modified":1760667312425},{"_id":"public/img/image-20241110174645738.png","hash":"947f6697d0e552f627485645a6708c867a45edfe","modified":1760667312425}],"Category":[{"name":"BigData","_id":"cmgu7uyea0008jkv2gxz83zml"},{"name":"DeepLearning","_id":"cmgu7uyef000ijkv21h31hmsd"},{"name":"hack","_id":"cmgu7uyef000ljkv2hn8mb5ky"},{"name":"电商数仓6.0","_id":"cmgu7uyel0016jkv2gl3bek43"},{"name":"DataX","_id":"cmgu7uyen001fjkv2g9hc9k0o"},{"name":"Hive on Spark","_id":"cmgu7uyeo001jjkv2ci2x8esp"}],"Data":[],"Page":[{"title":"category","type":"category","layout":"category","date":"2024-09-12T05:17:49.000Z","_content":"","source":"category/index.md","raw":"---\ntitle: category\ntype: \"category\"\nlayout: \"category\"\ndate: 2024-09-12 13:17:49\n---\n","updated":"2024-09-12T05:34:14.724Z","path":"category/index.html","comments":1,"_id":"cmgu7uydy0000jkv2bj5ed30d","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tag","type":"tag","layout":"tag","date":"2024-09-12T05:32:28.000Z","_content":"","source":"tag/index.md","raw":"---\ntitle: tag\ntype: \"tag\"\nlayout: \"tag\"\ndate: 2024-09-12 13:32:28\n---\n","updated":"2024-09-12T05:35:03.615Z","path":"tag/index.html","comments":1,"_id":"cmgu7uye50002jkv2hnf312vv","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"","author":"ZQ","date":"2024-09-11T12:20:00.000Z","_content":"","source":"_drafts/test.md","raw":"title: ''\nauthor: ZQ\ntags: []\ncategories: []\ndate: 2024-09-11 20:20:00\n---\n","slug":"test","published":0,"updated":"2024-09-11T12:25:08.537Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cmgu7uye30001jkv2bsg1he4q","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"程序 内存 进程 线程 浏览器和JavaScript和事件循环","_content":"\n\n\n### 进制数小知识点\n\n二进制 是 通过 10101010组成 \n\n八进制 通过 0 - 7 组成\n\n十进制 通过  0 - 9 组成 \n\n十六进制通过 0 - 9 a- f 组成\n\n### 什么是程序\n\n就是 app 和 电脑桌面上的 一些应用\n\n### 什么是内存\n\n这里的内存指的是 运行内存  不是 存储内存 cpu 会直接 读取 运行内存;\n\n为什么要有 运行内存 cpu 直接在 存储内存 上读写 不更好吗 而且 内存空间大?\n\n```bash\n如果cpu  直接读取 存储内存 非常慢 而且 存储内存的空间非常大\n```\n\n### 什么是进程\n\n什么是进程？；\n\n进程就是 我们 打开一个程序 内存空间 会给这个程序分配一个空间 运行进程\n\n### 什么是线程\n\n线程就是 在分配好的进程空间里面 由线程来运行这些 一个 程序 进程最少有一个线程(叫做主线程)\n（而浏览器是非常复杂的 浏览器 有多个进程） 浏览器最主要的三个进程 浏览器进程 网络进程 和 渲染进程\n\n### 程序 内存 进程 程序 线程  介绍\n\n 电脑桌面上对应的程序 当我们打开 程序的时候运行内存 会给 程序分配一片空间 叫 进程 每个 程序的进程空间 都是相互独立的 每个进程都最少有 一个 线程叫做主线程 来 运行这些代码\n\n### 浏览器进程\n\n 浏览器 有三个 主要的进程 浏览器进程 渲染进程 和 网络进程\n\n 而渲染主线程(渲染进程) 他 要 执行 css  html  js  所有的页面渲染\n\n### 渲染进程\n\n 渲染进程下面的 线程 有 GUI渲染线程 定时线程（定时器触发线程、）  和 交互线程（事件触发线程）\n\n### 如何理解js异步（面试题）\n\n（注意）不是浏览器线程  浏览器是 多线程的  但是 JavaScript（js） 是单线程语言\n\n```bash\nJS是一门单线程的语言，这是因为它运行在浏览器的渲染主线程中，而渲染主线程只有一个。果而渲染主线程承担着诸多的工作，渲染页面、执行 JS 都在其中运行。如果使用同步的方式，就极有可能导致主线程产生阻塞，从而导致消息队列中的很多其他任务无法得到执行这样一来，一方面会导致繁忙的主线程白白的消耗时间，另一方面导致页面无法及时更新，给用户造成卡死现象。\n所以浏览器采用异步的方式来避免。具体做法是当某些任务发生时，比如计时器、网络、事件监听，主线程将任务交给其他线程去处理，自身立即结束任务的执行，转而执行后续代码。当其他线程完成时，将事先传递的回调函数包装成任务，加入到消息队列的末尾排队，等待主线程调度执行。在这种异步模式下，浏览器永不阻塞，从而最大限度的保证了单线程的流畅运行。\n```\n\n### 任务有优先级吗\n\n任务没有优先级 在信息队列中先进先出\n在信息队列中 有优先级的\n\n根据W3C的最新解释\n\n 1. 每个任务都有一个任务类型 比如 微任务 定时任务 和 交互任务， 同一个类型的任务必须在同一个队列里面，不同的任务可以分属于不同的任务队列 在一次事件循环中，浏览器可以根据实际情况从不同的队列中取出执行\n 2. 浏览器必须准备一个 微任务队列，微任务队列中的任务优先其他任务执行\n\n 在目前 chrome 中 至少包含以下队列\n\n   1. 微任务队列（优先级最高）\n   2. 交互任务队列（优先级高）\n   3. 延时任务队列（优先级中）\n\n### 模型模拟（模拟）\n\n渲染主线程 [任务1, 任务2, 任务3]\n\n微任务队列 [排队1，排队2，]（优先级最高）\n\n延时任务队列 [排队1，排队2] (优先级中)\n\n交互任务 [排队1，排队2] (优先级高)\n\n### 阐述一下 JavaScript 的事件循环（面试题）\n\n```bash\n\n事件循环 又叫做消息循环 是浏览器主线程的工作方式 在Chrome的源码中 开启一个 不会结束的for循环，每次循环从信息队列中取出第一个任务执行 而其他线程只需要在合适的时候加入队列末尾即可\n\n根据 w3c官方解释：\n 每个任务都有一个任务类型 比如 微任务 定时任务 和 交互任务， 同一个类型的任务必须在同一个队列里面，不同的任务可以分属于不同的任务队列 在一次事件循环中，浏览器可以根据实际情况从不同的队列中取出执行\n\n```\n\n### js中的计时器中 能做到精确计时吗 为什么\n\n不能 因为:\n\n1. 在受 事件循环的影响，计时的回调函数 只能在主线程空闲时间运行，因此带来偏差\n2. 在w3c标准中，计时器 超过 五层 就会出现 4毫秒的最少时间，（简单解释）\n完整解释：按照W3C标准，浏览器计时时，如果嵌套层级超过五层，则会带来4毫秒的最少时间，这样在计时时间少于4毫秒时又带来偏差\n\n# 重新整理 浏览器进程和线程\n\n浏览器进程：浏览器有六个进程分别\n\n|进程名称 |数量 |进程介绍|\n|-------|-------|-------|\n|浏览器主进程(Browser)|1|主要负责进程管理，操作界面显示，用户的交互|\n|网络进程|1|负责网络资源加载|\n|渲染进程（浏览器内核Renderer进程内部是多线程）|N|负责将 HTML、CSS 和 JavaScript 转换成用户可以交互的操作界面。默认情况下，每个Tab都会被创建一个渲染进程。且渲染进程运行在沙箱模式下。|\n|GPU进程|1|负责3D效果、动画、图片等渲染。|\n|插件进程|N|负责插件的运行和隔离。隔离插件主要是防止出现单个插件崩溃导致页面或者浏览器崩溃的情况。|\n|缓存进程(Storage Service)|1| 负责提供浏览器存储的功能。他是从主进程中分离出来的服务|\n|音频进程(Audio Service)|1|主要处理 音频和视频|\n|数字解码进程(Data Decoder Service)|N|执行image，zip等文件的解码|\n\n重点是浏览器内核（渲染进程）\n\n1. `GUI进程`\n    负责渲染浏览器界面，解析HTML，CSS，构建DOM树和RenderObject树，布局和绘制等。\n    当界面需要重绘（Repaint）或由于某种操作引发回流(reflow)时，该线程就会执行\n    注意，GUI渲染线程与JS引擎线程是互斥的，当JS引擎执行时GUI线程会被挂起（相当于被冻结了），GUI更新会被保存在一个队列中等到JS引擎空闲时立即被执行。\n\n2. `JS引擎线程`\n   也称为JS内核，负责处理Javascript脚本程序。（例如V8引擎）\n   JS引擎线程负责解析Javascript脚本，运行代码。\n   JS引擎一直等待着任务队列中任务的到来，然后加以处理，一个Tab页（renderer进程）中无论什么时候都只有一个JS线程在运行JS程序\n   同样注意，GUI渲染线程与JS引擎线程是互斥的，所以如果JS执行的时间过长，这样就会造成页面的渲染不连贯，导致页面渲染加载阻塞。\n\n3. `事件触发线程`\n   归属于浏览器而不是JS引擎，用来控制事件循环（可以理解，JS引擎自己都忙不过来，需要浏览器另开线程协助）\n   当JS引擎执行代码块如setTimeOut时（也可来自浏览器内核的其他线程,如鼠标点击、AJAX异步请求等），会将对应任务添加到事件线程中\n   当对应的事件符合触发条件被触发时，该线程会把事件添加到待处理队列的队尾，等待JS引擎的处理\n   注意，由于JS的单线程关系，所以这些待处理队列中的事件都得排队等待JS引擎处理（当JS引擎空闲时才会去执行）\n\n4. `定时器线程`\n    传说中的setInterval与setTimeout所在线程\n    浏览器定时计数器并不是由JavaScript引擎计数的,（因为JavaScript引擎是单线程的, 如果处于阻塞线程状态就会影响记计时的准确）\n    因此通过单独线程来计时并触发定时（计时完毕后，添加到事件队列中，等待JS引擎空闲后执行）\n    注意，W3C在HTML标准中规定，规定要求setTimeout中低于4ms的时间间隔算为4ms。\n\n5. `异步http请求线程`\n   在XMLHttpRequest在连接后是通过浏览器新开一个线程请求\n   将检测到状态变更时，如果设置有回调函数，异步线程就产生状态变更事件，将这个回调再放入事件队列中。再由JavaScript引擎执行。\n\nWebWorker，JS的多线程？\n\n 1. 创建Worker时，JS引擎向浏览器申请开一个子线程（子线程是浏览器开的，完全受主线程控制，而且不能操作DOM）\n 2. S引擎线程与worker线程间通过特定的方式通信（postMessage API，需要通过序列化对象来与线程交互特定的数据）\n\n所以，如果有非常耗时的工作，请单独开一个Worker线程，这样里面不管如何翻天覆地都不会影响JS引擎主线程，\n只待计算出结果后，将结果通信给主线程即可，perfect!\n\n而且注意下，JS引擎是单线程的，这一点的本质仍然未改变，Worker可以理解是浏览器给JS引擎开的外挂，专门用来解决那些大量计算问题。\n\n更多详细[https://segmentfault.com/a/1190000012925872#item-4-2]\n\n关于defer和async\n\n正常引入JavaScrip JS引擎线程执行的时，GUI线程会被挂起，所以像下图展示效果先html 解析 遇到script 下载对应的资源下载后解析资源，再重新执行渲染HTML\n\n![](https://img.kancloud.cn/81/3e/813eafdcbccd1942f5be0380b56d35b2_716x184.png)\n\nAsync -- 异步\n\nasync 运行在浏览器执行其他操作时并行去下载指定js文件一旦文件的下载完成，浏览器将立即开始执行它。因此只要文件下载完 JS 文件的执行将阻止网页/HTML 文件的当前渲染。'简单'的说解决下载，让下载和其他操作可以同步，但当下载完成后的立即执行效果依旧会阻止'html' 渲染\n\nasync不会能保证在DOMContentLoaded之前或者之后执行\n\n![](https://img.kancloud.cn/79/91/799186d005394b15600cb7e97fed8562_599x168.png)\n\nAsync 只是会同步去下载js文件 还是 会执行 javascript 组织 html渲染\n\ndefer-- 延迟（常用重要）\n\n1.defer 属性的 JS 文件将与其他文件同时下载，仅在 HTML 文件呈现完成后才开始执行下载的资\n源，而不是像 async 在资源下载完成后立即执行。 因此，延迟文件的下载和执行都不会阻塞渲\n染。\n2.defer 加载顺序将始终按照指定\n3. 它会等待DOM Tree构建完成，在DOMContentLoaded事件之前先执行defer中的代码\n4. 并不会产生所谓的获取不到dom到情况\n\n注释：DOMContentLoaded是html页面生命周期 会在DOM树执行完毕就执行, onload 在 dom构建完成 并且完成了所有的图片 样式表 脚本等资源加载完成后触发\n\n![](https://img.kancloud.cn/cf/03/cf03cd6f810ad0ec78e7d83d5279c7a3_644x204.png)\n","source":"_posts/1环.md","raw":"---\ntitle: 程序 内存 进程 线程 浏览器和JavaScript和事件循环\n---\n\n\n\n### 进制数小知识点\n\n二进制 是 通过 10101010组成 \n\n八进制 通过 0 - 7 组成\n\n十进制 通过  0 - 9 组成 \n\n十六进制通过 0 - 9 a- f 组成\n\n### 什么是程序\n\n就是 app 和 电脑桌面上的 一些应用\n\n### 什么是内存\n\n这里的内存指的是 运行内存  不是 存储内存 cpu 会直接 读取 运行内存;\n\n为什么要有 运行内存 cpu 直接在 存储内存 上读写 不更好吗 而且 内存空间大?\n\n```bash\n如果cpu  直接读取 存储内存 非常慢 而且 存储内存的空间非常大\n```\n\n### 什么是进程\n\n什么是进程？；\n\n进程就是 我们 打开一个程序 内存空间 会给这个程序分配一个空间 运行进程\n\n### 什么是线程\n\n线程就是 在分配好的进程空间里面 由线程来运行这些 一个 程序 进程最少有一个线程(叫做主线程)\n（而浏览器是非常复杂的 浏览器 有多个进程） 浏览器最主要的三个进程 浏览器进程 网络进程 和 渲染进程\n\n### 程序 内存 进程 程序 线程  介绍\n\n 电脑桌面上对应的程序 当我们打开 程序的时候运行内存 会给 程序分配一片空间 叫 进程 每个 程序的进程空间 都是相互独立的 每个进程都最少有 一个 线程叫做主线程 来 运行这些代码\n\n### 浏览器进程\n\n 浏览器 有三个 主要的进程 浏览器进程 渲染进程 和 网络进程\n\n 而渲染主线程(渲染进程) 他 要 执行 css  html  js  所有的页面渲染\n\n### 渲染进程\n\n 渲染进程下面的 线程 有 GUI渲染线程 定时线程（定时器触发线程、）  和 交互线程（事件触发线程）\n\n### 如何理解js异步（面试题）\n\n（注意）不是浏览器线程  浏览器是 多线程的  但是 JavaScript（js） 是单线程语言\n\n```bash\nJS是一门单线程的语言，这是因为它运行在浏览器的渲染主线程中，而渲染主线程只有一个。果而渲染主线程承担着诸多的工作，渲染页面、执行 JS 都在其中运行。如果使用同步的方式，就极有可能导致主线程产生阻塞，从而导致消息队列中的很多其他任务无法得到执行这样一来，一方面会导致繁忙的主线程白白的消耗时间，另一方面导致页面无法及时更新，给用户造成卡死现象。\n所以浏览器采用异步的方式来避免。具体做法是当某些任务发生时，比如计时器、网络、事件监听，主线程将任务交给其他线程去处理，自身立即结束任务的执行，转而执行后续代码。当其他线程完成时，将事先传递的回调函数包装成任务，加入到消息队列的末尾排队，等待主线程调度执行。在这种异步模式下，浏览器永不阻塞，从而最大限度的保证了单线程的流畅运行。\n```\n\n### 任务有优先级吗\n\n任务没有优先级 在信息队列中先进先出\n在信息队列中 有优先级的\n\n根据W3C的最新解释\n\n 1. 每个任务都有一个任务类型 比如 微任务 定时任务 和 交互任务， 同一个类型的任务必须在同一个队列里面，不同的任务可以分属于不同的任务队列 在一次事件循环中，浏览器可以根据实际情况从不同的队列中取出执行\n 2. 浏览器必须准备一个 微任务队列，微任务队列中的任务优先其他任务执行\n\n 在目前 chrome 中 至少包含以下队列\n\n   1. 微任务队列（优先级最高）\n   2. 交互任务队列（优先级高）\n   3. 延时任务队列（优先级中）\n\n### 模型模拟（模拟）\n\n渲染主线程 [任务1, 任务2, 任务3]\n\n微任务队列 [排队1，排队2，]（优先级最高）\n\n延时任务队列 [排队1，排队2] (优先级中)\n\n交互任务 [排队1，排队2] (优先级高)\n\n### 阐述一下 JavaScript 的事件循环（面试题）\n\n```bash\n\n事件循环 又叫做消息循环 是浏览器主线程的工作方式 在Chrome的源码中 开启一个 不会结束的for循环，每次循环从信息队列中取出第一个任务执行 而其他线程只需要在合适的时候加入队列末尾即可\n\n根据 w3c官方解释：\n 每个任务都有一个任务类型 比如 微任务 定时任务 和 交互任务， 同一个类型的任务必须在同一个队列里面，不同的任务可以分属于不同的任务队列 在一次事件循环中，浏览器可以根据实际情况从不同的队列中取出执行\n\n```\n\n### js中的计时器中 能做到精确计时吗 为什么\n\n不能 因为:\n\n1. 在受 事件循环的影响，计时的回调函数 只能在主线程空闲时间运行，因此带来偏差\n2. 在w3c标准中，计时器 超过 五层 就会出现 4毫秒的最少时间，（简单解释）\n完整解释：按照W3C标准，浏览器计时时，如果嵌套层级超过五层，则会带来4毫秒的最少时间，这样在计时时间少于4毫秒时又带来偏差\n\n# 重新整理 浏览器进程和线程\n\n浏览器进程：浏览器有六个进程分别\n\n|进程名称 |数量 |进程介绍|\n|-------|-------|-------|\n|浏览器主进程(Browser)|1|主要负责进程管理，操作界面显示，用户的交互|\n|网络进程|1|负责网络资源加载|\n|渲染进程（浏览器内核Renderer进程内部是多线程）|N|负责将 HTML、CSS 和 JavaScript 转换成用户可以交互的操作界面。默认情况下，每个Tab都会被创建一个渲染进程。且渲染进程运行在沙箱模式下。|\n|GPU进程|1|负责3D效果、动画、图片等渲染。|\n|插件进程|N|负责插件的运行和隔离。隔离插件主要是防止出现单个插件崩溃导致页面或者浏览器崩溃的情况。|\n|缓存进程(Storage Service)|1| 负责提供浏览器存储的功能。他是从主进程中分离出来的服务|\n|音频进程(Audio Service)|1|主要处理 音频和视频|\n|数字解码进程(Data Decoder Service)|N|执行image，zip等文件的解码|\n\n重点是浏览器内核（渲染进程）\n\n1. `GUI进程`\n    负责渲染浏览器界面，解析HTML，CSS，构建DOM树和RenderObject树，布局和绘制等。\n    当界面需要重绘（Repaint）或由于某种操作引发回流(reflow)时，该线程就会执行\n    注意，GUI渲染线程与JS引擎线程是互斥的，当JS引擎执行时GUI线程会被挂起（相当于被冻结了），GUI更新会被保存在一个队列中等到JS引擎空闲时立即被执行。\n\n2. `JS引擎线程`\n   也称为JS内核，负责处理Javascript脚本程序。（例如V8引擎）\n   JS引擎线程负责解析Javascript脚本，运行代码。\n   JS引擎一直等待着任务队列中任务的到来，然后加以处理，一个Tab页（renderer进程）中无论什么时候都只有一个JS线程在运行JS程序\n   同样注意，GUI渲染线程与JS引擎线程是互斥的，所以如果JS执行的时间过长，这样就会造成页面的渲染不连贯，导致页面渲染加载阻塞。\n\n3. `事件触发线程`\n   归属于浏览器而不是JS引擎，用来控制事件循环（可以理解，JS引擎自己都忙不过来，需要浏览器另开线程协助）\n   当JS引擎执行代码块如setTimeOut时（也可来自浏览器内核的其他线程,如鼠标点击、AJAX异步请求等），会将对应任务添加到事件线程中\n   当对应的事件符合触发条件被触发时，该线程会把事件添加到待处理队列的队尾，等待JS引擎的处理\n   注意，由于JS的单线程关系，所以这些待处理队列中的事件都得排队等待JS引擎处理（当JS引擎空闲时才会去执行）\n\n4. `定时器线程`\n    传说中的setInterval与setTimeout所在线程\n    浏览器定时计数器并不是由JavaScript引擎计数的,（因为JavaScript引擎是单线程的, 如果处于阻塞线程状态就会影响记计时的准确）\n    因此通过单独线程来计时并触发定时（计时完毕后，添加到事件队列中，等待JS引擎空闲后执行）\n    注意，W3C在HTML标准中规定，规定要求setTimeout中低于4ms的时间间隔算为4ms。\n\n5. `异步http请求线程`\n   在XMLHttpRequest在连接后是通过浏览器新开一个线程请求\n   将检测到状态变更时，如果设置有回调函数，异步线程就产生状态变更事件，将这个回调再放入事件队列中。再由JavaScript引擎执行。\n\nWebWorker，JS的多线程？\n\n 1. 创建Worker时，JS引擎向浏览器申请开一个子线程（子线程是浏览器开的，完全受主线程控制，而且不能操作DOM）\n 2. S引擎线程与worker线程间通过特定的方式通信（postMessage API，需要通过序列化对象来与线程交互特定的数据）\n\n所以，如果有非常耗时的工作，请单独开一个Worker线程，这样里面不管如何翻天覆地都不会影响JS引擎主线程，\n只待计算出结果后，将结果通信给主线程即可，perfect!\n\n而且注意下，JS引擎是单线程的，这一点的本质仍然未改变，Worker可以理解是浏览器给JS引擎开的外挂，专门用来解决那些大量计算问题。\n\n更多详细[https://segmentfault.com/a/1190000012925872#item-4-2]\n\n关于defer和async\n\n正常引入JavaScrip JS引擎线程执行的时，GUI线程会被挂起，所以像下图展示效果先html 解析 遇到script 下载对应的资源下载后解析资源，再重新执行渲染HTML\n\n![](https://img.kancloud.cn/81/3e/813eafdcbccd1942f5be0380b56d35b2_716x184.png)\n\nAsync -- 异步\n\nasync 运行在浏览器执行其他操作时并行去下载指定js文件一旦文件的下载完成，浏览器将立即开始执行它。因此只要文件下载完 JS 文件的执行将阻止网页/HTML 文件的当前渲染。'简单'的说解决下载，让下载和其他操作可以同步，但当下载完成后的立即执行效果依旧会阻止'html' 渲染\n\nasync不会能保证在DOMContentLoaded之前或者之后执行\n\n![](https://img.kancloud.cn/79/91/799186d005394b15600cb7e97fed8562_599x168.png)\n\nAsync 只是会同步去下载js文件 还是 会执行 javascript 组织 html渲染\n\ndefer-- 延迟（常用重要）\n\n1.defer 属性的 JS 文件将与其他文件同时下载，仅在 HTML 文件呈现完成后才开始执行下载的资\n源，而不是像 async 在资源下载完成后立即执行。 因此，延迟文件的下载和执行都不会阻塞渲\n染。\n2.defer 加载顺序将始终按照指定\n3. 它会等待DOM Tree构建完成，在DOMContentLoaded事件之前先执行defer中的代码\n4. 并不会产生所谓的获取不到dom到情况\n\n注释：DOMContentLoaded是html页面生命周期 会在DOM树执行完毕就执行, onload 在 dom构建完成 并且完成了所有的图片 样式表 脚本等资源加载完成后触发\n\n![](https://img.kancloud.cn/cf/03/cf03cd6f810ad0ec78e7d83d5279c7a3_644x204.png)\n","slug":"1环","published":1,"date":"2024-09-11T23:21:34.584Z","updated":"2024-09-11T23:26:51.118Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cmgu7uye50003jkv235szcvya","content":"<h3 id=\"进制数小知识点\"><a href=\"#进制数小知识点\" class=\"headerlink\" title=\"进制数小知识点\"></a>进制数小知识点</h3><p>二进制 是 通过 10101010组成 </p>\n<p>八进制 通过 0 - 7 组成</p>\n<p>十进制 通过  0 - 9 组成 </p>\n<p>十六进制通过 0 - 9 a- f 组成</p>\n<h3 id=\"什么是程序\"><a href=\"#什么是程序\" class=\"headerlink\" title=\"什么是程序\"></a>什么是程序</h3><p>就是 app 和 电脑桌面上的 一些应用</p>\n<h3 id=\"什么是内存\"><a href=\"#什么是内存\" class=\"headerlink\" title=\"什么是内存\"></a>什么是内存</h3><p>这里的内存指的是 运行内存  不是 存储内存 cpu 会直接 读取 运行内存;</p>\n<p>为什么要有 运行内存 cpu 直接在 存储内存 上读写 不更好吗 而且 内存空间大?</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">如果cpu  直接读取 存储内存 非常慢 而且 存储内存的空间非常大</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"什么是进程\"><a href=\"#什么是进程\" class=\"headerlink\" title=\"什么是进程\"></a>什么是进程</h3><p>什么是进程？；</p>\n<p>进程就是 我们 打开一个程序 内存空间 会给这个程序分配一个空间 运行进程</p>\n<h3 id=\"什么是线程\"><a href=\"#什么是线程\" class=\"headerlink\" title=\"什么是线程\"></a>什么是线程</h3><p>线程就是 在分配好的进程空间里面 由线程来运行这些 一个 程序 进程最少有一个线程(叫做主线程)<br>（而浏览器是非常复杂的 浏览器 有多个进程） 浏览器最主要的三个进程 浏览器进程 网络进程 和 渲染进程</p>\n<h3 id=\"程序-内存-进程-程序-线程-介绍\"><a href=\"#程序-内存-进程-程序-线程-介绍\" class=\"headerlink\" title=\"程序 内存 进程 程序 线程  介绍\"></a>程序 内存 进程 程序 线程  介绍</h3><p> 电脑桌面上对应的程序 当我们打开 程序的时候运行内存 会给 程序分配一片空间 叫 进程 每个 程序的进程空间 都是相互独立的 每个进程都最少有 一个 线程叫做主线程 来 运行这些代码</p>\n<h3 id=\"浏览器进程\"><a href=\"#浏览器进程\" class=\"headerlink\" title=\"浏览器进程\"></a>浏览器进程</h3><p> 浏览器 有三个 主要的进程 浏览器进程 渲染进程 和 网络进程</p>\n<p> 而渲染主线程(渲染进程) 他 要 执行 css  html  js  所有的页面渲染</p>\n<h3 id=\"渲染进程\"><a href=\"#渲染进程\" class=\"headerlink\" title=\"渲染进程\"></a>渲染进程</h3><p> 渲染进程下面的 线程 有 GUI渲染线程 定时线程（定时器触发线程、）  和 交互线程（事件触发线程）</p>\n<h3 id=\"如何理解js异步（面试题）\"><a href=\"#如何理解js异步（面试题）\" class=\"headerlink\" title=\"如何理解js异步（面试题）\"></a>如何理解js异步（面试题）</h3><p>（注意）不是浏览器线程  浏览器是 多线程的  但是 JavaScript（js） 是单线程语言</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">JS是一门单线程的语言，这是因为它运行在浏览器的渲染主线程中，而渲染主线程只有一个。果而渲染主线程承担着诸多的工作，渲染页面、执行 JS 都在其中运行。如果使用同步的方式，就极有可能导致主线程产生阻塞，从而导致消息队列中的很多其他任务无法得到执行这样一来，一方面会导致繁忙的主线程白白的消耗时间，另一方面导致页面无法及时更新，给用户造成卡死现象。</span><br><span class=\"line\">所以浏览器采用异步的方式来避免。具体做法是当某些任务发生时，比如计时器、网络、事件监听，主线程将任务交给其他线程去处理，自身立即结束任务的执行，转而执行后续代码。当其他线程完成时，将事先传递的回调函数包装成任务，加入到消息队列的末尾排队，等待主线程调度执行。在这种异步模式下，浏览器永不阻塞，从而最大限度的保证了单线程的流畅运行。</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"任务有优先级吗\"><a href=\"#任务有优先级吗\" class=\"headerlink\" title=\"任务有优先级吗\"></a>任务有优先级吗</h3><p>任务没有优先级 在信息队列中先进先出<br>在信息队列中 有优先级的</p>\n<p>根据W3C的最新解释</p>\n<ol>\n<li>每个任务都有一个任务类型 比如 微任务 定时任务 和 交互任务， 同一个类型的任务必须在同一个队列里面，不同的任务可以分属于不同的任务队列 在一次事件循环中，浏览器可以根据实际情况从不同的队列中取出执行</li>\n<li>浏览器必须准备一个 微任务队列，微任务队列中的任务优先其他任务执行</li>\n</ol>\n<p> 在目前 chrome 中 至少包含以下队列</p>\n<ol>\n<li>微任务队列（优先级最高）</li>\n<li>交互任务队列（优先级高）</li>\n<li>延时任务队列（优先级中）</li>\n</ol>\n<h3 id=\"模型模拟（模拟）\"><a href=\"#模型模拟（模拟）\" class=\"headerlink\" title=\"模型模拟（模拟）\"></a>模型模拟（模拟）</h3><p>渲染主线程 [任务1, 任务2, 任务3]</p>\n<p>微任务队列 [排队1，排队2，]（优先级最高）</p>\n<p>延时任务队列 [排队1，排队2] (优先级中)</p>\n<p>交互任务 [排队1，排队2] (优先级高)</p>\n<h3 id=\"阐述一下-JavaScript-的事件循环（面试题）\"><a href=\"#阐述一下-JavaScript-的事件循环（面试题）\" class=\"headerlink\" title=\"阐述一下 JavaScript 的事件循环（面试题）\"></a>阐述一下 JavaScript 的事件循环（面试题）</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">事件循环 又叫做消息循环 是浏览器主线程的工作方式 在Chrome的源码中 开启一个 不会结束的<span class=\"keyword\">for</span>循环，每次循环从信息队列中取出第一个任务执行 而其他线程只需要在合适的时候加入队列末尾即可</span><br><span class=\"line\"></span><br><span class=\"line\">根据 w3c官方解释：</span><br><span class=\"line\"> 每个任务都有一个任务类型 比如 微任务 定时任务 和 交互任务， 同一个类型的任务必须在同一个队列里面，不同的任务可以分属于不同的任务队列 在一次事件循环中，浏览器可以根据实际情况从不同的队列中取出执行</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"js中的计时器中-能做到精确计时吗-为什么\"><a href=\"#js中的计时器中-能做到精确计时吗-为什么\" class=\"headerlink\" title=\"js中的计时器中 能做到精确计时吗 为什么\"></a>js中的计时器中 能做到精确计时吗 为什么</h3><p>不能 因为:</p>\n<ol>\n<li>在受 事件循环的影响，计时的回调函数 只能在主线程空闲时间运行，因此带来偏差</li>\n<li>在w3c标准中，计时器 超过 五层 就会出现 4毫秒的最少时间，（简单解释）<br>完整解释：按照W3C标准，浏览器计时时，如果嵌套层级超过五层，则会带来4毫秒的最少时间，这样在计时时间少于4毫秒时又带来偏差</li>\n</ol>\n<h1 id=\"重新整理-浏览器进程和线程\"><a href=\"#重新整理-浏览器进程和线程\" class=\"headerlink\" title=\"重新整理 浏览器进程和线程\"></a>重新整理 浏览器进程和线程</h1><p>浏览器进程：浏览器有六个进程分别</p>\n<table>\n<thead>\n<tr>\n<th>进程名称</th>\n<th>数量</th>\n<th>进程介绍</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>浏览器主进程(Browser)</td>\n<td>1</td>\n<td>主要负责进程管理，操作界面显示，用户的交互</td>\n</tr>\n<tr>\n<td>网络进程</td>\n<td>1</td>\n<td>负责网络资源加载</td>\n</tr>\n<tr>\n<td>渲染进程（浏览器内核Renderer进程内部是多线程）</td>\n<td>N</td>\n<td>负责将 HTML、CSS 和 JavaScript 转换成用户可以交互的操作界面。默认情况下，每个Tab都会被创建一个渲染进程。且渲染进程运行在沙箱模式下。</td>\n</tr>\n<tr>\n<td>GPU进程</td>\n<td>1</td>\n<td>负责3D效果、动画、图片等渲染。</td>\n</tr>\n<tr>\n<td>插件进程</td>\n<td>N</td>\n<td>负责插件的运行和隔离。隔离插件主要是防止出现单个插件崩溃导致页面或者浏览器崩溃的情况。</td>\n</tr>\n<tr>\n<td>缓存进程(Storage Service)</td>\n<td>1</td>\n<td>负责提供浏览器存储的功能。他是从主进程中分离出来的服务</td>\n</tr>\n<tr>\n<td>音频进程(Audio Service)</td>\n<td>1</td>\n<td>主要处理 音频和视频</td>\n</tr>\n<tr>\n<td>数字解码进程(Data Decoder Service)</td>\n<td>N</td>\n<td>执行image，zip等文件的解码</td>\n</tr>\n</tbody></table>\n<p>重点是浏览器内核（渲染进程）</p>\n<ol>\n<li><p><code>GUI进程</code><br> 负责渲染浏览器界面，解析HTML，CSS，构建DOM树和RenderObject树，布局和绘制等。<br> 当界面需要重绘（Repaint）或由于某种操作引发回流(reflow)时，该线程就会执行<br> 注意，GUI渲染线程与JS引擎线程是互斥的，当JS引擎执行时GUI线程会被挂起（相当于被冻结了），GUI更新会被保存在一个队列中等到JS引擎空闲时立即被执行。</p>\n</li>\n<li><p><code>JS引擎线程</code><br>也称为JS内核，负责处理Javascript脚本程序。（例如V8引擎）<br>JS引擎线程负责解析Javascript脚本，运行代码。<br>JS引擎一直等待着任务队列中任务的到来，然后加以处理，一个Tab页（renderer进程）中无论什么时候都只有一个JS线程在运行JS程序<br>同样注意，GUI渲染线程与JS引擎线程是互斥的，所以如果JS执行的时间过长，这样就会造成页面的渲染不连贯，导致页面渲染加载阻塞。</p>\n</li>\n<li><p><code>事件触发线程</code><br>归属于浏览器而不是JS引擎，用来控制事件循环（可以理解，JS引擎自己都忙不过来，需要浏览器另开线程协助）<br>当JS引擎执行代码块如setTimeOut时（也可来自浏览器内核的其他线程,如鼠标点击、AJAX异步请求等），会将对应任务添加到事件线程中<br>当对应的事件符合触发条件被触发时，该线程会把事件添加到待处理队列的队尾，等待JS引擎的处理<br>注意，由于JS的单线程关系，所以这些待处理队列中的事件都得排队等待JS引擎处理（当JS引擎空闲时才会去执行）</p>\n</li>\n<li><p><code>定时器线程</code><br> 传说中的setInterval与setTimeout所在线程<br> 浏览器定时计数器并不是由JavaScript引擎计数的,（因为JavaScript引擎是单线程的, 如果处于阻塞线程状态就会影响记计时的准确）<br> 因此通过单独线程来计时并触发定时（计时完毕后，添加到事件队列中，等待JS引擎空闲后执行）<br> 注意，W3C在HTML标准中规定，规定要求setTimeout中低于4ms的时间间隔算为4ms。</p>\n</li>\n<li><p><code>异步http请求线程</code><br>在XMLHttpRequest在连接后是通过浏览器新开一个线程请求<br>将检测到状态变更时，如果设置有回调函数，异步线程就产生状态变更事件，将这个回调再放入事件队列中。再由JavaScript引擎执行。</p>\n</li>\n</ol>\n<p>WebWorker，JS的多线程？</p>\n<ol>\n<li>创建Worker时，JS引擎向浏览器申请开一个子线程（子线程是浏览器开的，完全受主线程控制，而且不能操作DOM）</li>\n<li>S引擎线程与worker线程间通过特定的方式通信（postMessage API，需要通过序列化对象来与线程交互特定的数据）</li>\n</ol>\n<p>所以，如果有非常耗时的工作，请单独开一个Worker线程，这样里面不管如何翻天覆地都不会影响JS引擎主线程，<br>只待计算出结果后，将结果通信给主线程即可，perfect!</p>\n<p>而且注意下，JS引擎是单线程的，这一点的本质仍然未改变，Worker可以理解是浏览器给JS引擎开的外挂，专门用来解决那些大量计算问题。</p>\n<p>更多详细[<a href=\"https://segmentfault.com/a/1190000012925872#item-4-2]\">https://segmentfault.com/a/1190000012925872#item-4-2]</a></p>\n<p>关于defer和async</p>\n<p>正常引入JavaScrip JS引擎线程执行的时，GUI线程会被挂起，所以像下图展示效果先html 解析 遇到script 下载对应的资源下载后解析资源，再重新执行渲染HTML</p>\n<p><img src=\"https://img.kancloud.cn/81/3e/813eafdcbccd1942f5be0380b56d35b2_716x184.png\"></p>\n<p>Async – 异步</p>\n<p>async 运行在浏览器执行其他操作时并行去下载指定js文件一旦文件的下载完成，浏览器将立即开始执行它。因此只要文件下载完 JS 文件的执行将阻止网页&#x2F;HTML 文件的当前渲染。’简单’的说解决下载，让下载和其他操作可以同步，但当下载完成后的立即执行效果依旧会阻止’html’ 渲染</p>\n<p>async不会能保证在DOMContentLoaded之前或者之后执行</p>\n<p><img src=\"https://img.kancloud.cn/79/91/799186d005394b15600cb7e97fed8562_599x168.png\"></p>\n<p>Async 只是会同步去下载js文件 还是 会执行 javascript 组织 html渲染</p>\n<p>defer– 延迟（常用重要）</p>\n<p>1.defer 属性的 JS 文件将与其他文件同时下载，仅在 HTML 文件呈现完成后才开始执行下载的资<br>源，而不是像 async 在资源下载完成后立即执行。 因此，延迟文件的下载和执行都不会阻塞渲<br>染。<br>2.defer 加载顺序将始终按照指定<br>3. 它会等待DOM Tree构建完成，在DOMContentLoaded事件之前先执行defer中的代码<br>4. 并不会产生所谓的获取不到dom到情况</p>\n<p>注释：DOMContentLoaded是html页面生命周期 会在DOM树执行完毕就执行, onload 在 dom构建完成 并且完成了所有的图片 样式表 脚本等资源加载完成后触发</p>\n<p><img src=\"https://img.kancloud.cn/cf/03/cf03cd6f810ad0ec78e7d83d5279c7a3_644x204.png\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"进制数小知识点\"><a href=\"#进制数小知识点\" class=\"headerlink\" title=\"进制数小知识点\"></a>进制数小知识点</h3><p>二进制 是 通过 10101010组成 </p>\n<p>八进制 通过 0 - 7 组成</p>\n<p>十进制 通过  0 - 9 组成 </p>\n<p>十六进制通过 0 - 9 a- f 组成</p>\n<h3 id=\"什么是程序\"><a href=\"#什么是程序\" class=\"headerlink\" title=\"什么是程序\"></a>什么是程序</h3><p>就是 app 和 电脑桌面上的 一些应用</p>\n<h3 id=\"什么是内存\"><a href=\"#什么是内存\" class=\"headerlink\" title=\"什么是内存\"></a>什么是内存</h3><p>这里的内存指的是 运行内存  不是 存储内存 cpu 会直接 读取 运行内存;</p>\n<p>为什么要有 运行内存 cpu 直接在 存储内存 上读写 不更好吗 而且 内存空间大?</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">如果cpu  直接读取 存储内存 非常慢 而且 存储内存的空间非常大</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"什么是进程\"><a href=\"#什么是进程\" class=\"headerlink\" title=\"什么是进程\"></a>什么是进程</h3><p>什么是进程？；</p>\n<p>进程就是 我们 打开一个程序 内存空间 会给这个程序分配一个空间 运行进程</p>\n<h3 id=\"什么是线程\"><a href=\"#什么是线程\" class=\"headerlink\" title=\"什么是线程\"></a>什么是线程</h3><p>线程就是 在分配好的进程空间里面 由线程来运行这些 一个 程序 进程最少有一个线程(叫做主线程)<br>（而浏览器是非常复杂的 浏览器 有多个进程） 浏览器最主要的三个进程 浏览器进程 网络进程 和 渲染进程</p>\n<h3 id=\"程序-内存-进程-程序-线程-介绍\"><a href=\"#程序-内存-进程-程序-线程-介绍\" class=\"headerlink\" title=\"程序 内存 进程 程序 线程  介绍\"></a>程序 内存 进程 程序 线程  介绍</h3><p> 电脑桌面上对应的程序 当我们打开 程序的时候运行内存 会给 程序分配一片空间 叫 进程 每个 程序的进程空间 都是相互独立的 每个进程都最少有 一个 线程叫做主线程 来 运行这些代码</p>\n<h3 id=\"浏览器进程\"><a href=\"#浏览器进程\" class=\"headerlink\" title=\"浏览器进程\"></a>浏览器进程</h3><p> 浏览器 有三个 主要的进程 浏览器进程 渲染进程 和 网络进程</p>\n<p> 而渲染主线程(渲染进程) 他 要 执行 css  html  js  所有的页面渲染</p>\n<h3 id=\"渲染进程\"><a href=\"#渲染进程\" class=\"headerlink\" title=\"渲染进程\"></a>渲染进程</h3><p> 渲染进程下面的 线程 有 GUI渲染线程 定时线程（定时器触发线程、）  和 交互线程（事件触发线程）</p>\n<h3 id=\"如何理解js异步（面试题）\"><a href=\"#如何理解js异步（面试题）\" class=\"headerlink\" title=\"如何理解js异步（面试题）\"></a>如何理解js异步（面试题）</h3><p>（注意）不是浏览器线程  浏览器是 多线程的  但是 JavaScript（js） 是单线程语言</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">JS是一门单线程的语言，这是因为它运行在浏览器的渲染主线程中，而渲染主线程只有一个。果而渲染主线程承担着诸多的工作，渲染页面、执行 JS 都在其中运行。如果使用同步的方式，就极有可能导致主线程产生阻塞，从而导致消息队列中的很多其他任务无法得到执行这样一来，一方面会导致繁忙的主线程白白的消耗时间，另一方面导致页面无法及时更新，给用户造成卡死现象。</span><br><span class=\"line\">所以浏览器采用异步的方式来避免。具体做法是当某些任务发生时，比如计时器、网络、事件监听，主线程将任务交给其他线程去处理，自身立即结束任务的执行，转而执行后续代码。当其他线程完成时，将事先传递的回调函数包装成任务，加入到消息队列的末尾排队，等待主线程调度执行。在这种异步模式下，浏览器永不阻塞，从而最大限度的保证了单线程的流畅运行。</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"任务有优先级吗\"><a href=\"#任务有优先级吗\" class=\"headerlink\" title=\"任务有优先级吗\"></a>任务有优先级吗</h3><p>任务没有优先级 在信息队列中先进先出<br>在信息队列中 有优先级的</p>\n<p>根据W3C的最新解释</p>\n<ol>\n<li>每个任务都有一个任务类型 比如 微任务 定时任务 和 交互任务， 同一个类型的任务必须在同一个队列里面，不同的任务可以分属于不同的任务队列 在一次事件循环中，浏览器可以根据实际情况从不同的队列中取出执行</li>\n<li>浏览器必须准备一个 微任务队列，微任务队列中的任务优先其他任务执行</li>\n</ol>\n<p> 在目前 chrome 中 至少包含以下队列</p>\n<ol>\n<li>微任务队列（优先级最高）</li>\n<li>交互任务队列（优先级高）</li>\n<li>延时任务队列（优先级中）</li>\n</ol>\n<h3 id=\"模型模拟（模拟）\"><a href=\"#模型模拟（模拟）\" class=\"headerlink\" title=\"模型模拟（模拟）\"></a>模型模拟（模拟）</h3><p>渲染主线程 [任务1, 任务2, 任务3]</p>\n<p>微任务队列 [排队1，排队2，]（优先级最高）</p>\n<p>延时任务队列 [排队1，排队2] (优先级中)</p>\n<p>交互任务 [排队1，排队2] (优先级高)</p>\n<h3 id=\"阐述一下-JavaScript-的事件循环（面试题）\"><a href=\"#阐述一下-JavaScript-的事件循环（面试题）\" class=\"headerlink\" title=\"阐述一下 JavaScript 的事件循环（面试题）\"></a>阐述一下 JavaScript 的事件循环（面试题）</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">事件循环 又叫做消息循环 是浏览器主线程的工作方式 在Chrome的源码中 开启一个 不会结束的<span class=\"keyword\">for</span>循环，每次循环从信息队列中取出第一个任务执行 而其他线程只需要在合适的时候加入队列末尾即可</span><br><span class=\"line\"></span><br><span class=\"line\">根据 w3c官方解释：</span><br><span class=\"line\"> 每个任务都有一个任务类型 比如 微任务 定时任务 和 交互任务， 同一个类型的任务必须在同一个队列里面，不同的任务可以分属于不同的任务队列 在一次事件循环中，浏览器可以根据实际情况从不同的队列中取出执行</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"js中的计时器中-能做到精确计时吗-为什么\"><a href=\"#js中的计时器中-能做到精确计时吗-为什么\" class=\"headerlink\" title=\"js中的计时器中 能做到精确计时吗 为什么\"></a>js中的计时器中 能做到精确计时吗 为什么</h3><p>不能 因为:</p>\n<ol>\n<li>在受 事件循环的影响，计时的回调函数 只能在主线程空闲时间运行，因此带来偏差</li>\n<li>在w3c标准中，计时器 超过 五层 就会出现 4毫秒的最少时间，（简单解释）<br>完整解释：按照W3C标准，浏览器计时时，如果嵌套层级超过五层，则会带来4毫秒的最少时间，这样在计时时间少于4毫秒时又带来偏差</li>\n</ol>\n<h1 id=\"重新整理-浏览器进程和线程\"><a href=\"#重新整理-浏览器进程和线程\" class=\"headerlink\" title=\"重新整理 浏览器进程和线程\"></a>重新整理 浏览器进程和线程</h1><p>浏览器进程：浏览器有六个进程分别</p>\n<table>\n<thead>\n<tr>\n<th>进程名称</th>\n<th>数量</th>\n<th>进程介绍</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>浏览器主进程(Browser)</td>\n<td>1</td>\n<td>主要负责进程管理，操作界面显示，用户的交互</td>\n</tr>\n<tr>\n<td>网络进程</td>\n<td>1</td>\n<td>负责网络资源加载</td>\n</tr>\n<tr>\n<td>渲染进程（浏览器内核Renderer进程内部是多线程）</td>\n<td>N</td>\n<td>负责将 HTML、CSS 和 JavaScript 转换成用户可以交互的操作界面。默认情况下，每个Tab都会被创建一个渲染进程。且渲染进程运行在沙箱模式下。</td>\n</tr>\n<tr>\n<td>GPU进程</td>\n<td>1</td>\n<td>负责3D效果、动画、图片等渲染。</td>\n</tr>\n<tr>\n<td>插件进程</td>\n<td>N</td>\n<td>负责插件的运行和隔离。隔离插件主要是防止出现单个插件崩溃导致页面或者浏览器崩溃的情况。</td>\n</tr>\n<tr>\n<td>缓存进程(Storage Service)</td>\n<td>1</td>\n<td>负责提供浏览器存储的功能。他是从主进程中分离出来的服务</td>\n</tr>\n<tr>\n<td>音频进程(Audio Service)</td>\n<td>1</td>\n<td>主要处理 音频和视频</td>\n</tr>\n<tr>\n<td>数字解码进程(Data Decoder Service)</td>\n<td>N</td>\n<td>执行image，zip等文件的解码</td>\n</tr>\n</tbody></table>\n<p>重点是浏览器内核（渲染进程）</p>\n<ol>\n<li><p><code>GUI进程</code><br> 负责渲染浏览器界面，解析HTML，CSS，构建DOM树和RenderObject树，布局和绘制等。<br> 当界面需要重绘（Repaint）或由于某种操作引发回流(reflow)时，该线程就会执行<br> 注意，GUI渲染线程与JS引擎线程是互斥的，当JS引擎执行时GUI线程会被挂起（相当于被冻结了），GUI更新会被保存在一个队列中等到JS引擎空闲时立即被执行。</p>\n</li>\n<li><p><code>JS引擎线程</code><br>也称为JS内核，负责处理Javascript脚本程序。（例如V8引擎）<br>JS引擎线程负责解析Javascript脚本，运行代码。<br>JS引擎一直等待着任务队列中任务的到来，然后加以处理，一个Tab页（renderer进程）中无论什么时候都只有一个JS线程在运行JS程序<br>同样注意，GUI渲染线程与JS引擎线程是互斥的，所以如果JS执行的时间过长，这样就会造成页面的渲染不连贯，导致页面渲染加载阻塞。</p>\n</li>\n<li><p><code>事件触发线程</code><br>归属于浏览器而不是JS引擎，用来控制事件循环（可以理解，JS引擎自己都忙不过来，需要浏览器另开线程协助）<br>当JS引擎执行代码块如setTimeOut时（也可来自浏览器内核的其他线程,如鼠标点击、AJAX异步请求等），会将对应任务添加到事件线程中<br>当对应的事件符合触发条件被触发时，该线程会把事件添加到待处理队列的队尾，等待JS引擎的处理<br>注意，由于JS的单线程关系，所以这些待处理队列中的事件都得排队等待JS引擎处理（当JS引擎空闲时才会去执行）</p>\n</li>\n<li><p><code>定时器线程</code><br> 传说中的setInterval与setTimeout所在线程<br> 浏览器定时计数器并不是由JavaScript引擎计数的,（因为JavaScript引擎是单线程的, 如果处于阻塞线程状态就会影响记计时的准确）<br> 因此通过单独线程来计时并触发定时（计时完毕后，添加到事件队列中，等待JS引擎空闲后执行）<br> 注意，W3C在HTML标准中规定，规定要求setTimeout中低于4ms的时间间隔算为4ms。</p>\n</li>\n<li><p><code>异步http请求线程</code><br>在XMLHttpRequest在连接后是通过浏览器新开一个线程请求<br>将检测到状态变更时，如果设置有回调函数，异步线程就产生状态变更事件，将这个回调再放入事件队列中。再由JavaScript引擎执行。</p>\n</li>\n</ol>\n<p>WebWorker，JS的多线程？</p>\n<ol>\n<li>创建Worker时，JS引擎向浏览器申请开一个子线程（子线程是浏览器开的，完全受主线程控制，而且不能操作DOM）</li>\n<li>S引擎线程与worker线程间通过特定的方式通信（postMessage API，需要通过序列化对象来与线程交互特定的数据）</li>\n</ol>\n<p>所以，如果有非常耗时的工作，请单独开一个Worker线程，这样里面不管如何翻天覆地都不会影响JS引擎主线程，<br>只待计算出结果后，将结果通信给主线程即可，perfect!</p>\n<p>而且注意下，JS引擎是单线程的，这一点的本质仍然未改变，Worker可以理解是浏览器给JS引擎开的外挂，专门用来解决那些大量计算问题。</p>\n<p>更多详细[<a href=\"https://segmentfault.com/a/1190000012925872#item-4-2]\">https://segmentfault.com/a/1190000012925872#item-4-2]</a></p>\n<p>关于defer和async</p>\n<p>正常引入JavaScrip JS引擎线程执行的时，GUI线程会被挂起，所以像下图展示效果先html 解析 遇到script 下载对应的资源下载后解析资源，再重新执行渲染HTML</p>\n<p><img src=\"https://img.kancloud.cn/81/3e/813eafdcbccd1942f5be0380b56d35b2_716x184.png\"></p>\n<p>Async – 异步</p>\n<p>async 运行在浏览器执行其他操作时并行去下载指定js文件一旦文件的下载完成，浏览器将立即开始执行它。因此只要文件下载完 JS 文件的执行将阻止网页&#x2F;HTML 文件的当前渲染。’简单’的说解决下载，让下载和其他操作可以同步，但当下载完成后的立即执行效果依旧会阻止’html’ 渲染</p>\n<p>async不会能保证在DOMContentLoaded之前或者之后执行</p>\n<p><img src=\"https://img.kancloud.cn/79/91/799186d005394b15600cb7e97fed8562_599x168.png\"></p>\n<p>Async 只是会同步去下载js文件 还是 会执行 javascript 组织 html渲染</p>\n<p>defer– 延迟（常用重要）</p>\n<p>1.defer 属性的 JS 文件将与其他文件同时下载，仅在 HTML 文件呈现完成后才开始执行下载的资<br>源，而不是像 async 在资源下载完成后立即执行。 因此，延迟文件的下载和执行都不会阻塞渲<br>染。<br>2.defer 加载顺序将始终按照指定<br>3. 它会等待DOM Tree构建完成，在DOMContentLoaded事件之前先执行defer中的代码<br>4. 并不会产生所谓的获取不到dom到情况</p>\n<p>注释：DOMContentLoaded是html页面生命周期 会在DOM树执行完毕就执行, onload 在 dom构建完成 并且完成了所有的图片 样式表 脚本等资源加载完成后触发</p>\n<p><img src=\"https://img.kancloud.cn/cf/03/cf03cd6f810ad0ec78e7d83d5279c7a3_644x204.png\"></p>\n"},{"title":"关系型数据库设计规范","_content":"\n​\t数据库规范化是使用一系列范式设计数据库（通常是关系型数据库）的过程，目的是减少数据冗余，增强数据一致性。\n\n​\t关系型数据库一共有6种范式，分别是第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、巴斯-科德范式（BCNF）、第四范式(4NF）和第五范式（5NF）。遵循的范式级别越高，数据冗余性就越低。\n\n​\t数据依赖\n\n![](../img/image-20241110174645738.png)\n\n1、完全函数依赖：\n\n​\t设X，Y是关系R的两个属性集合，X’是X的真子集，存在X→Y，但对每一个X’都有X’!→Y，则称Y完全函数依赖于X。\n\n​\t人类语言：\n\n​\t比如通过，(学号，课程) 推出分数 ，但是单独用学号推断不出来分数，那么就可以说：分数 完全依赖于(学号，课程) 。\n\n即：通过AB能得出C，但是AB单独得不出C，那么说C完全依赖于AB。\n\n2、部分函数依赖\n\n​        假如 Y函数依赖于 X，但同时 Y 并不完全函数依赖于 X，那么我们就称 Y 部分函数依赖于 X，\n\n人类语言：\n\n​\t比如通过，(学号，课程) 推出姓名，因为其实直接可以通过，学号推出姓名，所以：姓名  部分依赖于 (学号，课程)\n\n即：通过AB能得出C，通过A也能得出C，或者通过B也能得出C，那么说C部分依赖于AB。\n\n 3、传递函数依赖\n\n​         传递函数依赖：设X，Y，Z是关系R中互不相同的属性集合，存在X→Y(Y !→X),Y→Z，则称Z传递函数依赖于X。\n\n人类语言：\n\n \t比如：学号 推出 系名 ， 系名 推出 系主任， 但是，系主任推不出学号，系主任主要依赖于系名。这种情况可以说：系主任 传递依赖于 学号\n\n通过A得到B，通过B得到C，但是C得不到A，那么说C传递依赖于A。\n\n### 第一范式：\n\n第一范式1NF核心原则是：属性不可分割\n\n![不符合一范式的表格设计](/./img/截屏2024-11-10-18.20.37.png)\n\n​\t很明显上图所示的表格设计是不符合第一范式的，商品列中的数据不是原子数据项，是可以进行分割的，因此对表格进行修改，让表格符合第一范式的要求，修改结果如下图所示：\n\n![符合一范式的表格设计](../img/2024-11-10-21.05.40.png)\n\n​\t实际上，1NF是所有关系型数据库的最基本要求，你在关系型数据库管理系统（RDBMS），例如SQL Server，Oracle，MySQL中创建数据表的时候，如果数据表的设计不符合这个最基本的要求，那么操作一定是不能成功的。也就是说，只要在RDBMS中已经存在的数据表，一定是符合1NF的。\n\n### 第二范式\n\n第二范式2NF核心原则：不能存在“部分函数依赖”\n\n![存在部分函数依赖](../img/截屏2024-11-10-21.26.40.png)\n\n​\t以上表格明显存在部分依赖，比如这种表的主键是学号，课名，分数确实完全依赖于（学号，课名），但是姓名并不完全依赖于（学号，课名）\n\n<img src=\"../img/截屏2024-11-10-21.29.41.png\" style=\"zoom:67%;\" />\n\n![](../img/截屏2024-11-10-21.30.24.png)\n\n以上符合第二范式，去掉部分函数依赖\n\n### 第三范式\n\n第三范式3NF核心原则：不能存在传递函数依赖\n\n​\t在下面这张表中，存在传递函数依赖：学号->系名->系主任，但是系主任推不出学号。\n\n![](../img/截屏2024-11-10-21.30.24.png)\n\n拆解如下：\n\n![](../img/截屏2024-11-10-21.41.09.png)","source":"_posts/BigData/关系型数据库设计规范.md","raw":"---\ntitle: 关系型数据库设计规范\ntag:\n  - DataBase\n  - BigData\ncategories:\n  - - BigData\n---\n\n​\t数据库规范化是使用一系列范式设计数据库（通常是关系型数据库）的过程，目的是减少数据冗余，增强数据一致性。\n\n​\t关系型数据库一共有6种范式，分别是第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、巴斯-科德范式（BCNF）、第四范式(4NF）和第五范式（5NF）。遵循的范式级别越高，数据冗余性就越低。\n\n​\t数据依赖\n\n![](../img/image-20241110174645738.png)\n\n1、完全函数依赖：\n\n​\t设X，Y是关系R的两个属性集合，X’是X的真子集，存在X→Y，但对每一个X’都有X’!→Y，则称Y完全函数依赖于X。\n\n​\t人类语言：\n\n​\t比如通过，(学号，课程) 推出分数 ，但是单独用学号推断不出来分数，那么就可以说：分数 完全依赖于(学号，课程) 。\n\n即：通过AB能得出C，但是AB单独得不出C，那么说C完全依赖于AB。\n\n2、部分函数依赖\n\n​        假如 Y函数依赖于 X，但同时 Y 并不完全函数依赖于 X，那么我们就称 Y 部分函数依赖于 X，\n\n人类语言：\n\n​\t比如通过，(学号，课程) 推出姓名，因为其实直接可以通过，学号推出姓名，所以：姓名  部分依赖于 (学号，课程)\n\n即：通过AB能得出C，通过A也能得出C，或者通过B也能得出C，那么说C部分依赖于AB。\n\n 3、传递函数依赖\n\n​         传递函数依赖：设X，Y，Z是关系R中互不相同的属性集合，存在X→Y(Y !→X),Y→Z，则称Z传递函数依赖于X。\n\n人类语言：\n\n \t比如：学号 推出 系名 ， 系名 推出 系主任， 但是，系主任推不出学号，系主任主要依赖于系名。这种情况可以说：系主任 传递依赖于 学号\n\n通过A得到B，通过B得到C，但是C得不到A，那么说C传递依赖于A。\n\n### 第一范式：\n\n第一范式1NF核心原则是：属性不可分割\n\n![不符合一范式的表格设计](/./img/截屏2024-11-10-18.20.37.png)\n\n​\t很明显上图所示的表格设计是不符合第一范式的，商品列中的数据不是原子数据项，是可以进行分割的，因此对表格进行修改，让表格符合第一范式的要求，修改结果如下图所示：\n\n![符合一范式的表格设计](../img/2024-11-10-21.05.40.png)\n\n​\t实际上，1NF是所有关系型数据库的最基本要求，你在关系型数据库管理系统（RDBMS），例如SQL Server，Oracle，MySQL中创建数据表的时候，如果数据表的设计不符合这个最基本的要求，那么操作一定是不能成功的。也就是说，只要在RDBMS中已经存在的数据表，一定是符合1NF的。\n\n### 第二范式\n\n第二范式2NF核心原则：不能存在“部分函数依赖”\n\n![存在部分函数依赖](../img/截屏2024-11-10-21.26.40.png)\n\n​\t以上表格明显存在部分依赖，比如这种表的主键是学号，课名，分数确实完全依赖于（学号，课名），但是姓名并不完全依赖于（学号，课名）\n\n<img src=\"../img/截屏2024-11-10-21.29.41.png\" style=\"zoom:67%;\" />\n\n![](../img/截屏2024-11-10-21.30.24.png)\n\n以上符合第二范式，去掉部分函数依赖\n\n### 第三范式\n\n第三范式3NF核心原则：不能存在传递函数依赖\n\n​\t在下面这张表中，存在传递函数依赖：学号->系名->系主任，但是系主任推不出学号。\n\n![](../img/截屏2024-11-10-21.30.24.png)\n\n拆解如下：\n\n![](../img/截屏2024-11-10-21.41.09.png)","slug":"BigData/关系型数据库设计规范","published":1,"date":"2024-11-11T04:38:25.892Z","updated":"2024-11-11T04:38:25.892Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cmgu7uye70004jkv2a0jo4hev","content":"<p>​\t数据库规范化是使用一系列范式设计数据库（通常是关系型数据库）的过程，目的是减少数据冗余，增强数据一致性。</p>\n<p>​\t关系型数据库一共有6种范式，分别是第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、巴斯-科德范式（BCNF）、第四范式(4NF）和第五范式（5NF）。遵循的范式级别越高，数据冗余性就越低。</p>\n<p>​\t数据依赖</p>\n<p><img src=\"/../img/image-20241110174645738.png\"></p>\n<p>1、完全函数依赖：</p>\n<p>​\t设X，Y是关系R的两个属性集合，X’是X的真子集，存在X→Y，但对每一个X’都有X’!→Y，则称Y完全函数依赖于X。</p>\n<p>​\t人类语言：</p>\n<p>​\t比如通过，(学号，课程) 推出分数 ，但是单独用学号推断不出来分数，那么就可以说：分数 完全依赖于(学号，课程) 。</p>\n<p>即：通过AB能得出C，但是AB单独得不出C，那么说C完全依赖于AB。</p>\n<p>2、部分函数依赖</p>\n<p>​        假如 Y函数依赖于 X，但同时 Y 并不完全函数依赖于 X，那么我们就称 Y 部分函数依赖于 X，</p>\n<p>人类语言：</p>\n<p>​\t比如通过，(学号，课程) 推出姓名，因为其实直接可以通过，学号推出姓名，所以：姓名  部分依赖于 (学号，课程)</p>\n<p>即：通过AB能得出C，通过A也能得出C，或者通过B也能得出C，那么说C部分依赖于AB。</p>\n<p> 3、传递函数依赖</p>\n<p>​         传递函数依赖：设X，Y，Z是关系R中互不相同的属性集合，存在X→Y(Y !→X),Y→Z，则称Z传递函数依赖于X。</p>\n<p>人类语言：</p>\n<pre><code> 比如：学号 推出 系名 ， 系名 推出 系主任， 但是，系主任推不出学号，系主任主要依赖于系名。这种情况可以说：系主任 传递依赖于 学号\n</code></pre>\n<p>通过A得到B，通过B得到C，但是C得不到A，那么说C传递依赖于A。</p>\n<h3 id=\"第一范式：\"><a href=\"#第一范式：\" class=\"headerlink\" title=\"第一范式：\"></a>第一范式：</h3><p>第一范式1NF核心原则是：属性不可分割</p>\n<p><img src=\"/./img/%E6%88%AA%E5%B1%8F2024-11-10-18.20.37.png\" alt=\"不符合一范式的表格设计\"></p>\n<p>​\t很明显上图所示的表格设计是不符合第一范式的，商品列中的数据不是原子数据项，是可以进行分割的，因此对表格进行修改，让表格符合第一范式的要求，修改结果如下图所示：</p>\n<p><img src=\"/../img/2024-11-10-21.05.40.png\" alt=\"符合一范式的表格设计\"></p>\n<p>​\t实际上，1NF是所有关系型数据库的最基本要求，你在关系型数据库管理系统（RDBMS），例如SQL Server，Oracle，MySQL中创建数据表的时候，如果数据表的设计不符合这个最基本的要求，那么操作一定是不能成功的。也就是说，只要在RDBMS中已经存在的数据表，一定是符合1NF的。</p>\n<h3 id=\"第二范式\"><a href=\"#第二范式\" class=\"headerlink\" title=\"第二范式\"></a>第二范式</h3><p>第二范式2NF核心原则：不能存在“部分函数依赖”</p>\n<p><img src=\"/../img/%E6%88%AA%E5%B1%8F2024-11-10-21.26.40.png\" alt=\"存在部分函数依赖\"></p>\n<p>​\t以上表格明显存在部分依赖，比如这种表的主键是学号，课名，分数确实完全依赖于（学号，课名），但是姓名并不完全依赖于（学号，课名）</p>\n<img src=\"../img/截屏2024-11-10-21.29.41.png\" style=\"zoom:67%;\" />\n\n<p><img src=\"/../img/%E6%88%AA%E5%B1%8F2024-11-10-21.30.24.png\"></p>\n<p>以上符合第二范式，去掉部分函数依赖</p>\n<h3 id=\"第三范式\"><a href=\"#第三范式\" class=\"headerlink\" title=\"第三范式\"></a>第三范式</h3><p>第三范式3NF核心原则：不能存在传递函数依赖</p>\n<p>​\t在下面这张表中，存在传递函数依赖：学号-&gt;系名-&gt;系主任，但是系主任推不出学号。</p>\n<p><img src=\"/../img/%E6%88%AA%E5%B1%8F2024-11-10-21.30.24.png\"></p>\n<p>拆解如下：</p>\n<p><img src=\"/../img/%E6%88%AA%E5%B1%8F2024-11-10-21.41.09.png\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>​\t数据库规范化是使用一系列范式设计数据库（通常是关系型数据库）的过程，目的是减少数据冗余，增强数据一致性。</p>\n<p>​\t关系型数据库一共有6种范式，分别是第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、巴斯-科德范式（BCNF）、第四范式(4NF）和第五范式（5NF）。遵循的范式级别越高，数据冗余性就越低。</p>\n<p>​\t数据依赖</p>\n<p><img src=\"/../img/image-20241110174645738.png\"></p>\n<p>1、完全函数依赖：</p>\n<p>​\t设X，Y是关系R的两个属性集合，X’是X的真子集，存在X→Y，但对每一个X’都有X’!→Y，则称Y完全函数依赖于X。</p>\n<p>​\t人类语言：</p>\n<p>​\t比如通过，(学号，课程) 推出分数 ，但是单独用学号推断不出来分数，那么就可以说：分数 完全依赖于(学号，课程) 。</p>\n<p>即：通过AB能得出C，但是AB单独得不出C，那么说C完全依赖于AB。</p>\n<p>2、部分函数依赖</p>\n<p>​        假如 Y函数依赖于 X，但同时 Y 并不完全函数依赖于 X，那么我们就称 Y 部分函数依赖于 X，</p>\n<p>人类语言：</p>\n<p>​\t比如通过，(学号，课程) 推出姓名，因为其实直接可以通过，学号推出姓名，所以：姓名  部分依赖于 (学号，课程)</p>\n<p>即：通过AB能得出C，通过A也能得出C，或者通过B也能得出C，那么说C部分依赖于AB。</p>\n<p> 3、传递函数依赖</p>\n<p>​         传递函数依赖：设X，Y，Z是关系R中互不相同的属性集合，存在X→Y(Y !→X),Y→Z，则称Z传递函数依赖于X。</p>\n<p>人类语言：</p>\n<pre><code> 比如：学号 推出 系名 ， 系名 推出 系主任， 但是，系主任推不出学号，系主任主要依赖于系名。这种情况可以说：系主任 传递依赖于 学号\n</code></pre>\n<p>通过A得到B，通过B得到C，但是C得不到A，那么说C传递依赖于A。</p>\n<h3 id=\"第一范式：\"><a href=\"#第一范式：\" class=\"headerlink\" title=\"第一范式：\"></a>第一范式：</h3><p>第一范式1NF核心原则是：属性不可分割</p>\n<p><img src=\"/./img/%E6%88%AA%E5%B1%8F2024-11-10-18.20.37.png\" alt=\"不符合一范式的表格设计\"></p>\n<p>​\t很明显上图所示的表格设计是不符合第一范式的，商品列中的数据不是原子数据项，是可以进行分割的，因此对表格进行修改，让表格符合第一范式的要求，修改结果如下图所示：</p>\n<p><img src=\"/../img/2024-11-10-21.05.40.png\" alt=\"符合一范式的表格设计\"></p>\n<p>​\t实际上，1NF是所有关系型数据库的最基本要求，你在关系型数据库管理系统（RDBMS），例如SQL Server，Oracle，MySQL中创建数据表的时候，如果数据表的设计不符合这个最基本的要求，那么操作一定是不能成功的。也就是说，只要在RDBMS中已经存在的数据表，一定是符合1NF的。</p>\n<h3 id=\"第二范式\"><a href=\"#第二范式\" class=\"headerlink\" title=\"第二范式\"></a>第二范式</h3><p>第二范式2NF核心原则：不能存在“部分函数依赖”</p>\n<p><img src=\"/../img/%E6%88%AA%E5%B1%8F2024-11-10-21.26.40.png\" alt=\"存在部分函数依赖\"></p>\n<p>​\t以上表格明显存在部分依赖，比如这种表的主键是学号，课名，分数确实完全依赖于（学号，课名），但是姓名并不完全依赖于（学号，课名）</p>\n<img src=\"../img/截屏2024-11-10-21.29.41.png\" style=\"zoom:67%;\" />\n\n<p><img src=\"/../img/%E6%88%AA%E5%B1%8F2024-11-10-21.30.24.png\"></p>\n<p>以上符合第二范式，去掉部分函数依赖</p>\n<h3 id=\"第三范式\"><a href=\"#第三范式\" class=\"headerlink\" title=\"第三范式\"></a>第三范式</h3><p>第三范式3NF核心原则：不能存在传递函数依赖</p>\n<p>​\t在下面这张表中，存在传递函数依赖：学号-&gt;系名-&gt;系主任，但是系主任推不出学号。</p>\n<p><img src=\"/../img/%E6%88%AA%E5%B1%8F2024-11-10-21.30.24.png\"></p>\n<p>拆解如下：</p>\n<p><img src=\"/../img/%E6%88%AA%E5%B1%8F2024-11-10-21.41.09.png\"></p>\n"},{"title":"HDFS高可用之HA架构","_content":"\n### 一、NameNode故障导致的问题\n\n1.NameNode机器宕机，导致整个集群不可用，重启NameNode后才可使用，\n\n2.NameNode机器软硬件升级，导致集群不可用\n\n以上两种情况会导致HDFS系统无法使用，为解决以上的问题Hadoop2.0推出了HDFS的高可用（HA）解决方案。\n\n\n\n### 二、HA高可用\n\nHDFS的HA由两个NameNode组成，一个是Active状态，另一个Standby。\n\n所有来自客户端的请求由Active状态的NameNode处理，Standby只同步Active状态NameNode的状态，一旦主节点（Active）发生故障就会借助Zookeeper实现自动的主备选举和切换，这样备用主节点就能立即接替故障节点并提供服务，而用户感觉不到明显的系统中断，使得整个系统更加高效可靠。\n\n共享存储系统即JournalNode集群，是实现NameNode高可用的最关键的部分，它保存了Name Node在运行过程中所产生的HDFS元数据，主备NameNode通过JornalNode实现元数据同步，在切换主备时，新的主NameNode在确认元数据完全同步之后才会提供服务，除此之外还需要共享HDFS数据块和DataNode之间的映射关系，DATaNode向主备NameNode上报数据块位置信息。\n","source":"_posts/BigData/大数据之HDFS.md","raw":"---\ntitle: HDFS高可用之HA架构\ntags:\n  - BigData\n  - HDFS\n  - 大数据\n  - 数据仓库\ncategories:\n  - - BigData\n---\n\n### 一、NameNode故障导致的问题\n\n1.NameNode机器宕机，导致整个集群不可用，重启NameNode后才可使用，\n\n2.NameNode机器软硬件升级，导致集群不可用\n\n以上两种情况会导致HDFS系统无法使用，为解决以上的问题Hadoop2.0推出了HDFS的高可用（HA）解决方案。\n\n\n\n### 二、HA高可用\n\nHDFS的HA由两个NameNode组成，一个是Active状态，另一个Standby。\n\n所有来自客户端的请求由Active状态的NameNode处理，Standby只同步Active状态NameNode的状态，一旦主节点（Active）发生故障就会借助Zookeeper实现自动的主备选举和切换，这样备用主节点就能立即接替故障节点并提供服务，而用户感觉不到明显的系统中断，使得整个系统更加高效可靠。\n\n共享存储系统即JournalNode集群，是实现NameNode高可用的最关键的部分，它保存了Name Node在运行过程中所产生的HDFS元数据，主备NameNode通过JornalNode实现元数据同步，在切换主备时，新的主NameNode在确认元数据完全同步之后才会提供服务，除此之外还需要共享HDFS数据块和DataNode之间的映射关系，DATaNode向主备NameNode上报数据块位置信息。\n","slug":"BigData/大数据之HDFS","published":1,"date":"2024-11-01T12:13:21.890Z","updated":"2024-11-01T12:53:20.506Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cmgu7uye80005jkv2dsbl5v1u","content":"<h3 id=\"一、NameNode故障导致的问题\"><a href=\"#一、NameNode故障导致的问题\" class=\"headerlink\" title=\"一、NameNode故障导致的问题\"></a>一、NameNode故障导致的问题</h3><p>1.NameNode机器宕机，导致整个集群不可用，重启NameNode后才可使用，</p>\n<p>2.NameNode机器软硬件升级，导致集群不可用</p>\n<p>以上两种情况会导致HDFS系统无法使用，为解决以上的问题Hadoop2.0推出了HDFS的高可用（HA）解决方案。</p>\n<h3 id=\"二、HA高可用\"><a href=\"#二、HA高可用\" class=\"headerlink\" title=\"二、HA高可用\"></a>二、HA高可用</h3><p>HDFS的HA由两个NameNode组成，一个是Active状态，另一个Standby。</p>\n<p>所有来自客户端的请求由Active状态的NameNode处理，Standby只同步Active状态NameNode的状态，一旦主节点（Active）发生故障就会借助Zookeeper实现自动的主备选举和切换，这样备用主节点就能立即接替故障节点并提供服务，而用户感觉不到明显的系统中断，使得整个系统更加高效可靠。</p>\n<p>共享存储系统即JournalNode集群，是实现NameNode高可用的最关键的部分，它保存了Name Node在运行过程中所产生的HDFS元数据，主备NameNode通过JornalNode实现元数据同步，在切换主备时，新的主NameNode在确认元数据完全同步之后才会提供服务，除此之外还需要共享HDFS数据块和DataNode之间的映射关系，DATaNode向主备NameNode上报数据块位置信息。</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"一、NameNode故障导致的问题\"><a href=\"#一、NameNode故障导致的问题\" class=\"headerlink\" title=\"一、NameNode故障导致的问题\"></a>一、NameNode故障导致的问题</h3><p>1.NameNode机器宕机，导致整个集群不可用，重启NameNode后才可使用，</p>\n<p>2.NameNode机器软硬件升级，导致集群不可用</p>\n<p>以上两种情况会导致HDFS系统无法使用，为解决以上的问题Hadoop2.0推出了HDFS的高可用（HA）解决方案。</p>\n<h3 id=\"二、HA高可用\"><a href=\"#二、HA高可用\" class=\"headerlink\" title=\"二、HA高可用\"></a>二、HA高可用</h3><p>HDFS的HA由两个NameNode组成，一个是Active状态，另一个Standby。</p>\n<p>所有来自客户端的请求由Active状态的NameNode处理，Standby只同步Active状态NameNode的状态，一旦主节点（Active）发生故障就会借助Zookeeper实现自动的主备选举和切换，这样备用主节点就能立即接替故障节点并提供服务，而用户感觉不到明显的系统中断，使得整个系统更加高效可靠。</p>\n<p>共享存储系统即JournalNode集群，是实现NameNode高可用的最关键的部分，它保存了Name Node在运行过程中所产生的HDFS元数据，主备NameNode通过JornalNode实现元数据同步，在切换主备时，新的主NameNode在确认元数据完全同步之后才会提供服务，除此之外还需要共享HDFS数据块和DataNode之间的映射关系，DATaNode向主备NameNode上报数据块位置信息。</p>\n"},{"title":"数据仓库-分层结构","_content":"\n### 一、ODS层-数据源\n\nODS（Operate Data Store）：对接日志服务器和业务数据库\n\n尽可能不变：\n\n​\t\t1.存储方式不要变\n\n​\t\t2.数据格式不要变\n\n​\t\t3.压缩格式不要变\n\n有些可以变：\n\n​\t\t1.融合异构数据\n\n​\t\t2.汇总不同时间的数据\n\n### 二、DWD层－加工数据\n\nDWD（Data Warehouse Detail）：对ODS层的数据进行加工处理，为了后面的统计分析做准备\n\n\n\n","source":"_posts/BigData/数据仓库-分层结构.md","raw":"---\ntitle: 数据仓库-分层结构\ntag:\n  - 数据仓库\ncategories:\n  - - BigData\n---\n\n### 一、ODS层-数据源\n\nODS（Operate Data Store）：对接日志服务器和业务数据库\n\n尽可能不变：\n\n​\t\t1.存储方式不要变\n\n​\t\t2.数据格式不要变\n\n​\t\t3.压缩格式不要变\n\n有些可以变：\n\n​\t\t1.融合异构数据\n\n​\t\t2.汇总不同时间的数据\n\n### 二、DWD层－加工数据\n\nDWD（Data Warehouse Detail）：对ODS层的数据进行加工处理，为了后面的统计分析做准备\n\n\n\n","slug":"BigData/数据仓库-分层结构","published":1,"date":"2024-09-15T16:08:51.650Z","updated":"2024-10-05T11:37:36.160Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cmgu7uye90007jkv21xkw3noq","content":"<h3 id=\"一、ODS层-数据源\"><a href=\"#一、ODS层-数据源\" class=\"headerlink\" title=\"一、ODS层-数据源\"></a>一、ODS层-数据源</h3><p>ODS（Operate Data Store）：对接日志服务器和业务数据库</p>\n<p>尽可能不变：</p>\n<p>​\t\t1.存储方式不要变</p>\n<p>​\t\t2.数据格式不要变</p>\n<p>​\t\t3.压缩格式不要变</p>\n<p>有些可以变：</p>\n<p>​\t\t1.融合异构数据</p>\n<p>​\t\t2.汇总不同时间的数据</p>\n<h3 id=\"二、DWD层－加工数据\"><a href=\"#二、DWD层－加工数据\" class=\"headerlink\" title=\"二、DWD层－加工数据\"></a>二、DWD层－加工数据</h3><p>DWD（Data Warehouse Detail）：对ODS层的数据进行加工处理，为了后面的统计分析做准备</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"一、ODS层-数据源\"><a href=\"#一、ODS层-数据源\" class=\"headerlink\" title=\"一、ODS层-数据源\"></a>一、ODS层-数据源</h3><p>ODS（Operate Data Store）：对接日志服务器和业务数据库</p>\n<p>尽可能不变：</p>\n<p>​\t\t1.存储方式不要变</p>\n<p>​\t\t2.数据格式不要变</p>\n<p>​\t\t3.压缩格式不要变</p>\n<p>有些可以变：</p>\n<p>​\t\t1.融合异构数据</p>\n<p>​\t\t2.汇总不同时间的数据</p>\n<h3 id=\"二、DWD层－加工数据\"><a href=\"#二、DWD层－加工数据\" class=\"headerlink\" title=\"二、DWD层－加工数据\"></a>二、DWD层－加工数据</h3><p>DWD（Data Warehouse Detail）：对ODS层的数据进行加工处理，为了后面的统计分析做准备</p>\n"},{"title":"深度学习 & pytorch","_content":"### 深度学习 & pytorch\n\n人工神经网络：\n\n* \n\n深度学习-多层感知：\n\n输入层 -->  隐藏层 -->  输出层\n\n机器学习和深度学习最大区别：\n\n* 机器学习：人工提取特征\n\n* 深度学习：机器提取\n\n  ![image-20251012202236295](../../img/image-20251012202236295.png)\n\n#### 机器学习历史：\n\n* 计算机科学之父 ：图灵\n* 信息论之父： 克劳德·香农\n* 人工智能之父： 约翰·麦卡锡\n* 人工智能元年： 1956年 \n\n#### 环境配置小贴士\n\n* 建议使用 conda powershell prompt , 图形化界面容易卡顿\n* 1. conda create -n 环境名 python=版本号    -- 创建虚拟环境\n  2. conda activate 环境名    --切换环境\n  3. conda deactivate 环境名     --切回base环境\n\n### Pytorch\n\n* 概念：\n\n  数据以张量类型存储\n\n  | 0维  | 1维  | 2维  | 3维      | ...  |\n  | ---- | ---- | ---- | -------- | ---- |\n  | 标量 | 矢量 | 矩阵 | 矩阵数组 |      |\n\n  \n\n##### 创建张量：\n\n* torch.tensor()    --指定数据和类型\n* torch.Tensor()   --指定数据大小[行，列]和内容\n* torch.IntTensor()   --指定类型直接创建\n\n##### 线性和随机张量\n\n* torch.linspace()   -- 根据步长大小生成步长个值的指定范围张量\n* torch.arrange()   --根据步长跳指定大小的值生成指定范围的张量\n* torch.rand()        --生成指定大小的随机张量\n* torch.randn()      -- 生成指定大小的正太分布张量\n* torch.Tensor()      --生成指定大小的随机张量\n\n##### 随机种子查询和设置\n\n* torch.initial_seed()     --查询当前随机种子的值\n* torch.manual_seed()     -- 设置随机种子的值\n\n##### 0/1/指定值张量\n\n* torch.ones()       -- 创建指定形状的值为1张量\n* torch.zeros()      -- 创建指定形状的值为0张量\n* torch.full()          -- 创建指定形状的值为指定值的张量\n* torch.ones_like()     -- 创建张量变量形状的值为1张量\n* torch.zeros_like()     --创建张量变量形状的值为0张量\n* torch.full_like()       --创建张量变量形状的指定值的张量\n\n##### 张量中元素类型以及转换操作\n\n* 张量.float()\n* 张量.int()\n* 张量.long()\n* 张量.short()\n* 张量.half()\n* 张量.double()\n* True/False 1/0\n* 张量.type(dtype=torch.小写类型名)\n* 张量.type(dtype=torch.int16/32/64/float16/32/64)\n\n* 张量.type(dtype=torch.大写类型Tensor)\n\n##### tensor和numpy互转\n\n* 张量转换为numpy   \n\n  张量.numpy()        --共享内存\n\n  张量.numpy().copy()    --不共享 \n\n  \n\n  numpy转换为张量\n\n  torch.from_numpy(ndarray)     --共享\n\n  torch.tensor(ndarray)          --不共享\n\n  torch.tensor(标量)  --单个标量转张量\n\n  tensor.item()       \n\n  ","source":"_posts/DeepLearning/DeepLearningDay1.md","raw":"---\ntitle: 深度学习 & pytorch\ncategories:\n  - - DeepLearning\n---\n### 深度学习 & pytorch\n\n人工神经网络：\n\n* \n\n深度学习-多层感知：\n\n输入层 -->  隐藏层 -->  输出层\n\n机器学习和深度学习最大区别：\n\n* 机器学习：人工提取特征\n\n* 深度学习：机器提取\n\n  ![image-20251012202236295](../../img/image-20251012202236295.png)\n\n#### 机器学习历史：\n\n* 计算机科学之父 ：图灵\n* 信息论之父： 克劳德·香农\n* 人工智能之父： 约翰·麦卡锡\n* 人工智能元年： 1956年 \n\n#### 环境配置小贴士\n\n* 建议使用 conda powershell prompt , 图形化界面容易卡顿\n* 1. conda create -n 环境名 python=版本号    -- 创建虚拟环境\n  2. conda activate 环境名    --切换环境\n  3. conda deactivate 环境名     --切回base环境\n\n### Pytorch\n\n* 概念：\n\n  数据以张量类型存储\n\n  | 0维  | 1维  | 2维  | 3维      | ...  |\n  | ---- | ---- | ---- | -------- | ---- |\n  | 标量 | 矢量 | 矩阵 | 矩阵数组 |      |\n\n  \n\n##### 创建张量：\n\n* torch.tensor()    --指定数据和类型\n* torch.Tensor()   --指定数据大小[行，列]和内容\n* torch.IntTensor()   --指定类型直接创建\n\n##### 线性和随机张量\n\n* torch.linspace()   -- 根据步长大小生成步长个值的指定范围张量\n* torch.arrange()   --根据步长跳指定大小的值生成指定范围的张量\n* torch.rand()        --生成指定大小的随机张量\n* torch.randn()      -- 生成指定大小的正太分布张量\n* torch.Tensor()      --生成指定大小的随机张量\n\n##### 随机种子查询和设置\n\n* torch.initial_seed()     --查询当前随机种子的值\n* torch.manual_seed()     -- 设置随机种子的值\n\n##### 0/1/指定值张量\n\n* torch.ones()       -- 创建指定形状的值为1张量\n* torch.zeros()      -- 创建指定形状的值为0张量\n* torch.full()          -- 创建指定形状的值为指定值的张量\n* torch.ones_like()     -- 创建张量变量形状的值为1张量\n* torch.zeros_like()     --创建张量变量形状的值为0张量\n* torch.full_like()       --创建张量变量形状的指定值的张量\n\n##### 张量中元素类型以及转换操作\n\n* 张量.float()\n* 张量.int()\n* 张量.long()\n* 张量.short()\n* 张量.half()\n* 张量.double()\n* True/False 1/0\n* 张量.type(dtype=torch.小写类型名)\n* 张量.type(dtype=torch.int16/32/64/float16/32/64)\n\n* 张量.type(dtype=torch.大写类型Tensor)\n\n##### tensor和numpy互转\n\n* 张量转换为numpy   \n\n  张量.numpy()        --共享内存\n\n  张量.numpy().copy()    --不共享 \n\n  \n\n  numpy转换为张量\n\n  torch.from_numpy(ndarray)     --共享\n\n  torch.tensor(ndarray)          --不共享\n\n  torch.tensor(标量)  --单个标量转张量\n\n  tensor.item()       \n\n  ","slug":"DeepLearning/DeepLearningDay1","published":1,"date":"2025-10-12T02:17:40.924Z","updated":"2025-10-12T13:00:39.793Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cmgu7uyea0009jkv2b4ox8k9r","content":"<h3 id=\"深度学习-pytorch\"><a href=\"#深度学习-pytorch\" class=\"headerlink\" title=\"深度学习 &amp; pytorch\"></a>深度学习 &amp; pytorch</h3><p>人工神经网络：</p>\n<p>* </p>\n<p>深度学习-多层感知：</p>\n<p>输入层 –&gt;  隐藏层 –&gt;  输出层</p>\n<p>机器学习和深度学习最大区别：</p>\n<ul>\n<li><p>机器学习：人工提取特征</p>\n</li>\n<li><p>深度学习：机器提取</p>\n<p><img src=\"/../../img/image-20251012202236295.png\" alt=\"image-20251012202236295\"></p>\n</li>\n</ul>\n<h4 id=\"机器学习历史：\"><a href=\"#机器学习历史：\" class=\"headerlink\" title=\"机器学习历史：\"></a>机器学习历史：</h4><ul>\n<li>计算机科学之父 ：图灵</li>\n<li>信息论之父： 克劳德·香农</li>\n<li>人工智能之父： 约翰·麦卡锡</li>\n<li>人工智能元年： 1956年</li>\n</ul>\n<h4 id=\"环境配置小贴士\"><a href=\"#环境配置小贴士\" class=\"headerlink\" title=\"环境配置小贴士\"></a>环境配置小贴士</h4><ul>\n<li>建议使用 conda powershell prompt , 图形化界面容易卡顿</li>\n<li><ol>\n<li>conda create -n 环境名 python&#x3D;版本号    – 创建虚拟环境</li>\n<li>conda activate 环境名    –切换环境</li>\n<li>conda deactivate 环境名     –切回base环境</li>\n</ol>\n</li>\n</ul>\n<h3 id=\"Pytorch\"><a href=\"#Pytorch\" class=\"headerlink\" title=\"Pytorch\"></a>Pytorch</h3><ul>\n<li><p>概念：</p>\n<p>数据以张量类型存储</p>\n<table>\n<thead>\n<tr>\n<th>0维</th>\n<th>1维</th>\n<th>2维</th>\n<th>3维</th>\n<th>…</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>标量</td>\n<td>矢量</td>\n<td>矩阵</td>\n<td>矩阵数组</td>\n<td></td>\n</tr>\n</tbody></table>\n</li>\n</ul>\n<h5 id=\"创建张量：\"><a href=\"#创建张量：\" class=\"headerlink\" title=\"创建张量：\"></a>创建张量：</h5><ul>\n<li>torch.tensor()    –指定数据和类型</li>\n<li>torch.Tensor()   –指定数据大小[行，列]和内容</li>\n<li>torch.IntTensor()   –指定类型直接创建</li>\n</ul>\n<h5 id=\"线性和随机张量\"><a href=\"#线性和随机张量\" class=\"headerlink\" title=\"线性和随机张量\"></a>线性和随机张量</h5><ul>\n<li>torch.linspace()   – 根据步长大小生成步长个值的指定范围张量</li>\n<li>torch.arrange()   –根据步长跳指定大小的值生成指定范围的张量</li>\n<li>torch.rand()        –生成指定大小的随机张量</li>\n<li>torch.randn()      – 生成指定大小的正太分布张量</li>\n<li>torch.Tensor()      –生成指定大小的随机张量</li>\n</ul>\n<h5 id=\"随机种子查询和设置\"><a href=\"#随机种子查询和设置\" class=\"headerlink\" title=\"随机种子查询和设置\"></a>随机种子查询和设置</h5><ul>\n<li>torch.initial_seed()     –查询当前随机种子的值</li>\n<li>torch.manual_seed()     – 设置随机种子的值</li>\n</ul>\n<h5 id=\"0-1-指定值张量\"><a href=\"#0-1-指定值张量\" class=\"headerlink\" title=\"0&#x2F;1&#x2F;指定值张量\"></a>0&#x2F;1&#x2F;指定值张量</h5><ul>\n<li>torch.ones()       – 创建指定形状的值为1张量</li>\n<li>torch.zeros()      – 创建指定形状的值为0张量</li>\n<li>torch.full()          – 创建指定形状的值为指定值的张量</li>\n<li>torch.ones_like()     – 创建张量变量形状的值为1张量</li>\n<li>torch.zeros_like()     –创建张量变量形状的值为0张量</li>\n<li>torch.full_like()       –创建张量变量形状的指定值的张量</li>\n</ul>\n<h5 id=\"张量中元素类型以及转换操作\"><a href=\"#张量中元素类型以及转换操作\" class=\"headerlink\" title=\"张量中元素类型以及转换操作\"></a>张量中元素类型以及转换操作</h5><ul>\n<li><p>张量.float()</p>\n</li>\n<li><p>张量.int()</p>\n</li>\n<li><p>张量.long()</p>\n</li>\n<li><p>张量.short()</p>\n</li>\n<li><p>张量.half()</p>\n</li>\n<li><p>张量.double()</p>\n</li>\n<li><p>True&#x2F;False 1&#x2F;0</p>\n</li>\n<li><p>张量.type(dtype&#x3D;torch.小写类型名)</p>\n</li>\n<li><p>张量.type(dtype&#x3D;torch.int16&#x2F;32&#x2F;64&#x2F;float16&#x2F;32&#x2F;64)</p>\n</li>\n<li><p>张量.type(dtype&#x3D;torch.大写类型Tensor)</p>\n</li>\n</ul>\n<h5 id=\"tensor和numpy互转\"><a href=\"#tensor和numpy互转\" class=\"headerlink\" title=\"tensor和numpy互转\"></a>tensor和numpy互转</h5><ul>\n<li><p>张量转换为numpy   </p>\n<p>张量.numpy()        –共享内存</p>\n<p>张量.numpy().copy()    –不共享 </p>\n<p>numpy转换为张量</p>\n<p>torch.from_numpy(ndarray)     –共享</p>\n<p>torch.tensor(ndarray)          –不共享</p>\n<p>torch.tensor(标量)  –单个标量转张量</p>\n<p>tensor.item()</p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"深度学习-pytorch\"><a href=\"#深度学习-pytorch\" class=\"headerlink\" title=\"深度学习 &amp; pytorch\"></a>深度学习 &amp; pytorch</h3><p>人工神经网络：</p>\n<p>* </p>\n<p>深度学习-多层感知：</p>\n<p>输入层 –&gt;  隐藏层 –&gt;  输出层</p>\n<p>机器学习和深度学习最大区别：</p>\n<ul>\n<li><p>机器学习：人工提取特征</p>\n</li>\n<li><p>深度学习：机器提取</p>\n<p><img src=\"/../../img/image-20251012202236295.png\" alt=\"image-20251012202236295\"></p>\n</li>\n</ul>\n<h4 id=\"机器学习历史：\"><a href=\"#机器学习历史：\" class=\"headerlink\" title=\"机器学习历史：\"></a>机器学习历史：</h4><ul>\n<li>计算机科学之父 ：图灵</li>\n<li>信息论之父： 克劳德·香农</li>\n<li>人工智能之父： 约翰·麦卡锡</li>\n<li>人工智能元年： 1956年</li>\n</ul>\n<h4 id=\"环境配置小贴士\"><a href=\"#环境配置小贴士\" class=\"headerlink\" title=\"环境配置小贴士\"></a>环境配置小贴士</h4><ul>\n<li>建议使用 conda powershell prompt , 图形化界面容易卡顿</li>\n<li><ol>\n<li>conda create -n 环境名 python&#x3D;版本号    – 创建虚拟环境</li>\n<li>conda activate 环境名    –切换环境</li>\n<li>conda deactivate 环境名     –切回base环境</li>\n</ol>\n</li>\n</ul>\n<h3 id=\"Pytorch\"><a href=\"#Pytorch\" class=\"headerlink\" title=\"Pytorch\"></a>Pytorch</h3><ul>\n<li><p>概念：</p>\n<p>数据以张量类型存储</p>\n<table>\n<thead>\n<tr>\n<th>0维</th>\n<th>1维</th>\n<th>2维</th>\n<th>3维</th>\n<th>…</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>标量</td>\n<td>矢量</td>\n<td>矩阵</td>\n<td>矩阵数组</td>\n<td></td>\n</tr>\n</tbody></table>\n</li>\n</ul>\n<h5 id=\"创建张量：\"><a href=\"#创建张量：\" class=\"headerlink\" title=\"创建张量：\"></a>创建张量：</h5><ul>\n<li>torch.tensor()    –指定数据和类型</li>\n<li>torch.Tensor()   –指定数据大小[行，列]和内容</li>\n<li>torch.IntTensor()   –指定类型直接创建</li>\n</ul>\n<h5 id=\"线性和随机张量\"><a href=\"#线性和随机张量\" class=\"headerlink\" title=\"线性和随机张量\"></a>线性和随机张量</h5><ul>\n<li>torch.linspace()   – 根据步长大小生成步长个值的指定范围张量</li>\n<li>torch.arrange()   –根据步长跳指定大小的值生成指定范围的张量</li>\n<li>torch.rand()        –生成指定大小的随机张量</li>\n<li>torch.randn()      – 生成指定大小的正太分布张量</li>\n<li>torch.Tensor()      –生成指定大小的随机张量</li>\n</ul>\n<h5 id=\"随机种子查询和设置\"><a href=\"#随机种子查询和设置\" class=\"headerlink\" title=\"随机种子查询和设置\"></a>随机种子查询和设置</h5><ul>\n<li>torch.initial_seed()     –查询当前随机种子的值</li>\n<li>torch.manual_seed()     – 设置随机种子的值</li>\n</ul>\n<h5 id=\"0-1-指定值张量\"><a href=\"#0-1-指定值张量\" class=\"headerlink\" title=\"0&#x2F;1&#x2F;指定值张量\"></a>0&#x2F;1&#x2F;指定值张量</h5><ul>\n<li>torch.ones()       – 创建指定形状的值为1张量</li>\n<li>torch.zeros()      – 创建指定形状的值为0张量</li>\n<li>torch.full()          – 创建指定形状的值为指定值的张量</li>\n<li>torch.ones_like()     – 创建张量变量形状的值为1张量</li>\n<li>torch.zeros_like()     –创建张量变量形状的值为0张量</li>\n<li>torch.full_like()       –创建张量变量形状的指定值的张量</li>\n</ul>\n<h5 id=\"张量中元素类型以及转换操作\"><a href=\"#张量中元素类型以及转换操作\" class=\"headerlink\" title=\"张量中元素类型以及转换操作\"></a>张量中元素类型以及转换操作</h5><ul>\n<li><p>张量.float()</p>\n</li>\n<li><p>张量.int()</p>\n</li>\n<li><p>张量.long()</p>\n</li>\n<li><p>张量.short()</p>\n</li>\n<li><p>张量.half()</p>\n</li>\n<li><p>张量.double()</p>\n</li>\n<li><p>True&#x2F;False 1&#x2F;0</p>\n</li>\n<li><p>张量.type(dtype&#x3D;torch.小写类型名)</p>\n</li>\n<li><p>张量.type(dtype&#x3D;torch.int16&#x2F;32&#x2F;64&#x2F;float16&#x2F;32&#x2F;64)</p>\n</li>\n<li><p>张量.type(dtype&#x3D;torch.大写类型Tensor)</p>\n</li>\n</ul>\n<h5 id=\"tensor和numpy互转\"><a href=\"#tensor和numpy互转\" class=\"headerlink\" title=\"tensor和numpy互转\"></a>tensor和numpy互转</h5><ul>\n<li><p>张量转换为numpy   </p>\n<p>张量.numpy()        –共享内存</p>\n<p>张量.numpy().copy()    –不共享 </p>\n<p>numpy转换为张量</p>\n<p>torch.from_numpy(ndarray)     –共享</p>\n<p>torch.tensor(ndarray)          –不共享</p>\n<p>torch.tensor(标量)  –单个标量转张量</p>\n<p>tensor.item()</p>\n</li>\n</ul>\n"},{"title":"下载Apple Music歌曲|解密ALAC","_content":"\n需要用到的工具：\n\n -安装 Go https://go.dev/dl/go1.22.1.windows-arm64.msi\n\n -安装 Python https://www.python.org/ftp/python/3.12.3-amd64.exe \n\n-安装 frida\n\n​\t `在cmd中输入“pip install frida-tools” `\n\n-下载 adb https://developer.android.com/tools/releases/platfrom-tools?hl=zh-cn#downloads\n\n 解压下载后的压缩包 \n\n-安装 MP4Box https://download/tsi.telecim-paristech.fr/gpac/new_builds/gpac_latest_head_win64.exe \n\n-安装 Android Studio https://developer.android.com/studio?hl=zh-cn 全部选择下一步即可 \n\n-下载 SAI 应用安装器 安装包 https://f-droid.org/repo/com.aefyr.sai.fdroid_60.apk \n\n-下载 Apple Music 安装包 https://www.apkmirror.com/apk/apple/apple-music/apple-music-3-6-0-beta-release/apple-music-3-6-0-beta-4-android-apk-download/ \n\n-下载 frida 服务端 https://github.com/frida/frida/releases/download/16.2.1/frida-server-16.2.1-android-x86_64.xz \n\n解压下载后的压缩包\n\n -下载 alac 解密文件 https://github.com/zhaarey/apple-music-alac-atmos-downloader/archive/refs/heads/main.zip \n\n解压下载后的压缩包 所有下载的东西必须全英文路径且不包含空格！！！\n\n> 转载BiliBili@[简纸Paper](https://space.bilibili.com/355150816)\n>\n> ","source":"_posts/hack/新建 文本.md","raw":"---\ntitle: 下载Apple Music歌曲|解密ALAC\ntags: ['hack', 'apple music']\ncategories:\n  - - hack\n---\n\n需要用到的工具：\n\n -安装 Go https://go.dev/dl/go1.22.1.windows-arm64.msi\n\n -安装 Python https://www.python.org/ftp/python/3.12.3-amd64.exe \n\n-安装 frida\n\n​\t `在cmd中输入“pip install frida-tools” `\n\n-下载 adb https://developer.android.com/tools/releases/platfrom-tools?hl=zh-cn#downloads\n\n 解压下载后的压缩包 \n\n-安装 MP4Box https://download/tsi.telecim-paristech.fr/gpac/new_builds/gpac_latest_head_win64.exe \n\n-安装 Android Studio https://developer.android.com/studio?hl=zh-cn 全部选择下一步即可 \n\n-下载 SAI 应用安装器 安装包 https://f-droid.org/repo/com.aefyr.sai.fdroid_60.apk \n\n-下载 Apple Music 安装包 https://www.apkmirror.com/apk/apple/apple-music/apple-music-3-6-0-beta-release/apple-music-3-6-0-beta-4-android-apk-download/ \n\n-下载 frida 服务端 https://github.com/frida/frida/releases/download/16.2.1/frida-server-16.2.1-android-x86_64.xz \n\n解压下载后的压缩包\n\n -下载 alac 解密文件 https://github.com/zhaarey/apple-music-alac-atmos-downloader/archive/refs/heads/main.zip \n\n解压下载后的压缩包 所有下载的东西必须全英文路径且不包含空格！！！\n\n> 转载BiliBili@[简纸Paper](https://space.bilibili.com/355150816)\n>\n> ","slug":"hack/新建 文本","published":1,"date":"2024-10-06T11:16:42.784Z","updated":"2024-10-06T12:13:38.864Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cmgu7uyeb000ajkv256t6d5ub","content":"<p>需要用到的工具：</p>\n<p> -安装 Go <a href=\"https://go.dev/dl/go1.22.1.windows-arm64.msi\">https://go.dev/dl/go1.22.1.windows-arm64.msi</a></p>\n<p> -安装 Python <a href=\"https://www.python.org/ftp/python/3.12.3-amd64.exe\">https://www.python.org/ftp/python/3.12.3-amd64.exe</a> </p>\n<p>-安装 frida</p>\n<p>​\t <code>在cmd中输入“pip install frida-tools” </code></p>\n<p>-下载 adb <a href=\"https://developer.android.com/tools/releases/platfrom-tools?hl=zh-cn#downloads\">https://developer.android.com/tools/releases/platfrom-tools?hl=zh-cn#downloads</a></p>\n<p> 解压下载后的压缩包 </p>\n<p>-安装 MP4Box <a href=\"https://download/tsi.telecim-paristech.fr/gpac/new_builds/gpac_latest_head_win64.exe\">https://download/tsi.telecim-paristech.fr/gpac/new_builds/gpac_latest_head_win64.exe</a> </p>\n<p>-安装 Android Studio <a href=\"https://developer.android.com/studio?hl=zh-cn\">https://developer.android.com/studio?hl=zh-cn</a> 全部选择下一步即可 </p>\n<p>-下载 SAI 应用安装器 安装包 <a href=\"https://f-droid.org/repo/com.aefyr.sai.fdroid_60.apk\">https://f-droid.org/repo/com.aefyr.sai.fdroid_60.apk</a> </p>\n<p>-下载 Apple Music 安装包 <a href=\"https://www.apkmirror.com/apk/apple/apple-music/apple-music-3-6-0-beta-release/apple-music-3-6-0-beta-4-android-apk-download/\">https://www.apkmirror.com/apk/apple/apple-music/apple-music-3-6-0-beta-release/apple-music-3-6-0-beta-4-android-apk-download/</a> </p>\n<p>-下载 frida 服务端 <a href=\"https://github.com/frida/frida/releases/download/16.2.1/frida-server-16.2.1-android-x86_64.xz\">https://github.com/frida/frida/releases/download/16.2.1/frida-server-16.2.1-android-x86_64.xz</a> </p>\n<p>解压下载后的压缩包</p>\n<p> -下载 alac 解密文件 <a href=\"https://github.com/zhaarey/apple-music-alac-atmos-downloader/archive/refs/heads/main.zip\">https://github.com/zhaarey/apple-music-alac-atmos-downloader/archive/refs/heads/main.zip</a> </p>\n<p>解压下载后的压缩包 所有下载的东西必须全英文路径且不包含空格！！！</p>\n<blockquote>\n<p>转载BiliBili@<a href=\"https://space.bilibili.com/355150816\">简纸Paper</a></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<p>需要用到的工具：</p>\n<p> -安装 Go <a href=\"https://go.dev/dl/go1.22.1.windows-arm64.msi\">https://go.dev/dl/go1.22.1.windows-arm64.msi</a></p>\n<p> -安装 Python <a href=\"https://www.python.org/ftp/python/3.12.3-amd64.exe\">https://www.python.org/ftp/python/3.12.3-amd64.exe</a> </p>\n<p>-安装 frida</p>\n<p>​\t <code>在cmd中输入“pip install frida-tools” </code></p>\n<p>-下载 adb <a href=\"https://developer.android.com/tools/releases/platfrom-tools?hl=zh-cn#downloads\">https://developer.android.com/tools/releases/platfrom-tools?hl=zh-cn#downloads</a></p>\n<p> 解压下载后的压缩包 </p>\n<p>-安装 MP4Box <a href=\"https://download/tsi.telecim-paristech.fr/gpac/new_builds/gpac_latest_head_win64.exe\">https://download/tsi.telecim-paristech.fr/gpac/new_builds/gpac_latest_head_win64.exe</a> </p>\n<p>-安装 Android Studio <a href=\"https://developer.android.com/studio?hl=zh-cn\">https://developer.android.com/studio?hl=zh-cn</a> 全部选择下一步即可 </p>\n<p>-下载 SAI 应用安装器 安装包 <a href=\"https://f-droid.org/repo/com.aefyr.sai.fdroid_60.apk\">https://f-droid.org/repo/com.aefyr.sai.fdroid_60.apk</a> </p>\n<p>-下载 Apple Music 安装包 <a href=\"https://www.apkmirror.com/apk/apple/apple-music/apple-music-3-6-0-beta-release/apple-music-3-6-0-beta-4-android-apk-download/\">https://www.apkmirror.com/apk/apple/apple-music/apple-music-3-6-0-beta-release/apple-music-3-6-0-beta-4-android-apk-download/</a> </p>\n<p>-下载 frida 服务端 <a href=\"https://github.com/frida/frida/releases/download/16.2.1/frida-server-16.2.1-android-x86_64.xz\">https://github.com/frida/frida/releases/download/16.2.1/frida-server-16.2.1-android-x86_64.xz</a> </p>\n<p>解压下载后的压缩包</p>\n<p> -下载 alac 解密文件 <a href=\"https://github.com/zhaarey/apple-music-alac-atmos-downloader/archive/refs/heads/main.zip\">https://github.com/zhaarey/apple-music-alac-atmos-downloader/archive/refs/heads/main.zip</a> </p>\n<p>解压下载后的压缩包 所有下载的东西必须全英文路径且不包含空格！！！</p>\n<blockquote>\n<p>转载BiliBili@<a href=\"https://space.bilibili.com/355150816\">简纸Paper</a></p>\n</blockquote>\n"},{"title":"pytorch 02","_content":"### Pytorch\n\n#### 张量的运算\n\nadd , sub, mul, div ,neg  -- 加减乘除\n\nadd_ , sub__, mul_, div_ ,neg_     -- 带下划线赋值生效\n\n或直接用符号(+ - * / )\n\n向量之间相乘条件: shape: [n,m]  * shape: [m, p]  = shape:[n,p]\n\n`向量1 @ 向量2`     -- 形状必须一致且符合相乘的条件元素之间才能相乘\n\n`torch.matmul(向量1，向量2) `   -- 形状可以不一致，但是必须符合相乘的条件\n\n#### 计算函数\n\n`torch.mean()`\n\n`torch.sqrt()`    -- 平方根\n\n`torch.sum()`\n\n`torch.exp() `    -- 指数函数\n\n`torch.log()`\n\n`torch.log2()`\n\n`torch.log10()`\n\n`torch.pow()  `   --幂\n\n\n\n#### 张量的索引\n\n* 张量.[行，列]\n* 张量.[[行],[列]]       -- 坐标轴：返回一个具体张量元素\n* 张量.[[行1，行2],[列1，列2]]     -- 返回两个值，坐标轴为：【行1，列1】【行2，列2】 \n* 张量.[行x，张量[行y] < z]         --布尔索引  y行返回小于z的 x行元素\n* 张量[1，1，1，2，3]     -- 多维索引    返回多维里2行3列的张量元素\n\n#### 张量形状操作\n\n##### `reshape()`\n\n最常用的改变形状函数\n\n`张量.reshape() `       --改变形状 ，保证数据不会损失\n\n![image-20251014212502665](../../img/image-20251014212502665.png)\n\n##### `squeeze() ` &  `unsqueeze()`\n\n降维函数 & 升维函数\n\n![image-20251014212255259](../../img/image-20251014212255259.png)\n\n##### `transpose() `& `permute()`\n\ntranspose:  交换张量形状/转置，将二行三列转为三行二列，一行四列转为四行一列等等...\n\npermute:  转置，一次可以转多个维度\n\n![image-20251014213043601](../../img/image-20251014213043601.png)\n\n##### `view()` & `contiguous()`\n\nview也是修改形状，只能修改连续的张量（内存地址中的连续）。\n\n**内存连续 (Contiguous)：** \n\n张量中**逻辑上相邻**的元素（通常按照行主序，即 C/Python/PyTorch 的默认存储方式），在内存地址上也是**紧密相邻**的。如果从内存起点依次读取，就能完整且按序地读出整个张量。\n\n![image-20251014213546574](../../img/image-20251014213546574.png)\n\n**转置示例：**\n\n如果将 T 转置为 T′，逻辑上 T′ 的元素顺序是 **[1,4,2,5,3,6]**。但是，在内存中，它仍然是 T 的原始存储：**[1,2,3,4,5,6]**。\n\n此时，要读取 T′ 逻辑上的第二个元素（4），程序需要跳过（或“跨步”）内存地址，因此 T′ 是**不连续的**。\n\n\n\n**如何破坏连续性？**\n\n一些张量操作会改变元素的**逻辑顺序**（新的形状/维度顺序），但**不会**改变底层数据在内存中的**物理排列**。这些操作会创建**不连续的张量视图**：\n\n- **`transpose()` (转置)**\n- **`permute()` (维度重排)**\n- **非连续切片** (例如，`tensor[:, ::2]`)\n\n\n\n**恢复连续性**\n\n- 使用 `tensor.contiguous()` 函数。\n- 该函数会检查张量是否连续。如果不连续，它会**创建一个新的张量**，并将原始数据按逻辑顺序**复制**到新张量的连续内存块中，从而恢复连续性。这个操作会消耗时间和内存。\n\n\n\n**为什么连续性很重要？**\n\n内存连续性是高性能计算的关键：\n\n- **利用缓存优势：** 当 CPU 或 GPU 访问一个连续张量的第一个元素时，由于局部性原理，系统会预先将后续相邻的元素加载到高速缓存中。这使得后续的数据访问速度大大提高。\n- **底层优化：** 许多高性能库（如 BLAS、cuDNN）和 PyTorch 的高效操作（如 `view()`）都要求输入张量是连续的，因为这样可以避免复杂的索引计算，直接通过内存指针操作数据。\n\n#### 张量拼接操作\n\n##### `cat()` &  `stack()`\n\n`torch.cat(data1,data2)`      -- 拼接两个张量数据， 不改变维数\n\n`torch.stack(data1,data2)`     -- 增加一个新维度，在内连接一列张量，输入的张量形状必须完全一致\n\n![image-20251014214735283](../../img/image-20251014214735283.png)\n\n![image-20251014214810748](../../img/image-20251014214810748.png)\n\n#### 自动微分模块\n\n**训练神经网络**时，最常用的算法就是**反向传播**。在该算法中，参数(模型权重)会根据损失函数关于对应参数的梯度进行调整。为了计算这些梯度，PyTorch内置了名为 `torch.autograd` 的微分模块。它支持任意计算图的自动梯度计算:\n\n![image-20251014215132441](../../img/image-20251014215132441.png)\n\n使用`torch.tensor(data,requires_grad=True)`创建张量时将require_grad设为True即可\n\n* Pytorch 不支持向量张量对向量张量的求导，只支持标量张量对向量张量的求导\n  * x如果是张量，y必须是标量（1个值）才能进行求导\n* 计算梯度:\n  * `y.backward()`    **--y是一个标量**\n* 获取x点的梯度值：\n  * `x.grad`    **--会累加上一次的梯度值**\n\n\n\n##### 梯度下降法最优解\n\n**梯度下降公式：**  w = w - r * grad      (r=学习率 ， grad=梯度值)\n\n清空上次的梯度值：  `x.grad.zero_()`\n\n![image-20251014215956562](../../img/image-20251014215956562.png)\n","source":"_posts/DeepLearning/DeepLearningDay2.md","raw":"---\ntitle: pytorch 02\ncategories:\n  - - DeepLearning\n---\n### Pytorch\n\n#### 张量的运算\n\nadd , sub, mul, div ,neg  -- 加减乘除\n\nadd_ , sub__, mul_, div_ ,neg_     -- 带下划线赋值生效\n\n或直接用符号(+ - * / )\n\n向量之间相乘条件: shape: [n,m]  * shape: [m, p]  = shape:[n,p]\n\n`向量1 @ 向量2`     -- 形状必须一致且符合相乘的条件元素之间才能相乘\n\n`torch.matmul(向量1，向量2) `   -- 形状可以不一致，但是必须符合相乘的条件\n\n#### 计算函数\n\n`torch.mean()`\n\n`torch.sqrt()`    -- 平方根\n\n`torch.sum()`\n\n`torch.exp() `    -- 指数函数\n\n`torch.log()`\n\n`torch.log2()`\n\n`torch.log10()`\n\n`torch.pow()  `   --幂\n\n\n\n#### 张量的索引\n\n* 张量.[行，列]\n* 张量.[[行],[列]]       -- 坐标轴：返回一个具体张量元素\n* 张量.[[行1，行2],[列1，列2]]     -- 返回两个值，坐标轴为：【行1，列1】【行2，列2】 \n* 张量.[行x，张量[行y] < z]         --布尔索引  y行返回小于z的 x行元素\n* 张量[1，1，1，2，3]     -- 多维索引    返回多维里2行3列的张量元素\n\n#### 张量形状操作\n\n##### `reshape()`\n\n最常用的改变形状函数\n\n`张量.reshape() `       --改变形状 ，保证数据不会损失\n\n![image-20251014212502665](../../img/image-20251014212502665.png)\n\n##### `squeeze() ` &  `unsqueeze()`\n\n降维函数 & 升维函数\n\n![image-20251014212255259](../../img/image-20251014212255259.png)\n\n##### `transpose() `& `permute()`\n\ntranspose:  交换张量形状/转置，将二行三列转为三行二列，一行四列转为四行一列等等...\n\npermute:  转置，一次可以转多个维度\n\n![image-20251014213043601](../../img/image-20251014213043601.png)\n\n##### `view()` & `contiguous()`\n\nview也是修改形状，只能修改连续的张量（内存地址中的连续）。\n\n**内存连续 (Contiguous)：** \n\n张量中**逻辑上相邻**的元素（通常按照行主序，即 C/Python/PyTorch 的默认存储方式），在内存地址上也是**紧密相邻**的。如果从内存起点依次读取，就能完整且按序地读出整个张量。\n\n![image-20251014213546574](../../img/image-20251014213546574.png)\n\n**转置示例：**\n\n如果将 T 转置为 T′，逻辑上 T′ 的元素顺序是 **[1,4,2,5,3,6]**。但是，在内存中，它仍然是 T 的原始存储：**[1,2,3,4,5,6]**。\n\n此时，要读取 T′ 逻辑上的第二个元素（4），程序需要跳过（或“跨步”）内存地址，因此 T′ 是**不连续的**。\n\n\n\n**如何破坏连续性？**\n\n一些张量操作会改变元素的**逻辑顺序**（新的形状/维度顺序），但**不会**改变底层数据在内存中的**物理排列**。这些操作会创建**不连续的张量视图**：\n\n- **`transpose()` (转置)**\n- **`permute()` (维度重排)**\n- **非连续切片** (例如，`tensor[:, ::2]`)\n\n\n\n**恢复连续性**\n\n- 使用 `tensor.contiguous()` 函数。\n- 该函数会检查张量是否连续。如果不连续，它会**创建一个新的张量**，并将原始数据按逻辑顺序**复制**到新张量的连续内存块中，从而恢复连续性。这个操作会消耗时间和内存。\n\n\n\n**为什么连续性很重要？**\n\n内存连续性是高性能计算的关键：\n\n- **利用缓存优势：** 当 CPU 或 GPU 访问一个连续张量的第一个元素时，由于局部性原理，系统会预先将后续相邻的元素加载到高速缓存中。这使得后续的数据访问速度大大提高。\n- **底层优化：** 许多高性能库（如 BLAS、cuDNN）和 PyTorch 的高效操作（如 `view()`）都要求输入张量是连续的，因为这样可以避免复杂的索引计算，直接通过内存指针操作数据。\n\n#### 张量拼接操作\n\n##### `cat()` &  `stack()`\n\n`torch.cat(data1,data2)`      -- 拼接两个张量数据， 不改变维数\n\n`torch.stack(data1,data2)`     -- 增加一个新维度，在内连接一列张量，输入的张量形状必须完全一致\n\n![image-20251014214735283](../../img/image-20251014214735283.png)\n\n![image-20251014214810748](../../img/image-20251014214810748.png)\n\n#### 自动微分模块\n\n**训练神经网络**时，最常用的算法就是**反向传播**。在该算法中，参数(模型权重)会根据损失函数关于对应参数的梯度进行调整。为了计算这些梯度，PyTorch内置了名为 `torch.autograd` 的微分模块。它支持任意计算图的自动梯度计算:\n\n![image-20251014215132441](../../img/image-20251014215132441.png)\n\n使用`torch.tensor(data,requires_grad=True)`创建张量时将require_grad设为True即可\n\n* Pytorch 不支持向量张量对向量张量的求导，只支持标量张量对向量张量的求导\n  * x如果是张量，y必须是标量（1个值）才能进行求导\n* 计算梯度:\n  * `y.backward()`    **--y是一个标量**\n* 获取x点的梯度值：\n  * `x.grad`    **--会累加上一次的梯度值**\n\n\n\n##### 梯度下降法最优解\n\n**梯度下降公式：**  w = w - r * grad      (r=学习率 ， grad=梯度值)\n\n清空上次的梯度值：  `x.grad.zero_()`\n\n![image-20251014215956562](../../img/image-20251014215956562.png)\n","slug":"DeepLearning/DeepLearningDay2","published":1,"date":"2025-10-13T00:51:48.646Z","updated":"2025-10-15T06:44:19.929Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cmgu7uyej0011jkv221vw072a","content":"<h3 id=\"Pytorch\"><a href=\"#Pytorch\" class=\"headerlink\" title=\"Pytorch\"></a>Pytorch</h3><h4 id=\"张量的运算\"><a href=\"#张量的运算\" class=\"headerlink\" title=\"张量的运算\"></a>张量的运算</h4><p>add , sub, mul, div ,neg  – 加减乘除</p>\n<p>add_ , sub__, mul_, div_ ,neg_     – 带下划线赋值生效</p>\n<p>或直接用符号(+ - * &#x2F; )</p>\n<p>向量之间相乘条件: shape: [n,m]  * shape: [m, p]  &#x3D; shape:[n,p]</p>\n<p><code>向量1 @ 向量2</code>     – 形状必须一致且符合相乘的条件元素之间才能相乘</p>\n<p><code>torch.matmul(向量1，向量2) </code>   – 形状可以不一致，但是必须符合相乘的条件</p>\n<h4 id=\"计算函数\"><a href=\"#计算函数\" class=\"headerlink\" title=\"计算函数\"></a>计算函数</h4><p><code>torch.mean()</code></p>\n<p><code>torch.sqrt()</code>    – 平方根</p>\n<p><code>torch.sum()</code></p>\n<p><code>torch.exp() </code>    – 指数函数</p>\n<p><code>torch.log()</code></p>\n<p><code>torch.log2()</code></p>\n<p><code>torch.log10()</code></p>\n<p><code>torch.pow()  </code>   –幂</p>\n<h4 id=\"张量的索引\"><a href=\"#张量的索引\" class=\"headerlink\" title=\"张量的索引\"></a>张量的索引</h4><ul>\n<li>张量.[行，列]</li>\n<li>张量.[[行],[列]]       – 坐标轴：返回一个具体张量元素</li>\n<li>张量.[[行1，行2],[列1，列2]]     – 返回两个值，坐标轴为：【行1，列1】【行2，列2】 </li>\n<li>张量.[行x，张量[行y] &lt; z]         –布尔索引  y行返回小于z的 x行元素</li>\n<li>张量[1，1，1，2，3]     – 多维索引    返回多维里2行3列的张量元素</li>\n</ul>\n<h4 id=\"张量形状操作\"><a href=\"#张量形状操作\" class=\"headerlink\" title=\"张量形状操作\"></a>张量形状操作</h4><h5 id=\"reshape\"><a href=\"#reshape\" class=\"headerlink\" title=\"reshape()\"></a><code>reshape()</code></h5><p>最常用的改变形状函数</p>\n<p><code>张量.reshape() </code>       –改变形状 ，保证数据不会损失</p>\n<p><img src=\"/../../img/image-20251014212502665.png\" alt=\"image-20251014212502665\"></p>\n<h5 id=\"squeeze-unsqueeze\"><a href=\"#squeeze-unsqueeze\" class=\"headerlink\" title=\"squeeze()  &amp;  unsqueeze()\"></a><code>squeeze() </code> &amp;  <code>unsqueeze()</code></h5><p>降维函数 &amp; 升维函数</p>\n<p><img src=\"/../../img/image-20251014212255259.png\" alt=\"image-20251014212255259\"></p>\n<h5 id=\"transpose-permute\"><a href=\"#transpose-permute\" class=\"headerlink\" title=\"transpose() &amp; permute()\"></a><code>transpose() </code>&amp; <code>permute()</code></h5><p>transpose:  交换张量形状&#x2F;转置，将二行三列转为三行二列，一行四列转为四行一列等等…</p>\n<p>permute:  转置，一次可以转多个维度</p>\n<p><img src=\"/../../img/image-20251014213043601.png\" alt=\"image-20251014213043601\"></p>\n<h5 id=\"view-contiguous\"><a href=\"#view-contiguous\" class=\"headerlink\" title=\"view() &amp; contiguous()\"></a><code>view()</code> &amp; <code>contiguous()</code></h5><p>view也是修改形状，只能修改连续的张量（内存地址中的连续）。</p>\n<p><strong>内存连续 (Contiguous)：</strong> </p>\n<p>张量中<strong>逻辑上相邻</strong>的元素（通常按照行主序，即 C&#x2F;Python&#x2F;PyTorch 的默认存储方式），在内存地址上也是<strong>紧密相邻</strong>的。如果从内存起点依次读取，就能完整且按序地读出整个张量。</p>\n<p><img src=\"/../../img/image-20251014213546574.png\" alt=\"image-20251014213546574\"></p>\n<p><strong>转置示例：</strong></p>\n<p>如果将 T 转置为 T′，逻辑上 T′ 的元素顺序是 **[1,4,2,5,3,6]<strong>。但是，在内存中，它仍然是 T 的原始存储：</strong>[1,2,3,4,5,6]**。</p>\n<p>此时，要读取 T′ 逻辑上的第二个元素（4），程序需要跳过（或“跨步”）内存地址，因此 T′ 是<strong>不连续的</strong>。</p>\n<p><strong>如何破坏连续性？</strong></p>\n<p>一些张量操作会改变元素的<strong>逻辑顺序</strong>（新的形状&#x2F;维度顺序），但<strong>不会</strong>改变底层数据在内存中的<strong>物理排列</strong>。这些操作会创建<strong>不连续的张量视图</strong>：</p>\n<ul>\n<li><strong><code>transpose()</code> (转置)</strong></li>\n<li><strong><code>permute()</code> (维度重排)</strong></li>\n<li><strong>非连续切片</strong> (例如，<code>tensor[:, ::2]</code>)</li>\n</ul>\n<p><strong>恢复连续性</strong></p>\n<ul>\n<li>使用 <code>tensor.contiguous()</code> 函数。</li>\n<li>该函数会检查张量是否连续。如果不连续，它会<strong>创建一个新的张量</strong>，并将原始数据按逻辑顺序<strong>复制</strong>到新张量的连续内存块中，从而恢复连续性。这个操作会消耗时间和内存。</li>\n</ul>\n<p><strong>为什么连续性很重要？</strong></p>\n<p>内存连续性是高性能计算的关键：</p>\n<ul>\n<li><strong>利用缓存优势：</strong> 当 CPU 或 GPU 访问一个连续张量的第一个元素时，由于局部性原理，系统会预先将后续相邻的元素加载到高速缓存中。这使得后续的数据访问速度大大提高。</li>\n<li><strong>底层优化：</strong> 许多高性能库（如 BLAS、cuDNN）和 PyTorch 的高效操作（如 <code>view()</code>）都要求输入张量是连续的，因为这样可以避免复杂的索引计算，直接通过内存指针操作数据。</li>\n</ul>\n<h4 id=\"张量拼接操作\"><a href=\"#张量拼接操作\" class=\"headerlink\" title=\"张量拼接操作\"></a>张量拼接操作</h4><h5 id=\"cat-stack\"><a href=\"#cat-stack\" class=\"headerlink\" title=\"cat() &amp;  stack()\"></a><code>cat()</code> &amp;  <code>stack()</code></h5><p><code>torch.cat(data1,data2)</code>      – 拼接两个张量数据， 不改变维数</p>\n<p><code>torch.stack(data1,data2)</code>     – 增加一个新维度，在内连接一列张量，输入的张量形状必须完全一致</p>\n<p><img src=\"/../../img/image-20251014214735283.png\" alt=\"image-20251014214735283\"></p>\n<p><img src=\"/../../img/image-20251014214810748.png\" alt=\"image-20251014214810748\"></p>\n<h4 id=\"自动微分模块\"><a href=\"#自动微分模块\" class=\"headerlink\" title=\"自动微分模块\"></a>自动微分模块</h4><p><strong>训练神经网络</strong>时，最常用的算法就是<strong>反向传播</strong>。在该算法中，参数(模型权重)会根据损失函数关于对应参数的梯度进行调整。为了计算这些梯度，PyTorch内置了名为 <code>torch.autograd</code> 的微分模块。它支持任意计算图的自动梯度计算:</p>\n<p><img src=\"/../../img/image-20251014215132441.png\" alt=\"image-20251014215132441\"></p>\n<p>使用<code>torch.tensor(data,requires_grad=True)</code>创建张量时将require_grad设为True即可</p>\n<ul>\n<li>Pytorch 不支持向量张量对向量张量的求导，只支持标量张量对向量张量的求导<ul>\n<li>x如果是张量，y必须是标量（1个值）才能进行求导</li>\n</ul>\n</li>\n<li>计算梯度:<ul>\n<li><code>y.backward()</code>    <strong>–y是一个标量</strong></li>\n</ul>\n</li>\n<li>获取x点的梯度值：<ul>\n<li><code>x.grad</code>    <strong>–会累加上一次的梯度值</strong></li>\n</ul>\n</li>\n</ul>\n<h5 id=\"梯度下降法最优解\"><a href=\"#梯度下降法最优解\" class=\"headerlink\" title=\"梯度下降法最优解\"></a>梯度下降法最优解</h5><p><strong>梯度下降公式：</strong>  w &#x3D; w - r * grad      (r&#x3D;学习率 ， grad&#x3D;梯度值)</p>\n<p>清空上次的梯度值：  <code>x.grad.zero_()</code></p>\n<p><img src=\"/../../img/image-20251014215956562.png\" alt=\"image-20251014215956562\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Pytorch\"><a href=\"#Pytorch\" class=\"headerlink\" title=\"Pytorch\"></a>Pytorch</h3><h4 id=\"张量的运算\"><a href=\"#张量的运算\" class=\"headerlink\" title=\"张量的运算\"></a>张量的运算</h4><p>add , sub, mul, div ,neg  – 加减乘除</p>\n<p>add_ , sub__, mul_, div_ ,neg_     – 带下划线赋值生效</p>\n<p>或直接用符号(+ - * &#x2F; )</p>\n<p>向量之间相乘条件: shape: [n,m]  * shape: [m, p]  &#x3D; shape:[n,p]</p>\n<p><code>向量1 @ 向量2</code>     – 形状必须一致且符合相乘的条件元素之间才能相乘</p>\n<p><code>torch.matmul(向量1，向量2) </code>   – 形状可以不一致，但是必须符合相乘的条件</p>\n<h4 id=\"计算函数\"><a href=\"#计算函数\" class=\"headerlink\" title=\"计算函数\"></a>计算函数</h4><p><code>torch.mean()</code></p>\n<p><code>torch.sqrt()</code>    – 平方根</p>\n<p><code>torch.sum()</code></p>\n<p><code>torch.exp() </code>    – 指数函数</p>\n<p><code>torch.log()</code></p>\n<p><code>torch.log2()</code></p>\n<p><code>torch.log10()</code></p>\n<p><code>torch.pow()  </code>   –幂</p>\n<h4 id=\"张量的索引\"><a href=\"#张量的索引\" class=\"headerlink\" title=\"张量的索引\"></a>张量的索引</h4><ul>\n<li>张量.[行，列]</li>\n<li>张量.[[行],[列]]       – 坐标轴：返回一个具体张量元素</li>\n<li>张量.[[行1，行2],[列1，列2]]     – 返回两个值，坐标轴为：【行1，列1】【行2，列2】 </li>\n<li>张量.[行x，张量[行y] &lt; z]         –布尔索引  y行返回小于z的 x行元素</li>\n<li>张量[1，1，1，2，3]     – 多维索引    返回多维里2行3列的张量元素</li>\n</ul>\n<h4 id=\"张量形状操作\"><a href=\"#张量形状操作\" class=\"headerlink\" title=\"张量形状操作\"></a>张量形状操作</h4><h5 id=\"reshape\"><a href=\"#reshape\" class=\"headerlink\" title=\"reshape()\"></a><code>reshape()</code></h5><p>最常用的改变形状函数</p>\n<p><code>张量.reshape() </code>       –改变形状 ，保证数据不会损失</p>\n<p><img src=\"/../../img/image-20251014212502665.png\" alt=\"image-20251014212502665\"></p>\n<h5 id=\"squeeze-unsqueeze\"><a href=\"#squeeze-unsqueeze\" class=\"headerlink\" title=\"squeeze()  &amp;  unsqueeze()\"></a><code>squeeze() </code> &amp;  <code>unsqueeze()</code></h5><p>降维函数 &amp; 升维函数</p>\n<p><img src=\"/../../img/image-20251014212255259.png\" alt=\"image-20251014212255259\"></p>\n<h5 id=\"transpose-permute\"><a href=\"#transpose-permute\" class=\"headerlink\" title=\"transpose() &amp; permute()\"></a><code>transpose() </code>&amp; <code>permute()</code></h5><p>transpose:  交换张量形状&#x2F;转置，将二行三列转为三行二列，一行四列转为四行一列等等…</p>\n<p>permute:  转置，一次可以转多个维度</p>\n<p><img src=\"/../../img/image-20251014213043601.png\" alt=\"image-20251014213043601\"></p>\n<h5 id=\"view-contiguous\"><a href=\"#view-contiguous\" class=\"headerlink\" title=\"view() &amp; contiguous()\"></a><code>view()</code> &amp; <code>contiguous()</code></h5><p>view也是修改形状，只能修改连续的张量（内存地址中的连续）。</p>\n<p><strong>内存连续 (Contiguous)：</strong> </p>\n<p>张量中<strong>逻辑上相邻</strong>的元素（通常按照行主序，即 C&#x2F;Python&#x2F;PyTorch 的默认存储方式），在内存地址上也是<strong>紧密相邻</strong>的。如果从内存起点依次读取，就能完整且按序地读出整个张量。</p>\n<p><img src=\"/../../img/image-20251014213546574.png\" alt=\"image-20251014213546574\"></p>\n<p><strong>转置示例：</strong></p>\n<p>如果将 T 转置为 T′，逻辑上 T′ 的元素顺序是 **[1,4,2,5,3,6]<strong>。但是，在内存中，它仍然是 T 的原始存储：</strong>[1,2,3,4,5,6]**。</p>\n<p>此时，要读取 T′ 逻辑上的第二个元素（4），程序需要跳过（或“跨步”）内存地址，因此 T′ 是<strong>不连续的</strong>。</p>\n<p><strong>如何破坏连续性？</strong></p>\n<p>一些张量操作会改变元素的<strong>逻辑顺序</strong>（新的形状&#x2F;维度顺序），但<strong>不会</strong>改变底层数据在内存中的<strong>物理排列</strong>。这些操作会创建<strong>不连续的张量视图</strong>：</p>\n<ul>\n<li><strong><code>transpose()</code> (转置)</strong></li>\n<li><strong><code>permute()</code> (维度重排)</strong></li>\n<li><strong>非连续切片</strong> (例如，<code>tensor[:, ::2]</code>)</li>\n</ul>\n<p><strong>恢复连续性</strong></p>\n<ul>\n<li>使用 <code>tensor.contiguous()</code> 函数。</li>\n<li>该函数会检查张量是否连续。如果不连续，它会<strong>创建一个新的张量</strong>，并将原始数据按逻辑顺序<strong>复制</strong>到新张量的连续内存块中，从而恢复连续性。这个操作会消耗时间和内存。</li>\n</ul>\n<p><strong>为什么连续性很重要？</strong></p>\n<p>内存连续性是高性能计算的关键：</p>\n<ul>\n<li><strong>利用缓存优势：</strong> 当 CPU 或 GPU 访问一个连续张量的第一个元素时，由于局部性原理，系统会预先将后续相邻的元素加载到高速缓存中。这使得后续的数据访问速度大大提高。</li>\n<li><strong>底层优化：</strong> 许多高性能库（如 BLAS、cuDNN）和 PyTorch 的高效操作（如 <code>view()</code>）都要求输入张量是连续的，因为这样可以避免复杂的索引计算，直接通过内存指针操作数据。</li>\n</ul>\n<h4 id=\"张量拼接操作\"><a href=\"#张量拼接操作\" class=\"headerlink\" title=\"张量拼接操作\"></a>张量拼接操作</h4><h5 id=\"cat-stack\"><a href=\"#cat-stack\" class=\"headerlink\" title=\"cat() &amp;  stack()\"></a><code>cat()</code> &amp;  <code>stack()</code></h5><p><code>torch.cat(data1,data2)</code>      – 拼接两个张量数据， 不改变维数</p>\n<p><code>torch.stack(data1,data2)</code>     – 增加一个新维度，在内连接一列张量，输入的张量形状必须完全一致</p>\n<p><img src=\"/../../img/image-20251014214735283.png\" alt=\"image-20251014214735283\"></p>\n<p><img src=\"/../../img/image-20251014214810748.png\" alt=\"image-20251014214810748\"></p>\n<h4 id=\"自动微分模块\"><a href=\"#自动微分模块\" class=\"headerlink\" title=\"自动微分模块\"></a>自动微分模块</h4><p><strong>训练神经网络</strong>时，最常用的算法就是<strong>反向传播</strong>。在该算法中，参数(模型权重)会根据损失函数关于对应参数的梯度进行调整。为了计算这些梯度，PyTorch内置了名为 <code>torch.autograd</code> 的微分模块。它支持任意计算图的自动梯度计算:</p>\n<p><img src=\"/../../img/image-20251014215132441.png\" alt=\"image-20251014215132441\"></p>\n<p>使用<code>torch.tensor(data,requires_grad=True)</code>创建张量时将require_grad设为True即可</p>\n<ul>\n<li>Pytorch 不支持向量张量对向量张量的求导，只支持标量张量对向量张量的求导<ul>\n<li>x如果是张量，y必须是标量（1个值）才能进行求导</li>\n</ul>\n</li>\n<li>计算梯度:<ul>\n<li><code>y.backward()</code>    <strong>–y是一个标量</strong></li>\n</ul>\n</li>\n<li>获取x点的梯度值：<ul>\n<li><code>x.grad</code>    <strong>–会累加上一次的梯度值</strong></li>\n</ul>\n</li>\n</ul>\n<h5 id=\"梯度下降法最优解\"><a href=\"#梯度下降法最优解\" class=\"headerlink\" title=\"梯度下降法最优解\"></a>梯度下降法最优解</h5><p><strong>梯度下降公式：</strong>  w &#x3D; w - r * grad      (r&#x3D;学习率 ， grad&#x3D;梯度值)</p>\n<p>清空上次的梯度值：  <code>x.grad.zero_()</code></p>\n<p><img src=\"/../../img/image-20251014215956562.png\" alt=\"image-20251014215956562\"></p>\n"},{"title":"电商数仓-优惠券维度表","_content":"\n站在优惠券的角度分析指标\n\n统计行为\n\n分析状态\n\n分析表：coupon_info\n\n```sql\nDROP TABLE IF EXISTS dim_coupon_full;\nCREATE EXTERNAL TABLE dim_coupon_full\n(\n    `id`                  STRING COMMENT '优惠券编号',\n    `coupon_name`       STRING COMMENT '优惠券名称',\n    `coupon_type_code` STRING COMMENT '优惠券类型编码',\n    `coupon_type_name` STRING COMMENT '优惠券类型名称',\n    `condition_amount` DECIMAL(16, 2) COMMENT '满额数',\n    `condition_num`     BIGINT COMMENT '满件数',\n    `activity_id`       STRING COMMENT '活动编号',\n    `benefit_amount`   DECIMAL(16, 2) COMMENT '减免金额',\n    `benefit_discount` DECIMAL(16, 2) COMMENT '折扣',\n    `benefit_rule`     STRING COMMENT '优惠规则:满元*减*元，满*件打*折',\n    `create_time`       STRING COMMENT '创建时间',\n    `range_type_code`  STRING COMMENT '优惠范围类型编码',\n    `range_type_name`  STRING COMMENT '优惠范围类型名称',\n    `limit_num`         BIGINT COMMENT '最多领取次数',\n    `taken_count`       BIGINT COMMENT '已领取次数',\n    `start_time`        STRING COMMENT '可以领取的开始时间',\n    `end_time`          STRING COMMENT '可以领取的结束时间',\n    `operate_time`      STRING COMMENT '修改时间',\n    `expire_time`       STRING COMMENT '过期时间'\n) COMMENT '优惠券维度表'\n    PARTITIONED BY (`dt` STRING)\n    STORED AS ORC\n    LOCATION '/warehouse/gmall/dim/dim_coupon_full/'\n    TBLPROPERTIES ('orc.compress' = 'snappy');\n```\n\n数据装载：\n\n​\t\t\t\t-load：\n\n​\t\t\t\t\t\t\t-coupon_info(主维表)\n\n​\t\t\t\t\t\t\t-base_dic（字典表）\n\n​\t\t\t\t-save:\n\n​\t   ![](../../img/image-20241014020343184.png)\n\n```sql\ninsert overwrite table dim_coupon_full partition(dt='2022-06-08')\nselect\n    id,\n    coupon_name,\n    coupon_type,\n    coupon_dic.dic_name,\n    condition_amount,\n    condition_num,\n    activity_id,\n    benefit_amount,\n    benefit_discount,\n    case coupon_type\n        when '3201' then concat('满',condition_amount,'元减',benefit_amount,'元')\n        when '3202' then concat('满',condition_num,'件打', benefit_discount,' 折')\n        when '3203' then concat('减',benefit_amount,'元')\n    end benefit_rule,\n    create_time,\n    range_type,\n    range_dic.dic_name,\n    limit_num,\n    taken_count,\n    start_time,\n    end_time,\n    operate_time,\n    expire_time\nfrom\n(\n    select\n        id,\n        coupon_name,\n        coupon_type,\n        condition_amount,\n        condition_num,\n        activity_id,\n        benefit_amount,\n        benefit_discount,\n        create_time,\n        range_type,\n        limit_num,\n        taken_count,\n        start_time,\n        end_time,\n        operate_time,\n        expire_time\n    from ods_coupon_info_full\n    where dt='2022-06-08'\n)ci\nleft join\n(\n    select\n        dic_code,\n        dic_name\n    from ods_base_dic_full\n    where dt='2022-06-08'\n    and parent_code='32'\n)coupon_dic\non ci.coupon_type=coupon_dic.dic_code\nleft join\n(\n    select\n        dic_code,\n        dic_name\n    from ods_base_dic_full\n    where dt='2022-06-08'\n    and parent_code='33'\n)range_dic\non ci.range_type=range_dic.dic_code;\n```","source":"_posts/电商数仓6.0/优惠券维度表.md","raw":"---\ntitle: 电商数仓-优惠券维度表\ntags:\n  - BigData\n  - 电商数仓6.0\ncategories:\n  - - 电商数仓6.0\n---\n\n站在优惠券的角度分析指标\n\n统计行为\n\n分析状态\n\n分析表：coupon_info\n\n```sql\nDROP TABLE IF EXISTS dim_coupon_full;\nCREATE EXTERNAL TABLE dim_coupon_full\n(\n    `id`                  STRING COMMENT '优惠券编号',\n    `coupon_name`       STRING COMMENT '优惠券名称',\n    `coupon_type_code` STRING COMMENT '优惠券类型编码',\n    `coupon_type_name` STRING COMMENT '优惠券类型名称',\n    `condition_amount` DECIMAL(16, 2) COMMENT '满额数',\n    `condition_num`     BIGINT COMMENT '满件数',\n    `activity_id`       STRING COMMENT '活动编号',\n    `benefit_amount`   DECIMAL(16, 2) COMMENT '减免金额',\n    `benefit_discount` DECIMAL(16, 2) COMMENT '折扣',\n    `benefit_rule`     STRING COMMENT '优惠规则:满元*减*元，满*件打*折',\n    `create_time`       STRING COMMENT '创建时间',\n    `range_type_code`  STRING COMMENT '优惠范围类型编码',\n    `range_type_name`  STRING COMMENT '优惠范围类型名称',\n    `limit_num`         BIGINT COMMENT '最多领取次数',\n    `taken_count`       BIGINT COMMENT '已领取次数',\n    `start_time`        STRING COMMENT '可以领取的开始时间',\n    `end_time`          STRING COMMENT '可以领取的结束时间',\n    `operate_time`      STRING COMMENT '修改时间',\n    `expire_time`       STRING COMMENT '过期时间'\n) COMMENT '优惠券维度表'\n    PARTITIONED BY (`dt` STRING)\n    STORED AS ORC\n    LOCATION '/warehouse/gmall/dim/dim_coupon_full/'\n    TBLPROPERTIES ('orc.compress' = 'snappy');\n```\n\n数据装载：\n\n​\t\t\t\t-load：\n\n​\t\t\t\t\t\t\t-coupon_info(主维表)\n\n​\t\t\t\t\t\t\t-base_dic（字典表）\n\n​\t\t\t\t-save:\n\n​\t   ![](../../img/image-20241014020343184.png)\n\n```sql\ninsert overwrite table dim_coupon_full partition(dt='2022-06-08')\nselect\n    id,\n    coupon_name,\n    coupon_type,\n    coupon_dic.dic_name,\n    condition_amount,\n    condition_num,\n    activity_id,\n    benefit_amount,\n    benefit_discount,\n    case coupon_type\n        when '3201' then concat('满',condition_amount,'元减',benefit_amount,'元')\n        when '3202' then concat('满',condition_num,'件打', benefit_discount,' 折')\n        when '3203' then concat('减',benefit_amount,'元')\n    end benefit_rule,\n    create_time,\n    range_type,\n    range_dic.dic_name,\n    limit_num,\n    taken_count,\n    start_time,\n    end_time,\n    operate_time,\n    expire_time\nfrom\n(\n    select\n        id,\n        coupon_name,\n        coupon_type,\n        condition_amount,\n        condition_num,\n        activity_id,\n        benefit_amount,\n        benefit_discount,\n        create_time,\n        range_type,\n        limit_num,\n        taken_count,\n        start_time,\n        end_time,\n        operate_time,\n        expire_time\n    from ods_coupon_info_full\n    where dt='2022-06-08'\n)ci\nleft join\n(\n    select\n        dic_code,\n        dic_name\n    from ods_base_dic_full\n    where dt='2022-06-08'\n    and parent_code='32'\n)coupon_dic\non ci.coupon_type=coupon_dic.dic_code\nleft join\n(\n    select\n        dic_code,\n        dic_name\n    from ods_base_dic_full\n    where dt='2022-06-08'\n    and parent_code='33'\n)range_dic\non ci.range_type=range_dic.dic_code;\n```","slug":"电商数仓6.0/优惠券维度表","published":1,"date":"2024-10-13T17:07:35.388Z","updated":"2024-10-13T18:07:34.832Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cmgu7uyej0012jkv2656u6atx","content":"<p>站在优惠券的角度分析指标</p>\n<p>统计行为</p>\n<p>分析状态</p>\n<p>分析表：coupon_info</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">DROP</span> <span class=\"keyword\">TABLE</span> IF <span class=\"keyword\">EXISTS</span> dim_coupon_full;</span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">EXTERNAL</span> <span class=\"keyword\">TABLE</span> dim_coupon_full</span><br><span class=\"line\">(</span><br><span class=\"line\">    `id`                  STRING COMMENT <span class=\"string\">&#x27;优惠券编号&#x27;</span>,</span><br><span class=\"line\">    `coupon_name`       STRING COMMENT <span class=\"string\">&#x27;优惠券名称&#x27;</span>,</span><br><span class=\"line\">    `coupon_type_code` STRING COMMENT <span class=\"string\">&#x27;优惠券类型编码&#x27;</span>,</span><br><span class=\"line\">    `coupon_type_name` STRING COMMENT <span class=\"string\">&#x27;优惠券类型名称&#x27;</span>,</span><br><span class=\"line\">    `condition_amount` <span class=\"type\">DECIMAL</span>(<span class=\"number\">16</span>, <span class=\"number\">2</span>) COMMENT <span class=\"string\">&#x27;满额数&#x27;</span>,</span><br><span class=\"line\">    `condition_num`     <span class=\"type\">BIGINT</span> COMMENT <span class=\"string\">&#x27;满件数&#x27;</span>,</span><br><span class=\"line\">    `activity_id`       STRING COMMENT <span class=\"string\">&#x27;活动编号&#x27;</span>,</span><br><span class=\"line\">    `benefit_amount`   <span class=\"type\">DECIMAL</span>(<span class=\"number\">16</span>, <span class=\"number\">2</span>) COMMENT <span class=\"string\">&#x27;减免金额&#x27;</span>,</span><br><span class=\"line\">    `benefit_discount` <span class=\"type\">DECIMAL</span>(<span class=\"number\">16</span>, <span class=\"number\">2</span>) COMMENT <span class=\"string\">&#x27;折扣&#x27;</span>,</span><br><span class=\"line\">    `benefit_rule`     STRING COMMENT <span class=\"string\">&#x27;优惠规则:满元*减*元，满*件打*折&#x27;</span>,</span><br><span class=\"line\">    `create_time`       STRING COMMENT <span class=\"string\">&#x27;创建时间&#x27;</span>,</span><br><span class=\"line\">    `range_type_code`  STRING COMMENT <span class=\"string\">&#x27;优惠范围类型编码&#x27;</span>,</span><br><span class=\"line\">    `range_type_name`  STRING COMMENT <span class=\"string\">&#x27;优惠范围类型名称&#x27;</span>,</span><br><span class=\"line\">    `limit_num`         <span class=\"type\">BIGINT</span> COMMENT <span class=\"string\">&#x27;最多领取次数&#x27;</span>,</span><br><span class=\"line\">    `taken_count`       <span class=\"type\">BIGINT</span> COMMENT <span class=\"string\">&#x27;已领取次数&#x27;</span>,</span><br><span class=\"line\">    `start_time`        STRING COMMENT <span class=\"string\">&#x27;可以领取的开始时间&#x27;</span>,</span><br><span class=\"line\">    `end_time`          STRING COMMENT <span class=\"string\">&#x27;可以领取的结束时间&#x27;</span>,</span><br><span class=\"line\">    `operate_time`      STRING COMMENT <span class=\"string\">&#x27;修改时间&#x27;</span>,</span><br><span class=\"line\">    `expire_time`       STRING COMMENT <span class=\"string\">&#x27;过期时间&#x27;</span></span><br><span class=\"line\">) COMMENT <span class=\"string\">&#x27;优惠券维度表&#x27;</span></span><br><span class=\"line\">    PARTITIONED <span class=\"keyword\">BY</span> (`dt` STRING)</span><br><span class=\"line\">    STORED <span class=\"keyword\">AS</span> ORC</span><br><span class=\"line\">    LOCATION <span class=\"string\">&#x27;/warehouse/gmall/dim/dim_coupon_full/&#x27;</span></span><br><span class=\"line\">    TBLPROPERTIES (<span class=\"string\">&#x27;orc.compress&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;snappy&#x27;</span>);</span><br></pre></td></tr></table></figure>\n\n<p>数据装载：</p>\n<p>​\t\t\t\t-load：</p>\n<p>​\t\t\t\t\t\t\t-coupon_info(主维表)</p>\n<p>​\t\t\t\t\t\t\t-base_dic（字典表）</p>\n<p>​\t\t\t\t-save:</p>\n<p>​\t   <img src=\"/../../img/image-20241014020343184.png\"></p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">insert</span> overwrite <span class=\"keyword\">table</span> dim_coupon_full <span class=\"keyword\">partition</span>(dt<span class=\"operator\">=</span><span class=\"string\">&#x27;2022-06-08&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">select</span></span><br><span class=\"line\">    id,</span><br><span class=\"line\">    coupon_name,</span><br><span class=\"line\">    coupon_type,</span><br><span class=\"line\">    coupon_dic.dic_name,</span><br><span class=\"line\">    condition_amount,</span><br><span class=\"line\">    condition_num,</span><br><span class=\"line\">    activity_id,</span><br><span class=\"line\">    benefit_amount,</span><br><span class=\"line\">    benefit_discount,</span><br><span class=\"line\">    <span class=\"keyword\">case</span> coupon_type</span><br><span class=\"line\">        <span class=\"keyword\">when</span> <span class=\"string\">&#x27;3201&#x27;</span> <span class=\"keyword\">then</span> concat(<span class=\"string\">&#x27;满&#x27;</span>,condition_amount,<span class=\"string\">&#x27;元减&#x27;</span>,benefit_amount,<span class=\"string\">&#x27;元&#x27;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">when</span> <span class=\"string\">&#x27;3202&#x27;</span> <span class=\"keyword\">then</span> concat(<span class=\"string\">&#x27;满&#x27;</span>,condition_num,<span class=\"string\">&#x27;件打&#x27;</span>, benefit_discount,<span class=\"string\">&#x27; 折&#x27;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">when</span> <span class=\"string\">&#x27;3203&#x27;</span> <span class=\"keyword\">then</span> concat(<span class=\"string\">&#x27;减&#x27;</span>,benefit_amount,<span class=\"string\">&#x27;元&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">end</span> benefit_rule,</span><br><span class=\"line\">    create_time,</span><br><span class=\"line\">    range_type,</span><br><span class=\"line\">    range_dic.dic_name,</span><br><span class=\"line\">    limit_num,</span><br><span class=\"line\">    taken_count,</span><br><span class=\"line\">    start_time,</span><br><span class=\"line\">    end_time,</span><br><span class=\"line\">    operate_time,</span><br><span class=\"line\">    expire_time</span><br><span class=\"line\"><span class=\"keyword\">from</span></span><br><span class=\"line\">(</span><br><span class=\"line\">    <span class=\"keyword\">select</span></span><br><span class=\"line\">        id,</span><br><span class=\"line\">        coupon_name,</span><br><span class=\"line\">        coupon_type,</span><br><span class=\"line\">        condition_amount,</span><br><span class=\"line\">        condition_num,</span><br><span class=\"line\">        activity_id,</span><br><span class=\"line\">        benefit_amount,</span><br><span class=\"line\">        benefit_discount,</span><br><span class=\"line\">        create_time,</span><br><span class=\"line\">        range_type,</span><br><span class=\"line\">        limit_num,</span><br><span class=\"line\">        taken_count,</span><br><span class=\"line\">        start_time,</span><br><span class=\"line\">        end_time,</span><br><span class=\"line\">        operate_time,</span><br><span class=\"line\">        expire_time</span><br><span class=\"line\">    <span class=\"keyword\">from</span> ods_coupon_info_full</span><br><span class=\"line\">    <span class=\"keyword\">where</span> dt<span class=\"operator\">=</span><span class=\"string\">&#x27;2022-06-08&#x27;</span></span><br><span class=\"line\">)ci</span><br><span class=\"line\"><span class=\"keyword\">left</span> <span class=\"keyword\">join</span></span><br><span class=\"line\">(</span><br><span class=\"line\">    <span class=\"keyword\">select</span></span><br><span class=\"line\">        dic_code,</span><br><span class=\"line\">        dic_name</span><br><span class=\"line\">    <span class=\"keyword\">from</span> ods_base_dic_full</span><br><span class=\"line\">    <span class=\"keyword\">where</span> dt<span class=\"operator\">=</span><span class=\"string\">&#x27;2022-06-08&#x27;</span></span><br><span class=\"line\">    <span class=\"keyword\">and</span> parent_code<span class=\"operator\">=</span><span class=\"string\">&#x27;32&#x27;</span></span><br><span class=\"line\">)coupon_dic</span><br><span class=\"line\"><span class=\"keyword\">on</span> ci.coupon_type<span class=\"operator\">=</span>coupon_dic.dic_code</span><br><span class=\"line\"><span class=\"keyword\">left</span> <span class=\"keyword\">join</span></span><br><span class=\"line\">(</span><br><span class=\"line\">    <span class=\"keyword\">select</span></span><br><span class=\"line\">        dic_code,</span><br><span class=\"line\">        dic_name</span><br><span class=\"line\">    <span class=\"keyword\">from</span> ods_base_dic_full</span><br><span class=\"line\">    <span class=\"keyword\">where</span> dt<span class=\"operator\">=</span><span class=\"string\">&#x27;2022-06-08&#x27;</span></span><br><span class=\"line\">    <span class=\"keyword\">and</span> parent_code<span class=\"operator\">=</span><span class=\"string\">&#x27;33&#x27;</span></span><br><span class=\"line\">)range_dic</span><br><span class=\"line\"><span class=\"keyword\">on</span> ci.range_type<span class=\"operator\">=</span>range_dic.dic_code;</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<p>站在优惠券的角度分析指标</p>\n<p>统计行为</p>\n<p>分析状态</p>\n<p>分析表：coupon_info</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">DROP</span> <span class=\"keyword\">TABLE</span> IF <span class=\"keyword\">EXISTS</span> dim_coupon_full;</span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">EXTERNAL</span> <span class=\"keyword\">TABLE</span> dim_coupon_full</span><br><span class=\"line\">(</span><br><span class=\"line\">    `id`                  STRING COMMENT <span class=\"string\">&#x27;优惠券编号&#x27;</span>,</span><br><span class=\"line\">    `coupon_name`       STRING COMMENT <span class=\"string\">&#x27;优惠券名称&#x27;</span>,</span><br><span class=\"line\">    `coupon_type_code` STRING COMMENT <span class=\"string\">&#x27;优惠券类型编码&#x27;</span>,</span><br><span class=\"line\">    `coupon_type_name` STRING COMMENT <span class=\"string\">&#x27;优惠券类型名称&#x27;</span>,</span><br><span class=\"line\">    `condition_amount` <span class=\"type\">DECIMAL</span>(<span class=\"number\">16</span>, <span class=\"number\">2</span>) COMMENT <span class=\"string\">&#x27;满额数&#x27;</span>,</span><br><span class=\"line\">    `condition_num`     <span class=\"type\">BIGINT</span> COMMENT <span class=\"string\">&#x27;满件数&#x27;</span>,</span><br><span class=\"line\">    `activity_id`       STRING COMMENT <span class=\"string\">&#x27;活动编号&#x27;</span>,</span><br><span class=\"line\">    `benefit_amount`   <span class=\"type\">DECIMAL</span>(<span class=\"number\">16</span>, <span class=\"number\">2</span>) COMMENT <span class=\"string\">&#x27;减免金额&#x27;</span>,</span><br><span class=\"line\">    `benefit_discount` <span class=\"type\">DECIMAL</span>(<span class=\"number\">16</span>, <span class=\"number\">2</span>) COMMENT <span class=\"string\">&#x27;折扣&#x27;</span>,</span><br><span class=\"line\">    `benefit_rule`     STRING COMMENT <span class=\"string\">&#x27;优惠规则:满元*减*元，满*件打*折&#x27;</span>,</span><br><span class=\"line\">    `create_time`       STRING COMMENT <span class=\"string\">&#x27;创建时间&#x27;</span>,</span><br><span class=\"line\">    `range_type_code`  STRING COMMENT <span class=\"string\">&#x27;优惠范围类型编码&#x27;</span>,</span><br><span class=\"line\">    `range_type_name`  STRING COMMENT <span class=\"string\">&#x27;优惠范围类型名称&#x27;</span>,</span><br><span class=\"line\">    `limit_num`         <span class=\"type\">BIGINT</span> COMMENT <span class=\"string\">&#x27;最多领取次数&#x27;</span>,</span><br><span class=\"line\">    `taken_count`       <span class=\"type\">BIGINT</span> COMMENT <span class=\"string\">&#x27;已领取次数&#x27;</span>,</span><br><span class=\"line\">    `start_time`        STRING COMMENT <span class=\"string\">&#x27;可以领取的开始时间&#x27;</span>,</span><br><span class=\"line\">    `end_time`          STRING COMMENT <span class=\"string\">&#x27;可以领取的结束时间&#x27;</span>,</span><br><span class=\"line\">    `operate_time`      STRING COMMENT <span class=\"string\">&#x27;修改时间&#x27;</span>,</span><br><span class=\"line\">    `expire_time`       STRING COMMENT <span class=\"string\">&#x27;过期时间&#x27;</span></span><br><span class=\"line\">) COMMENT <span class=\"string\">&#x27;优惠券维度表&#x27;</span></span><br><span class=\"line\">    PARTITIONED <span class=\"keyword\">BY</span> (`dt` STRING)</span><br><span class=\"line\">    STORED <span class=\"keyword\">AS</span> ORC</span><br><span class=\"line\">    LOCATION <span class=\"string\">&#x27;/warehouse/gmall/dim/dim_coupon_full/&#x27;</span></span><br><span class=\"line\">    TBLPROPERTIES (<span class=\"string\">&#x27;orc.compress&#x27;</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;snappy&#x27;</span>);</span><br></pre></td></tr></table></figure>\n\n<p>数据装载：</p>\n<p>​\t\t\t\t-load：</p>\n<p>​\t\t\t\t\t\t\t-coupon_info(主维表)</p>\n<p>​\t\t\t\t\t\t\t-base_dic（字典表）</p>\n<p>​\t\t\t\t-save:</p>\n<p>​\t   <img src=\"/../../img/image-20241014020343184.png\"></p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">insert</span> overwrite <span class=\"keyword\">table</span> dim_coupon_full <span class=\"keyword\">partition</span>(dt<span class=\"operator\">=</span><span class=\"string\">&#x27;2022-06-08&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">select</span></span><br><span class=\"line\">    id,</span><br><span class=\"line\">    coupon_name,</span><br><span class=\"line\">    coupon_type,</span><br><span class=\"line\">    coupon_dic.dic_name,</span><br><span class=\"line\">    condition_amount,</span><br><span class=\"line\">    condition_num,</span><br><span class=\"line\">    activity_id,</span><br><span class=\"line\">    benefit_amount,</span><br><span class=\"line\">    benefit_discount,</span><br><span class=\"line\">    <span class=\"keyword\">case</span> coupon_type</span><br><span class=\"line\">        <span class=\"keyword\">when</span> <span class=\"string\">&#x27;3201&#x27;</span> <span class=\"keyword\">then</span> concat(<span class=\"string\">&#x27;满&#x27;</span>,condition_amount,<span class=\"string\">&#x27;元减&#x27;</span>,benefit_amount,<span class=\"string\">&#x27;元&#x27;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">when</span> <span class=\"string\">&#x27;3202&#x27;</span> <span class=\"keyword\">then</span> concat(<span class=\"string\">&#x27;满&#x27;</span>,condition_num,<span class=\"string\">&#x27;件打&#x27;</span>, benefit_discount,<span class=\"string\">&#x27; 折&#x27;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">when</span> <span class=\"string\">&#x27;3203&#x27;</span> <span class=\"keyword\">then</span> concat(<span class=\"string\">&#x27;减&#x27;</span>,benefit_amount,<span class=\"string\">&#x27;元&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">end</span> benefit_rule,</span><br><span class=\"line\">    create_time,</span><br><span class=\"line\">    range_type,</span><br><span class=\"line\">    range_dic.dic_name,</span><br><span class=\"line\">    limit_num,</span><br><span class=\"line\">    taken_count,</span><br><span class=\"line\">    start_time,</span><br><span class=\"line\">    end_time,</span><br><span class=\"line\">    operate_time,</span><br><span class=\"line\">    expire_time</span><br><span class=\"line\"><span class=\"keyword\">from</span></span><br><span class=\"line\">(</span><br><span class=\"line\">    <span class=\"keyword\">select</span></span><br><span class=\"line\">        id,</span><br><span class=\"line\">        coupon_name,</span><br><span class=\"line\">        coupon_type,</span><br><span class=\"line\">        condition_amount,</span><br><span class=\"line\">        condition_num,</span><br><span class=\"line\">        activity_id,</span><br><span class=\"line\">        benefit_amount,</span><br><span class=\"line\">        benefit_discount,</span><br><span class=\"line\">        create_time,</span><br><span class=\"line\">        range_type,</span><br><span class=\"line\">        limit_num,</span><br><span class=\"line\">        taken_count,</span><br><span class=\"line\">        start_time,</span><br><span class=\"line\">        end_time,</span><br><span class=\"line\">        operate_time,</span><br><span class=\"line\">        expire_time</span><br><span class=\"line\">    <span class=\"keyword\">from</span> ods_coupon_info_full</span><br><span class=\"line\">    <span class=\"keyword\">where</span> dt<span class=\"operator\">=</span><span class=\"string\">&#x27;2022-06-08&#x27;</span></span><br><span class=\"line\">)ci</span><br><span class=\"line\"><span class=\"keyword\">left</span> <span class=\"keyword\">join</span></span><br><span class=\"line\">(</span><br><span class=\"line\">    <span class=\"keyword\">select</span></span><br><span class=\"line\">        dic_code,</span><br><span class=\"line\">        dic_name</span><br><span class=\"line\">    <span class=\"keyword\">from</span> ods_base_dic_full</span><br><span class=\"line\">    <span class=\"keyword\">where</span> dt<span class=\"operator\">=</span><span class=\"string\">&#x27;2022-06-08&#x27;</span></span><br><span class=\"line\">    <span class=\"keyword\">and</span> parent_code<span class=\"operator\">=</span><span class=\"string\">&#x27;32&#x27;</span></span><br><span class=\"line\">)coupon_dic</span><br><span class=\"line\"><span class=\"keyword\">on</span> ci.coupon_type<span class=\"operator\">=</span>coupon_dic.dic_code</span><br><span class=\"line\"><span class=\"keyword\">left</span> <span class=\"keyword\">join</span></span><br><span class=\"line\">(</span><br><span class=\"line\">    <span class=\"keyword\">select</span></span><br><span class=\"line\">        dic_code,</span><br><span class=\"line\">        dic_name</span><br><span class=\"line\">    <span class=\"keyword\">from</span> ods_base_dic_full</span><br><span class=\"line\">    <span class=\"keyword\">where</span> dt<span class=\"operator\">=</span><span class=\"string\">&#x27;2022-06-08&#x27;</span></span><br><span class=\"line\">    <span class=\"keyword\">and</span> parent_code<span class=\"operator\">=</span><span class=\"string\">&#x27;33&#x27;</span></span><br><span class=\"line\">)range_dic</span><br><span class=\"line\"><span class=\"keyword\">on</span> ci.range_type<span class=\"operator\">=</span>range_dic.dic_code;</span><br></pre></td></tr></table></figure>"},{"title":"深度学习-线性回归模型训练步骤","_content":"\n\n\n## 模型训练\n\n训练核心步骤如下：\n\n\n\n![深度学习模型训练核心步骤](../../img/image-20251015144217715.png)\n\n\n\n```python\n# 导包\nimport torch\nfrom sklearn.datasets import make_regression\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nfrom torch import optim\nimport matplotlib\n\nmatplotlib.use('TkAgg')\nimport matplotlib.pyplot as plt\n# 在文件开头添加，忽略此警告\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", message=\".*__array_wrap__ must accept context.*\")\n\n# 解决中文乱码问题\nplt.rcParams['font.sans-serif'] = ['KaiTi']\nplt.rcParams['axes.unicode_minus'] = False\n\n\ndef get_data(bias):\n    # 1.1 制作数据\n    x, y, coef = make_regression(n_samples=100, n_features=1, n_targets=1, noise=10, coef=True, bias=bias,\n                                 random_state=1)\n    # 1.2 转换为张量\n    x = torch.tensor(x, dtype=torch.float)\n    y = torch.tensor(y, dtype=torch.float)\n    print(x.shape, y.shape)\n    # 返回结果\n    return x, y, coef\n\n\ndef model_train(x, y, epochs, batch_size, lr):\n    # TODO 1.准备数据\n    # 1.3 创建数据集\n    dataset = TensorDataset(x, y)\n    # 1.4 创建数据加载器\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    print(len(dataset))\n    print(len(dataloader))  # 32 32 32 4\n    # TODO 2.准备模型\n    model = nn.Linear(in_features=1, out_features=1)\n    # TODO 3.准备损失函数\n    loss_fn = nn.MSELoss()\n    # TODO 4.准备优化器\n    optimizer = optim.SGD(model.parameters(), lr=lr)\n    # TODO 5.训练模型\n    # 设置轮次\n    epochs = epochs\n    # todo 获取总损失,总批次,计算截止到当前轮损失并添加损失列表中,方便后续绘制图表\n    total_loss, batch_cnt, loss_list = 0.0, 0, []\n    # todo 遍历轮次\n    for epoch in range(epochs):\n        # todo 遍历批次数据\n        for batch_x, batch_y in dataloader:\n            # todo 前向传播\n            # 计算预测数据\n            y_pred = model(batch_x)\n            # 计算损失\n            loss = loss_fn(y_pred, batch_y.reshape(-1, 1))  # 此处需将batch_y转换为[32,1],否则会产生形状不一致的广播问题,影响误差计算\n            # todo 累计损失和批次数\n            total_loss += loss.item()\n            batch_cnt += 1\n            # todo 反向传播\n            # 梯度清零\n            optimizer.zero_grad()\n            # 计算梯度\n            loss.backward()  # 此处底层自动求导数,把求出的导数赋值给w.grad属性\n            # 参数更新\n            optimizer.step()  # 此处底层还是 w旧-学习率*梯度\n        # todo 计算截止到当前轮损失,打印日志并添加损失列表中,方便后续绘制图表\n        epoch_loss = total_loss / batch_cnt\n        print(f\"截止到第{epoch + 1}轮,最新损失为:{epoch_loss}\")\n        loss_list.append(epoch_loss)\n    # todo 代码走到此处,说明训练完成,模型可以保存\n    print(f\"训练完成,整体参数为:{model.state_dict()},权重为:{model.weight},偏置为:{model.bias}\")\n    torch.save(model.state_dict(), \"model.pth\")\n    print(\"模型保存成功\")\n    # 返回损失列表\n    return loss_list, model\n\n\ndef show_model_loss(epochs, loss_list, x, y, coef, model):\n    # todo 绘制每轮对应的损失折线图\n    plt.plot(range(epochs), loss_list)\n    plt.title(\"模型训练效果对比\")\n    plt.xlabel(\"轮次\")\n    plt.ylabel(\"损失值\")\n    plt.show()\n    # todo 绘制数据本身散点图\n    plt.scatter(x, y)\n    # todo 绘制模型预测结果和真实结果对比的图表\n    x = torch.linspace(x.min(), x.max(), 1000)\n    y_true = torch.tensor([v * coef + bias for v in x]) # 计算真实值\n    y_pred = torch.tensor([v * model.weight + model.bias for v in x]) # 计算预测值\n    plt.plot(x, y_true, label=\"真实\", color=\"red\")\n    plt.plot(x, y_pred, label=\"预测\", color=\"green\")\n    plt.legend()\n    plt.show()\n\n\nif __name__ == '__main__':\n    # 处理数据\n    bias = 14.5\n    x, y, coef = get_data(bias)\n    # 模型训练\n    epochs = 100\n    loss_list, model = model_train(x, y, epochs=epochs, batch_size=32, lr=0.01)\n    # 绘制图表\n    show_model_loss(epochs, loss_list, x, y, coef, model)\n```","source":"_posts/DeepLearning/DeepLearningDay3_1.md","raw":"---\ntitle: 深度学习-线性回归模型训练步骤\ncategories:\n  - - DeepLearning\n---\n\n\n\n## 模型训练\n\n训练核心步骤如下：\n\n\n\n![深度学习模型训练核心步骤](../../img/image-20251015144217715.png)\n\n\n\n```python\n# 导包\nimport torch\nfrom sklearn.datasets import make_regression\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nfrom torch import optim\nimport matplotlib\n\nmatplotlib.use('TkAgg')\nimport matplotlib.pyplot as plt\n# 在文件开头添加，忽略此警告\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", message=\".*__array_wrap__ must accept context.*\")\n\n# 解决中文乱码问题\nplt.rcParams['font.sans-serif'] = ['KaiTi']\nplt.rcParams['axes.unicode_minus'] = False\n\n\ndef get_data(bias):\n    # 1.1 制作数据\n    x, y, coef = make_regression(n_samples=100, n_features=1, n_targets=1, noise=10, coef=True, bias=bias,\n                                 random_state=1)\n    # 1.2 转换为张量\n    x = torch.tensor(x, dtype=torch.float)\n    y = torch.tensor(y, dtype=torch.float)\n    print(x.shape, y.shape)\n    # 返回结果\n    return x, y, coef\n\n\ndef model_train(x, y, epochs, batch_size, lr):\n    # TODO 1.准备数据\n    # 1.3 创建数据集\n    dataset = TensorDataset(x, y)\n    # 1.4 创建数据加载器\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    print(len(dataset))\n    print(len(dataloader))  # 32 32 32 4\n    # TODO 2.准备模型\n    model = nn.Linear(in_features=1, out_features=1)\n    # TODO 3.准备损失函数\n    loss_fn = nn.MSELoss()\n    # TODO 4.准备优化器\n    optimizer = optim.SGD(model.parameters(), lr=lr)\n    # TODO 5.训练模型\n    # 设置轮次\n    epochs = epochs\n    # todo 获取总损失,总批次,计算截止到当前轮损失并添加损失列表中,方便后续绘制图表\n    total_loss, batch_cnt, loss_list = 0.0, 0, []\n    # todo 遍历轮次\n    for epoch in range(epochs):\n        # todo 遍历批次数据\n        for batch_x, batch_y in dataloader:\n            # todo 前向传播\n            # 计算预测数据\n            y_pred = model(batch_x)\n            # 计算损失\n            loss = loss_fn(y_pred, batch_y.reshape(-1, 1))  # 此处需将batch_y转换为[32,1],否则会产生形状不一致的广播问题,影响误差计算\n            # todo 累计损失和批次数\n            total_loss += loss.item()\n            batch_cnt += 1\n            # todo 反向传播\n            # 梯度清零\n            optimizer.zero_grad()\n            # 计算梯度\n            loss.backward()  # 此处底层自动求导数,把求出的导数赋值给w.grad属性\n            # 参数更新\n            optimizer.step()  # 此处底层还是 w旧-学习率*梯度\n        # todo 计算截止到当前轮损失,打印日志并添加损失列表中,方便后续绘制图表\n        epoch_loss = total_loss / batch_cnt\n        print(f\"截止到第{epoch + 1}轮,最新损失为:{epoch_loss}\")\n        loss_list.append(epoch_loss)\n    # todo 代码走到此处,说明训练完成,模型可以保存\n    print(f\"训练完成,整体参数为:{model.state_dict()},权重为:{model.weight},偏置为:{model.bias}\")\n    torch.save(model.state_dict(), \"model.pth\")\n    print(\"模型保存成功\")\n    # 返回损失列表\n    return loss_list, model\n\n\ndef show_model_loss(epochs, loss_list, x, y, coef, model):\n    # todo 绘制每轮对应的损失折线图\n    plt.plot(range(epochs), loss_list)\n    plt.title(\"模型训练效果对比\")\n    plt.xlabel(\"轮次\")\n    plt.ylabel(\"损失值\")\n    plt.show()\n    # todo 绘制数据本身散点图\n    plt.scatter(x, y)\n    # todo 绘制模型预测结果和真实结果对比的图表\n    x = torch.linspace(x.min(), x.max(), 1000)\n    y_true = torch.tensor([v * coef + bias for v in x]) # 计算真实值\n    y_pred = torch.tensor([v * model.weight + model.bias for v in x]) # 计算预测值\n    plt.plot(x, y_true, label=\"真实\", color=\"red\")\n    plt.plot(x, y_pred, label=\"预测\", color=\"green\")\n    plt.legend()\n    plt.show()\n\n\nif __name__ == '__main__':\n    # 处理数据\n    bias = 14.5\n    x, y, coef = get_data(bias)\n    # 模型训练\n    epochs = 100\n    loss_list, model = model_train(x, y, epochs=epochs, batch_size=32, lr=0.01)\n    # 绘制图表\n    show_model_loss(epochs, loss_list, x, y, coef, model)\n```","slug":"DeepLearning/DeepLearningDay3_1","published":1,"date":"2025-10-15T06:42:56.766Z","updated":"2025-10-16T14:20:11.982Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cmgu7uyej0013jkv2fpghbq4m","content":"<h2 id=\"模型训练\"><a href=\"#模型训练\" class=\"headerlink\" title=\"模型训练\"></a>模型训练</h2><p>训练核心步骤如下：</p>\n<p><img src=\"/../../img/image-20251015144217715.png\" alt=\"深度学习模型训练核心步骤\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 导包</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_regression</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> TensorDataset</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> DataLoader</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> optim</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib</span><br><span class=\"line\"></span><br><span class=\"line\">matplotlib.use(<span class=\"string\">&#x27;TkAgg&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"comment\"># 在文件开头添加，忽略此警告</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> warnings</span><br><span class=\"line\"></span><br><span class=\"line\">warnings.filterwarnings(<span class=\"string\">&quot;ignore&quot;</span>, message=<span class=\"string\">&quot;.*__array_wrap__ must accept context.*&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 解决中文乱码问题</span></span><br><span class=\"line\">plt.rcParams[<span class=\"string\">&#x27;font.sans-serif&#x27;</span>] = [<span class=\"string\">&#x27;KaiTi&#x27;</span>]</span><br><span class=\"line\">plt.rcParams[<span class=\"string\">&#x27;axes.unicode_minus&#x27;</span>] = <span class=\"literal\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_data</span>(<span class=\"params\">bias</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 1.1 制作数据</span></span><br><span class=\"line\">    x, y, coef = make_regression(n_samples=<span class=\"number\">100</span>, n_features=<span class=\"number\">1</span>, n_targets=<span class=\"number\">1</span>, noise=<span class=\"number\">10</span>, coef=<span class=\"literal\">True</span>, bias=bias,</span><br><span class=\"line\">                                 random_state=<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 1.2 转换为张量</span></span><br><span class=\"line\">    x = torch.tensor(x, dtype=torch.<span class=\"built_in\">float</span>)</span><br><span class=\"line\">    y = torch.tensor(y, dtype=torch.<span class=\"built_in\">float</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(x.shape, y.shape)</span><br><span class=\"line\">    <span class=\"comment\"># 返回结果</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x, y, coef</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">model_train</span>(<span class=\"params\">x, y, epochs, batch_size, lr</span>):</span><br><span class=\"line\">    <span class=\"comment\"># TODO 1.准备数据</span></span><br><span class=\"line\">    <span class=\"comment\"># 1.3 创建数据集</span></span><br><span class=\"line\">    dataset = TensorDataset(x, y)</span><br><span class=\"line\">    <span class=\"comment\"># 1.4 创建数据加载器</span></span><br><span class=\"line\">    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"built_in\">len</span>(dataset))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"built_in\">len</span>(dataloader))  <span class=\"comment\"># 32 32 32 4</span></span><br><span class=\"line\">    <span class=\"comment\"># TODO 2.准备模型</span></span><br><span class=\"line\">    model = nn.Linear(in_features=<span class=\"number\">1</span>, out_features=<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"comment\"># TODO 3.准备损失函数</span></span><br><span class=\"line\">    loss_fn = nn.MSELoss()</span><br><span class=\"line\">    <span class=\"comment\"># TODO 4.准备优化器</span></span><br><span class=\"line\">    optimizer = optim.SGD(model.parameters(), lr=lr)</span><br><span class=\"line\">    <span class=\"comment\"># TODO 5.训练模型</span></span><br><span class=\"line\">    <span class=\"comment\"># 设置轮次</span></span><br><span class=\"line\">    epochs = epochs</span><br><span class=\"line\">    <span class=\"comment\"># todo 获取总损失,总批次,计算截止到当前轮损失并添加损失列表中,方便后续绘制图表</span></span><br><span class=\"line\">    total_loss, batch_cnt, loss_list = <span class=\"number\">0.0</span>, <span class=\"number\">0</span>, []</span><br><span class=\"line\">    <span class=\"comment\"># todo 遍历轮次</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(epochs):</span><br><span class=\"line\">        <span class=\"comment\"># todo 遍历批次数据</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> batch_x, batch_y <span class=\"keyword\">in</span> dataloader:</span><br><span class=\"line\">            <span class=\"comment\"># todo 前向传播</span></span><br><span class=\"line\">            <span class=\"comment\"># 计算预测数据</span></span><br><span class=\"line\">            y_pred = model(batch_x)</span><br><span class=\"line\">            <span class=\"comment\"># 计算损失</span></span><br><span class=\"line\">            loss = loss_fn(y_pred, batch_y.reshape(-<span class=\"number\">1</span>, <span class=\"number\">1</span>))  <span class=\"comment\"># 此处需将batch_y转换为[32,1],否则会产生形状不一致的广播问题,影响误差计算</span></span><br><span class=\"line\">            <span class=\"comment\"># todo 累计损失和批次数</span></span><br><span class=\"line\">            total_loss += loss.item()</span><br><span class=\"line\">            batch_cnt += <span class=\"number\">1</span></span><br><span class=\"line\">            <span class=\"comment\"># todo 反向传播</span></span><br><span class=\"line\">            <span class=\"comment\"># 梯度清零</span></span><br><span class=\"line\">            optimizer.zero_grad()</span><br><span class=\"line\">            <span class=\"comment\"># 计算梯度</span></span><br><span class=\"line\">            loss.backward()  <span class=\"comment\"># 此处底层自动求导数,把求出的导数赋值给w.grad属性</span></span><br><span class=\"line\">            <span class=\"comment\"># 参数更新</span></span><br><span class=\"line\">            optimizer.step()  <span class=\"comment\"># 此处底层还是 w旧-学习率*梯度</span></span><br><span class=\"line\">        <span class=\"comment\"># todo 计算截止到当前轮损失,打印日志并添加损失列表中,方便后续绘制图表</span></span><br><span class=\"line\">        epoch_loss = total_loss / batch_cnt</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;截止到第<span class=\"subst\">&#123;epoch + <span class=\"number\">1</span>&#125;</span>轮,最新损失为:<span class=\"subst\">&#123;epoch_loss&#125;</span>&quot;</span>)</span><br><span class=\"line\">        loss_list.append(epoch_loss)</span><br><span class=\"line\">    <span class=\"comment\"># todo 代码走到此处,说明训练完成,模型可以保存</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;训练完成,整体参数为:<span class=\"subst\">&#123;model.state_dict()&#125;</span>,权重为:<span class=\"subst\">&#123;model.weight&#125;</span>,偏置为:<span class=\"subst\">&#123;model.bias&#125;</span>&quot;</span>)</span><br><span class=\"line\">    torch.save(model.state_dict(), <span class=\"string\">&quot;model.pth&quot;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;模型保存成功&quot;</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 返回损失列表</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> loss_list, model</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">show_model_loss</span>(<span class=\"params\">epochs, loss_list, x, y, coef, model</span>):</span><br><span class=\"line\">    <span class=\"comment\"># todo 绘制每轮对应的损失折线图</span></span><br><span class=\"line\">    plt.plot(<span class=\"built_in\">range</span>(epochs), loss_list)</span><br><span class=\"line\">    plt.title(<span class=\"string\">&quot;模型训练效果对比&quot;</span>)</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">&quot;轮次&quot;</span>)</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">&quot;损失值&quot;</span>)</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\">    <span class=\"comment\"># todo 绘制数据本身散点图</span></span><br><span class=\"line\">    plt.scatter(x, y)</span><br><span class=\"line\">    <span class=\"comment\"># todo 绘制模型预测结果和真实结果对比的图表</span></span><br><span class=\"line\">    x = torch.linspace(x.<span class=\"built_in\">min</span>(), x.<span class=\"built_in\">max</span>(), <span class=\"number\">1000</span>)</span><br><span class=\"line\">    y_true = torch.tensor([v * coef + bias <span class=\"keyword\">for</span> v <span class=\"keyword\">in</span> x]) <span class=\"comment\"># 计算真实值</span></span><br><span class=\"line\">    y_pred = torch.tensor([v * model.weight + model.bias <span class=\"keyword\">for</span> v <span class=\"keyword\">in</span> x]) <span class=\"comment\"># 计算预测值</span></span><br><span class=\"line\">    plt.plot(x, y_true, label=<span class=\"string\">&quot;真实&quot;</span>, color=<span class=\"string\">&quot;red&quot;</span>)</span><br><span class=\"line\">    plt.plot(x, y_pred, label=<span class=\"string\">&quot;预测&quot;</span>, color=<span class=\"string\">&quot;green&quot;</span>)</span><br><span class=\"line\">    plt.legend()</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    <span class=\"comment\"># 处理数据</span></span><br><span class=\"line\">    bias = <span class=\"number\">14.5</span></span><br><span class=\"line\">    x, y, coef = get_data(bias)</span><br><span class=\"line\">    <span class=\"comment\"># 模型训练</span></span><br><span class=\"line\">    epochs = <span class=\"number\">100</span></span><br><span class=\"line\">    loss_list, model = model_train(x, y, epochs=epochs, batch_size=<span class=\"number\">32</span>, lr=<span class=\"number\">0.01</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 绘制图表</span></span><br><span class=\"line\">    show_model_loss(epochs, loss_list, x, y, coef, model)</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"模型训练\"><a href=\"#模型训练\" class=\"headerlink\" title=\"模型训练\"></a>模型训练</h2><p>训练核心步骤如下：</p>\n<p><img src=\"/../../img/image-20251015144217715.png\" alt=\"深度学习模型训练核心步骤\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 导包</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_regression</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> TensorDataset</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> DataLoader</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> optim</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib</span><br><span class=\"line\"></span><br><span class=\"line\">matplotlib.use(<span class=\"string\">&#x27;TkAgg&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"comment\"># 在文件开头添加，忽略此警告</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> warnings</span><br><span class=\"line\"></span><br><span class=\"line\">warnings.filterwarnings(<span class=\"string\">&quot;ignore&quot;</span>, message=<span class=\"string\">&quot;.*__array_wrap__ must accept context.*&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 解决中文乱码问题</span></span><br><span class=\"line\">plt.rcParams[<span class=\"string\">&#x27;font.sans-serif&#x27;</span>] = [<span class=\"string\">&#x27;KaiTi&#x27;</span>]</span><br><span class=\"line\">plt.rcParams[<span class=\"string\">&#x27;axes.unicode_minus&#x27;</span>] = <span class=\"literal\">False</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_data</span>(<span class=\"params\">bias</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 1.1 制作数据</span></span><br><span class=\"line\">    x, y, coef = make_regression(n_samples=<span class=\"number\">100</span>, n_features=<span class=\"number\">1</span>, n_targets=<span class=\"number\">1</span>, noise=<span class=\"number\">10</span>, coef=<span class=\"literal\">True</span>, bias=bias,</span><br><span class=\"line\">                                 random_state=<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 1.2 转换为张量</span></span><br><span class=\"line\">    x = torch.tensor(x, dtype=torch.<span class=\"built_in\">float</span>)</span><br><span class=\"line\">    y = torch.tensor(y, dtype=torch.<span class=\"built_in\">float</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(x.shape, y.shape)</span><br><span class=\"line\">    <span class=\"comment\"># 返回结果</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x, y, coef</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">model_train</span>(<span class=\"params\">x, y, epochs, batch_size, lr</span>):</span><br><span class=\"line\">    <span class=\"comment\"># TODO 1.准备数据</span></span><br><span class=\"line\">    <span class=\"comment\"># 1.3 创建数据集</span></span><br><span class=\"line\">    dataset = TensorDataset(x, y)</span><br><span class=\"line\">    <span class=\"comment\"># 1.4 创建数据加载器</span></span><br><span class=\"line\">    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"built_in\">len</span>(dataset))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"built_in\">len</span>(dataloader))  <span class=\"comment\"># 32 32 32 4</span></span><br><span class=\"line\">    <span class=\"comment\"># TODO 2.准备模型</span></span><br><span class=\"line\">    model = nn.Linear(in_features=<span class=\"number\">1</span>, out_features=<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"comment\"># TODO 3.准备损失函数</span></span><br><span class=\"line\">    loss_fn = nn.MSELoss()</span><br><span class=\"line\">    <span class=\"comment\"># TODO 4.准备优化器</span></span><br><span class=\"line\">    optimizer = optim.SGD(model.parameters(), lr=lr)</span><br><span class=\"line\">    <span class=\"comment\"># TODO 5.训练模型</span></span><br><span class=\"line\">    <span class=\"comment\"># 设置轮次</span></span><br><span class=\"line\">    epochs = epochs</span><br><span class=\"line\">    <span class=\"comment\"># todo 获取总损失,总批次,计算截止到当前轮损失并添加损失列表中,方便后续绘制图表</span></span><br><span class=\"line\">    total_loss, batch_cnt, loss_list = <span class=\"number\">0.0</span>, <span class=\"number\">0</span>, []</span><br><span class=\"line\">    <span class=\"comment\"># todo 遍历轮次</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(epochs):</span><br><span class=\"line\">        <span class=\"comment\"># todo 遍历批次数据</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> batch_x, batch_y <span class=\"keyword\">in</span> dataloader:</span><br><span class=\"line\">            <span class=\"comment\"># todo 前向传播</span></span><br><span class=\"line\">            <span class=\"comment\"># 计算预测数据</span></span><br><span class=\"line\">            y_pred = model(batch_x)</span><br><span class=\"line\">            <span class=\"comment\"># 计算损失</span></span><br><span class=\"line\">            loss = loss_fn(y_pred, batch_y.reshape(-<span class=\"number\">1</span>, <span class=\"number\">1</span>))  <span class=\"comment\"># 此处需将batch_y转换为[32,1],否则会产生形状不一致的广播问题,影响误差计算</span></span><br><span class=\"line\">            <span class=\"comment\"># todo 累计损失和批次数</span></span><br><span class=\"line\">            total_loss += loss.item()</span><br><span class=\"line\">            batch_cnt += <span class=\"number\">1</span></span><br><span class=\"line\">            <span class=\"comment\"># todo 反向传播</span></span><br><span class=\"line\">            <span class=\"comment\"># 梯度清零</span></span><br><span class=\"line\">            optimizer.zero_grad()</span><br><span class=\"line\">            <span class=\"comment\"># 计算梯度</span></span><br><span class=\"line\">            loss.backward()  <span class=\"comment\"># 此处底层自动求导数,把求出的导数赋值给w.grad属性</span></span><br><span class=\"line\">            <span class=\"comment\"># 参数更新</span></span><br><span class=\"line\">            optimizer.step()  <span class=\"comment\"># 此处底层还是 w旧-学习率*梯度</span></span><br><span class=\"line\">        <span class=\"comment\"># todo 计算截止到当前轮损失,打印日志并添加损失列表中,方便后续绘制图表</span></span><br><span class=\"line\">        epoch_loss = total_loss / batch_cnt</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;截止到第<span class=\"subst\">&#123;epoch + <span class=\"number\">1</span>&#125;</span>轮,最新损失为:<span class=\"subst\">&#123;epoch_loss&#125;</span>&quot;</span>)</span><br><span class=\"line\">        loss_list.append(epoch_loss)</span><br><span class=\"line\">    <span class=\"comment\"># todo 代码走到此处,说明训练完成,模型可以保存</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;训练完成,整体参数为:<span class=\"subst\">&#123;model.state_dict()&#125;</span>,权重为:<span class=\"subst\">&#123;model.weight&#125;</span>,偏置为:<span class=\"subst\">&#123;model.bias&#125;</span>&quot;</span>)</span><br><span class=\"line\">    torch.save(model.state_dict(), <span class=\"string\">&quot;model.pth&quot;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;模型保存成功&quot;</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 返回损失列表</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> loss_list, model</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">show_model_loss</span>(<span class=\"params\">epochs, loss_list, x, y, coef, model</span>):</span><br><span class=\"line\">    <span class=\"comment\"># todo 绘制每轮对应的损失折线图</span></span><br><span class=\"line\">    plt.plot(<span class=\"built_in\">range</span>(epochs), loss_list)</span><br><span class=\"line\">    plt.title(<span class=\"string\">&quot;模型训练效果对比&quot;</span>)</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">&quot;轮次&quot;</span>)</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">&quot;损失值&quot;</span>)</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\">    <span class=\"comment\"># todo 绘制数据本身散点图</span></span><br><span class=\"line\">    plt.scatter(x, y)</span><br><span class=\"line\">    <span class=\"comment\"># todo 绘制模型预测结果和真实结果对比的图表</span></span><br><span class=\"line\">    x = torch.linspace(x.<span class=\"built_in\">min</span>(), x.<span class=\"built_in\">max</span>(), <span class=\"number\">1000</span>)</span><br><span class=\"line\">    y_true = torch.tensor([v * coef + bias <span class=\"keyword\">for</span> v <span class=\"keyword\">in</span> x]) <span class=\"comment\"># 计算真实值</span></span><br><span class=\"line\">    y_pred = torch.tensor([v * model.weight + model.bias <span class=\"keyword\">for</span> v <span class=\"keyword\">in</span> x]) <span class=\"comment\"># 计算预测值</span></span><br><span class=\"line\">    plt.plot(x, y_true, label=<span class=\"string\">&quot;真实&quot;</span>, color=<span class=\"string\">&quot;red&quot;</span>)</span><br><span class=\"line\">    plt.plot(x, y_pred, label=<span class=\"string\">&quot;预测&quot;</span>, color=<span class=\"string\">&quot;green&quot;</span>)</span><br><span class=\"line\">    plt.legend()</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    <span class=\"comment\"># 处理数据</span></span><br><span class=\"line\">    bias = <span class=\"number\">14.5</span></span><br><span class=\"line\">    x, y, coef = get_data(bias)</span><br><span class=\"line\">    <span class=\"comment\"># 模型训练</span></span><br><span class=\"line\">    epochs = <span class=\"number\">100</span></span><br><span class=\"line\">    loss_list, model = model_train(x, y, epochs=epochs, batch_size=<span class=\"number\">32</span>, lr=<span class=\"number\">0.01</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 绘制图表</span></span><br><span class=\"line\">    show_model_loss(epochs, loss_list, x, y, coef, model)</span><br></pre></td></tr></table></figure>"},{"title":"人工神经网络-sigmoid函数","Label":"-- NN -- DeepLearning -- Artificial Neural Network","_content":"\n## 人工神经网络\n\n### 简介\n\n**人工神经网络**是模仿人脑神经元结构的一种计算框架或模型，工作流程分为**两个主要步骤**：\n\n* 1.加权求和（线性部分）：\n  \n  * 神经元接收来自上一层的所有输入 x1,x2,…。\n  * 每个输入都乘上一个相应的**权重** w1,w2,…（表示输入的重要性）。\n  * 将所有加权输入求和，并加上一个**偏置** b：\n  * z=x1w1+x2w2+⋯+xnwn+b=xw+b\n  \n* 2.激活（非线性部分）：\n  \n  * 将加权和 z 输入到**激活函数** f(⋅) 中。\n  \n  * 激活函数引入了**非线性**，使得网络模型可以逼近任意函数，提升网络对复杂问题的拟合能力，决定了神经元最终的输出 a，并将信号传递给下一层：\n  \n  * a=f(z)=f(xw+b)\n  \n  * 四大激活函数：\n  \n    * sigmoid：将任意的输入映射到0和1之间，当输入的值大致在 **<-6** **或者** **>6** 时，意味着输入任何值得到的激活值都是差不多的，这样会丢失部分的信息。比如：输入 100 和输出 10000 经过 sigmoid 的激活值几乎都是等于 1 的，但是输入的数据之间相差 100 倍的信息就丢失了。\n    * 对于 sigmoid 函数而言，输入值在 **[-6, 6]** 之间输出值才会**有明显差异**，输入值在 **[-3, 3]** 之间才会**有比较好的效果**\n    * 通过导数图像，我们发现**导数数值范围是** **(0, 0.25)**，当输入 <-6 或者 >6 时，sigmoid 激活函数图像的**导数接近为** **0**，此时**网络参数将更新极其缓慢，或者无法更新。**\n    * 一般来说， sigmoid 网络在 **5** **层之内**就会产生**梯度消失**现象。而且，该激活函数并不是以 0 为中心的，所以在实践中这种激活函数使用的很少。**sigmoid函数一般只用于二分类的输出层**。\n  \n    ![sigmoid函数图像/导数图像](../../img/image-20251016210747455.png)\n  \n    * relu\n    * tanh\n    * SoftMax\n\n**神经网络结构**：\n\n![神经网络结构](../../img/image-20251015234446051.png)\n\n* 输入层：接受原始数据\n* 隐藏层：预测数据计算损失函数\n* 输出层：输出预测值，根据损失值再更新参数（权重（w），偏置（b））\n\n权重和偏置可以通过优化算法（如SGD）不断调整以达到预测结果与真实值之间损失值的最小化\n\n**前向传播：**\n\n从输入层开始传输数据到隐藏层中每个神经元加权求和，并通过激活函数产生输出，最后传到输出层产生一个预测值y\n\n**计算损失**：\n\n将预测和真实标签比较，通过损失函数计算差值，得到损失值\n\n**反向传播&优化：**\n\n通过**反向传播算法**，计算损失值 J 对网络中每一个权重和偏置的**梯度**（即损失值相对于参数的变化率）。\n\n**梯度**指明了调整参数以减少损失值的方向。\n\n步骤：\n\n* 1.梯度清零\n* 2.基于损失值计算梯度值\n* 3.更新参数\n\n**流程总结**\n\n通过不断重复这个“前向传播 → 计算损失 → 反向传播 → 调整参数”的循环，神经网络的模型能力就得到了提升。\n\n\n\n","source":"_posts/DeepLearning/DeepLearningDay3_2.md","raw":"---\ntitle: 人工神经网络-sigmoid函数\ncategories:\n  - - DeepLearning\nLabel:\n  -- NN\n  -- DeepLearning\n  -- Artificial Neural Network\n---\n\n## 人工神经网络\n\n### 简介\n\n**人工神经网络**是模仿人脑神经元结构的一种计算框架或模型，工作流程分为**两个主要步骤**：\n\n* 1.加权求和（线性部分）：\n  \n  * 神经元接收来自上一层的所有输入 x1,x2,…。\n  * 每个输入都乘上一个相应的**权重** w1,w2,…（表示输入的重要性）。\n  * 将所有加权输入求和，并加上一个**偏置** b：\n  * z=x1w1+x2w2+⋯+xnwn+b=xw+b\n  \n* 2.激活（非线性部分）：\n  \n  * 将加权和 z 输入到**激活函数** f(⋅) 中。\n  \n  * 激活函数引入了**非线性**，使得网络模型可以逼近任意函数，提升网络对复杂问题的拟合能力，决定了神经元最终的输出 a，并将信号传递给下一层：\n  \n  * a=f(z)=f(xw+b)\n  \n  * 四大激活函数：\n  \n    * sigmoid：将任意的输入映射到0和1之间，当输入的值大致在 **<-6** **或者** **>6** 时，意味着输入任何值得到的激活值都是差不多的，这样会丢失部分的信息。比如：输入 100 和输出 10000 经过 sigmoid 的激活值几乎都是等于 1 的，但是输入的数据之间相差 100 倍的信息就丢失了。\n    * 对于 sigmoid 函数而言，输入值在 **[-6, 6]** 之间输出值才会**有明显差异**，输入值在 **[-3, 3]** 之间才会**有比较好的效果**\n    * 通过导数图像，我们发现**导数数值范围是** **(0, 0.25)**，当输入 <-6 或者 >6 时，sigmoid 激活函数图像的**导数接近为** **0**，此时**网络参数将更新极其缓慢，或者无法更新。**\n    * 一般来说， sigmoid 网络在 **5** **层之内**就会产生**梯度消失**现象。而且，该激活函数并不是以 0 为中心的，所以在实践中这种激活函数使用的很少。**sigmoid函数一般只用于二分类的输出层**。\n  \n    ![sigmoid函数图像/导数图像](../../img/image-20251016210747455.png)\n  \n    * relu\n    * tanh\n    * SoftMax\n\n**神经网络结构**：\n\n![神经网络结构](../../img/image-20251015234446051.png)\n\n* 输入层：接受原始数据\n* 隐藏层：预测数据计算损失函数\n* 输出层：输出预测值，根据损失值再更新参数（权重（w），偏置（b））\n\n权重和偏置可以通过优化算法（如SGD）不断调整以达到预测结果与真实值之间损失值的最小化\n\n**前向传播：**\n\n从输入层开始传输数据到隐藏层中每个神经元加权求和，并通过激活函数产生输出，最后传到输出层产生一个预测值y\n\n**计算损失**：\n\n将预测和真实标签比较，通过损失函数计算差值，得到损失值\n\n**反向传播&优化：**\n\n通过**反向传播算法**，计算损失值 J 对网络中每一个权重和偏置的**梯度**（即损失值相对于参数的变化率）。\n\n**梯度**指明了调整参数以减少损失值的方向。\n\n步骤：\n\n* 1.梯度清零\n* 2.基于损失值计算梯度值\n* 3.更新参数\n\n**流程总结**\n\n通过不断重复这个“前向传播 → 计算损失 → 反向传播 → 调整参数”的循环，神经网络的模型能力就得到了提升。\n\n\n\n","slug":"DeepLearning/DeepLearningDay3_2","published":1,"date":"2025-10-15T06:42:56.766Z","updated":"2025-10-16T14:13:09.728Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cmgu7uyek0015jkv282ew4myn","content":"<h2 id=\"人工神经网络\"><a href=\"#人工神经网络\" class=\"headerlink\" title=\"人工神经网络\"></a>人工神经网络</h2><h3 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h3><p><strong>人工神经网络</strong>是模仿人脑神经元结构的一种计算框架或模型，工作流程分为<strong>两个主要步骤</strong>：</p>\n<ul>\n<li><p>1.加权求和（线性部分）：</p>\n<ul>\n<li>神经元接收来自上一层的所有输入 x1,x2,…。</li>\n<li>每个输入都乘上一个相应的<strong>权重</strong> w1,w2,…（表示输入的重要性）。</li>\n<li>将所有加权输入求和，并加上一个<strong>偏置</strong> b：</li>\n<li>z&#x3D;x1w1+x2w2+⋯+xnwn+b&#x3D;xw+b</li>\n</ul>\n</li>\n<li><p>2.激活（非线性部分）：</p>\n<ul>\n<li><p>将加权和 z 输入到<strong>激活函数</strong> f(⋅) 中。</p>\n</li>\n<li><p>激活函数引入了<strong>非线性</strong>，使得网络模型可以逼近任意函数，提升网络对复杂问题的拟合能力，决定了神经元最终的输出 a，并将信号传递给下一层：</p>\n</li>\n<li><p>a&#x3D;f(z)&#x3D;f(xw+b)</p>\n</li>\n<li><p>四大激活函数：</p>\n<ul>\n<li>sigmoid：将任意的输入映射到0和1之间，当输入的值大致在 <strong>&lt;-6** **或者** **&gt;6</strong> 时，意味着输入任何值得到的激活值都是差不多的，这样会丢失部分的信息。比如：输入 100 和输出 10000 经过 sigmoid 的激活值几乎都是等于 1 的，但是输入的数据之间相差 100 倍的信息就丢失了。</li>\n<li>对于 sigmoid 函数而言，输入值在 <strong>[-6, 6]</strong> 之间输出值才会<strong>有明显差异</strong>，输入值在 <strong>[-3, 3]</strong> 之间才会<strong>有比较好的效果</strong></li>\n<li>通过导数图像，我们发现<strong>导数数值范围是</strong> <strong>(0, 0.25)<strong>，当输入 &lt;-6 或者 &gt;6 时，sigmoid 激活函数图像的</strong>导数接近为</strong> <strong>0</strong>，此时<strong>网络参数将更新极其缓慢，或者无法更新。</strong></li>\n<li>一般来说， sigmoid 网络在 <strong>5</strong> <strong>层之内</strong>就会产生<strong>梯度消失</strong>现象。而且，该激活函数并不是以 0 为中心的，所以在实践中这种激活函数使用的很少。<strong>sigmoid函数一般只用于二分类的输出层</strong>。</li>\n</ul>\n<p><img src=\"/../../img/image-20251016210747455.png\" alt=\"sigmoid函数图像/导数图像\"></p>\n<ul>\n<li>relu</li>\n<li>tanh</li>\n<li>SoftMax</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>神经网络结构</strong>：</p>\n<p><img src=\"/../../img/image-20251015234446051.png\" alt=\"神经网络结构\"></p>\n<ul>\n<li>输入层：接受原始数据</li>\n<li>隐藏层：预测数据计算损失函数</li>\n<li>输出层：输出预测值，根据损失值再更新参数（权重（w），偏置（b））</li>\n</ul>\n<p>权重和偏置可以通过优化算法（如SGD）不断调整以达到预测结果与真实值之间损失值的最小化</p>\n<p><strong>前向传播：</strong></p>\n<p>从输入层开始传输数据到隐藏层中每个神经元加权求和，并通过激活函数产生输出，最后传到输出层产生一个预测值y</p>\n<p><strong>计算损失</strong>：</p>\n<p>将预测和真实标签比较，通过损失函数计算差值，得到损失值</p>\n<p><strong>反向传播&amp;优化：</strong></p>\n<p>通过<strong>反向传播算法</strong>，计算损失值 J 对网络中每一个权重和偏置的<strong>梯度</strong>（即损失值相对于参数的变化率）。</p>\n<p><strong>梯度</strong>指明了调整参数以减少损失值的方向。</p>\n<p>步骤：</p>\n<ul>\n<li>1.梯度清零</li>\n<li>2.基于损失值计算梯度值</li>\n<li>3.更新参数</li>\n</ul>\n<p><strong>流程总结</strong></p>\n<p>通过不断重复这个“前向传播 → 计算损失 → 反向传播 → 调整参数”的循环，神经网络的模型能力就得到了提升。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"人工神经网络\"><a href=\"#人工神经网络\" class=\"headerlink\" title=\"人工神经网络\"></a>人工神经网络</h2><h3 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h3><p><strong>人工神经网络</strong>是模仿人脑神经元结构的一种计算框架或模型，工作流程分为<strong>两个主要步骤</strong>：</p>\n<ul>\n<li><p>1.加权求和（线性部分）：</p>\n<ul>\n<li>神经元接收来自上一层的所有输入 x1,x2,…。</li>\n<li>每个输入都乘上一个相应的<strong>权重</strong> w1,w2,…（表示输入的重要性）。</li>\n<li>将所有加权输入求和，并加上一个<strong>偏置</strong> b：</li>\n<li>z&#x3D;x1w1+x2w2+⋯+xnwn+b&#x3D;xw+b</li>\n</ul>\n</li>\n<li><p>2.激活（非线性部分）：</p>\n<ul>\n<li><p>将加权和 z 输入到<strong>激活函数</strong> f(⋅) 中。</p>\n</li>\n<li><p>激活函数引入了<strong>非线性</strong>，使得网络模型可以逼近任意函数，提升网络对复杂问题的拟合能力，决定了神经元最终的输出 a，并将信号传递给下一层：</p>\n</li>\n<li><p>a&#x3D;f(z)&#x3D;f(xw+b)</p>\n</li>\n<li><p>四大激活函数：</p>\n<ul>\n<li>sigmoid：将任意的输入映射到0和1之间，当输入的值大致在 <strong>&lt;-6** **或者** **&gt;6</strong> 时，意味着输入任何值得到的激活值都是差不多的，这样会丢失部分的信息。比如：输入 100 和输出 10000 经过 sigmoid 的激活值几乎都是等于 1 的，但是输入的数据之间相差 100 倍的信息就丢失了。</li>\n<li>对于 sigmoid 函数而言，输入值在 <strong>[-6, 6]</strong> 之间输出值才会<strong>有明显差异</strong>，输入值在 <strong>[-3, 3]</strong> 之间才会<strong>有比较好的效果</strong></li>\n<li>通过导数图像，我们发现<strong>导数数值范围是</strong> <strong>(0, 0.25)<strong>，当输入 &lt;-6 或者 &gt;6 时，sigmoid 激活函数图像的</strong>导数接近为</strong> <strong>0</strong>，此时<strong>网络参数将更新极其缓慢，或者无法更新。</strong></li>\n<li>一般来说， sigmoid 网络在 <strong>5</strong> <strong>层之内</strong>就会产生<strong>梯度消失</strong>现象。而且，该激活函数并不是以 0 为中心的，所以在实践中这种激活函数使用的很少。<strong>sigmoid函数一般只用于二分类的输出层</strong>。</li>\n</ul>\n<p><img src=\"/../../img/image-20251016210747455.png\" alt=\"sigmoid函数图像/导数图像\"></p>\n<ul>\n<li>relu</li>\n<li>tanh</li>\n<li>SoftMax</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><strong>神经网络结构</strong>：</p>\n<p><img src=\"/../../img/image-20251015234446051.png\" alt=\"神经网络结构\"></p>\n<ul>\n<li>输入层：接受原始数据</li>\n<li>隐藏层：预测数据计算损失函数</li>\n<li>输出层：输出预测值，根据损失值再更新参数（权重（w），偏置（b））</li>\n</ul>\n<p>权重和偏置可以通过优化算法（如SGD）不断调整以达到预测结果与真实值之间损失值的最小化</p>\n<p><strong>前向传播：</strong></p>\n<p>从输入层开始传输数据到隐藏层中每个神经元加权求和，并通过激活函数产生输出，最后传到输出层产生一个预测值y</p>\n<p><strong>计算损失</strong>：</p>\n<p>将预测和真实标签比较，通过损失函数计算差值，得到损失值</p>\n<p><strong>反向传播&amp;优化：</strong></p>\n<p>通过<strong>反向传播算法</strong>，计算损失值 J 对网络中每一个权重和偏置的<strong>梯度</strong>（即损失值相对于参数的变化率）。</p>\n<p><strong>梯度</strong>指明了调整参数以减少损失值的方向。</p>\n<p>步骤：</p>\n<ul>\n<li>1.梯度清零</li>\n<li>2.基于损失值计算梯度值</li>\n<li>3.更新参数</li>\n</ul>\n<p><strong>流程总结</strong></p>\n<p>通过不断重复这个“前向传播 → 计算损失 → 反向传播 → 调整参数”的循环，神经网络的模型能力就得到了提升。</p>\n"},{"title":"在使用DataX时发生的一些问题","_content":"\n\n\n### 1.在第一次安装后测试自检时报错datax/plugin/reader/._xxxx/plugin.json]不存在\n\n**自检代码：**\n\n`python /opt/module/datax/bin/datax.py /opt/module/datax/job/job.json`\n\n**报错：**\n\n `[main] WARN  ConfigParser - 插件[streamreader,streamwriter]加载失败，1s后重试... Exception:Code:[Common-00], Describe:[您提供的配置文件存在错误信息，请检查您的作业配置 .] - 配置信息错误，您提供的配置文件[/home/hadoop/apps/datax/plugin/reader/._drdsreader/plugin.json]不存在. 请检查您的配置文件.` \n\n**解决方法：**\n\n删除datax/plugin目录下自带的临时文件\n\n**代码如下：**\n\nfind ./datax/plugin/* -type f -name \".*er\" | [xargs]\n\n### MySQL写数据到HDFS时，执行datax脚本报错\n\n![error](./img/1.png)\n\n然而在我折腾了一下午jdbc的配置后（在navicat上测试远程连接，idea上用jdbc远程连接成功）发现无事发生，还是同样的报错.....\n\n![navicat](../img/image-20240911233804427.png)\n\n![idea中JDBC测试](../img/image-20240911234004466.png)\n\n🤡\n\n怀疑虚拟机的JDBC驱动没装，在官网上下好包后在虚拟机装上发现还是同样的报错，最后发现datax的依赖库中jdbc版本太低，导致无法远程连接（官网下载最新的包替换问题解决）...😅\n\n![我请问呢？](../img/image-20240911234722046.png)\n\n","source":"_posts/BigData/DataX/20240911.md","raw":"---\ntitle: 在使用DataX时发生的一些问题\ntags:\n  - DataX\ncategories:\n  - - BigData\n  - - DataX\n---\n\n\n\n### 1.在第一次安装后测试自检时报错datax/plugin/reader/._xxxx/plugin.json]不存在\n\n**自检代码：**\n\n`python /opt/module/datax/bin/datax.py /opt/module/datax/job/job.json`\n\n**报错：**\n\n `[main] WARN  ConfigParser - 插件[streamreader,streamwriter]加载失败，1s后重试... Exception:Code:[Common-00], Describe:[您提供的配置文件存在错误信息，请检查您的作业配置 .] - 配置信息错误，您提供的配置文件[/home/hadoop/apps/datax/plugin/reader/._drdsreader/plugin.json]不存在. 请检查您的配置文件.` \n\n**解决方法：**\n\n删除datax/plugin目录下自带的临时文件\n\n**代码如下：**\n\nfind ./datax/plugin/* -type f -name \".*er\" | [xargs]\n\n### MySQL写数据到HDFS时，执行datax脚本报错\n\n![error](./img/1.png)\n\n然而在我折腾了一下午jdbc的配置后（在navicat上测试远程连接，idea上用jdbc远程连接成功）发现无事发生，还是同样的报错.....\n\n![navicat](../img/image-20240911233804427.png)\n\n![idea中JDBC测试](../img/image-20240911234004466.png)\n\n🤡\n\n怀疑虚拟机的JDBC驱动没装，在官网上下好包后在虚拟机装上发现还是同样的报错，最后发现datax的依赖库中jdbc版本太低，导致无法远程连接（官网下载最新的包替换问题解决）...😅\n\n![我请问呢？](../img/image-20240911234722046.png)\n\n","slug":"BigData/DataX/20240911","published":1,"date":"2024-09-11T14:25:03.527Z","updated":"2024-09-12T06:26:33.531Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cmgu7uyel0017jkv2hogldwaq","content":"<h3 id=\"1-在第一次安装后测试自检时报错datax-plugin-reader-xxxx-plugin-json-不存在\"><a href=\"#1-在第一次安装后测试自检时报错datax-plugin-reader-xxxx-plugin-json-不存在\" class=\"headerlink\" title=\"1.在第一次安装后测试自检时报错datax&#x2F;plugin&#x2F;reader&#x2F;._xxxx&#x2F;plugin.json]不存在\"></a>1.在第一次安装后测试自检时报错datax&#x2F;plugin&#x2F;reader&#x2F;._xxxx&#x2F;plugin.json]不存在</h3><p><strong>自检代码：</strong></p>\n<p><code>python /opt/module/datax/bin/datax.py /opt/module/datax/job/job.json</code></p>\n<p><strong>报错：</strong></p>\n<p> <code>[main] WARN  ConfigParser - 插件[streamreader,streamwriter]加载失败，1s后重试... Exception:Code:[Common-00], Describe:[您提供的配置文件存在错误信息，请检查您的作业配置 .] - 配置信息错误，您提供的配置文件[/home/hadoop/apps/datax/plugin/reader/._drdsreader/plugin.json]不存在. 请检查您的配置文件.</code> </p>\n<p><strong>解决方法：</strong></p>\n<p>删除datax&#x2F;plugin目录下自带的临时文件</p>\n<p><strong>代码如下：</strong></p>\n<p>find .&#x2F;datax&#x2F;plugin&#x2F;* -type f -name “.*er” | [xargs]</p>\n<h3 id=\"MySQL写数据到HDFS时，执行datax脚本报错\"><a href=\"#MySQL写数据到HDFS时，执行datax脚本报错\" class=\"headerlink\" title=\"MySQL写数据到HDFS时，执行datax脚本报错\"></a>MySQL写数据到HDFS时，执行datax脚本报错</h3><p><img src=\"/./img/1.png\" alt=\"error\"></p>\n<p>然而在我折腾了一下午jdbc的配置后（在navicat上测试远程连接，idea上用jdbc远程连接成功）发现无事发生，还是同样的报错…..</p>\n<p><img src=\"/../img/image-20240911233804427.png\" alt=\"navicat\"></p>\n<p><img src=\"/../img/image-20240911234004466.png\" alt=\"idea中JDBC测试\"></p>\n<p>🤡</p>\n<p>怀疑虚拟机的JDBC驱动没装，在官网上下好包后在虚拟机装上发现还是同样的报错，最后发现datax的依赖库中jdbc版本太低，导致无法远程连接（官网下载最新的包替换问题解决）…😅</p>\n<p><img src=\"/../img/image-20240911234722046.png\" alt=\"我请问呢？\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-在第一次安装后测试自检时报错datax-plugin-reader-xxxx-plugin-json-不存在\"><a href=\"#1-在第一次安装后测试自检时报错datax-plugin-reader-xxxx-plugin-json-不存在\" class=\"headerlink\" title=\"1.在第一次安装后测试自检时报错datax&#x2F;plugin&#x2F;reader&#x2F;._xxxx&#x2F;plugin.json]不存在\"></a>1.在第一次安装后测试自检时报错datax&#x2F;plugin&#x2F;reader&#x2F;._xxxx&#x2F;plugin.json]不存在</h3><p><strong>自检代码：</strong></p>\n<p><code>python /opt/module/datax/bin/datax.py /opt/module/datax/job/job.json</code></p>\n<p><strong>报错：</strong></p>\n<p> <code>[main] WARN  ConfigParser - 插件[streamreader,streamwriter]加载失败，1s后重试... Exception:Code:[Common-00], Describe:[您提供的配置文件存在错误信息，请检查您的作业配置 .] - 配置信息错误，您提供的配置文件[/home/hadoop/apps/datax/plugin/reader/._drdsreader/plugin.json]不存在. 请检查您的配置文件.</code> </p>\n<p><strong>解决方法：</strong></p>\n<p>删除datax&#x2F;plugin目录下自带的临时文件</p>\n<p><strong>代码如下：</strong></p>\n<p>find .&#x2F;datax&#x2F;plugin&#x2F;* -type f -name “.*er” | [xargs]</p>\n<h3 id=\"MySQL写数据到HDFS时，执行datax脚本报错\"><a href=\"#MySQL写数据到HDFS时，执行datax脚本报错\" class=\"headerlink\" title=\"MySQL写数据到HDFS时，执行datax脚本报错\"></a>MySQL写数据到HDFS时，执行datax脚本报错</h3><p><img src=\"/./img/1.png\" alt=\"error\"></p>\n<p>然而在我折腾了一下午jdbc的配置后（在navicat上测试远程连接，idea上用jdbc远程连接成功）发现无事发生，还是同样的报错…..</p>\n<p><img src=\"/../img/image-20240911233804427.png\" alt=\"navicat\"></p>\n<p><img src=\"/../img/image-20240911234004466.png\" alt=\"idea中JDBC测试\"></p>\n<p>🤡</p>\n<p>怀疑虚拟机的JDBC驱动没装，在官网上下好包后在虚拟机装上发现还是同样的报错，最后发现datax的依赖库中jdbc版本太低，导致无法远程连接（官网下载最新的包替换问题解决）…😅</p>\n<p><img src=\"/../img/image-20240911234722046.png\" alt=\"我请问呢？\"></p>\n"},{"title":"Hive on Spark 使用insert时报错Failed to create Spark client for Spark session XXX  return code  30041","_content":"\n情况如下：\n\n本来hive on spark是正常的状态，增删改查都没有问题，直到一天虚拟机因为笔记本非正常关机后重启完虚拟机仍在工作，\n\n![image-20241005192256436](../../../img/image-20241005192256436.png)\n\n然后在关闭集群时，node02，03正常关闭，node01死活关闭不了（关闭虚拟机后再次打开node01还是运行状态），直接任务管理器强退相关任务和服务后才关闭，最后导致metastore数据库异常，hive执行插入操作异常，报错：\n\n`[2024-10-05 18:28:26] [42000][30041] Error while processing statement: FAILED: Execution Error, return code 30041 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. Failed to create Spark client for Spark session d8f9be8d-27ba-464b-af51-94b5e3275c47`\n\n解决方法:\n\n~~删除metastore重新创建并初始化元数据库:~~\n\n~~`mysql> drop database metastore;`~~\n\n~~`mysql> create database metastore;`~~\n\n~~`[root@node01 conf]$ schematool -initSchema -dbType mysql -verbose`~~\n\n~~然后重启hive和集群执行插入数据测试：~~\n\n~~![image-20241005193544038](../../../img/image-20241005193544038.png)~~\n\n~~成功~~\n\n十几分钟后再次使用又遇到同样的报错，\n\n![spark-standalone](../../../img/image-20241006090215878.png)\n\n然后发现spark用的是大学上课装的老版本，而我的/etc/profile配置文件里是这样的：\n\n![profile](../../../img/image-20241006123230093.png)\n\n（即使/etc/profile.d里配置的SPARK_HOME是正确的路径）\n\n删除老版本Spark再修改profile文件Spark_home位置后，Hive on Spark再次复活。\n\n![复活证明](../../../img/image-20241006172059609.png)\n","source":"_posts/BigData/Hive on Spark/新建 文本文档.md","raw":"---\ntitle: >-\n  Hive on Spark 使用insert时报错Failed to create Spark client for Spark session XXX \n  return code  30041\ntag:\n  - spark\ncategories:\n  - - BigData\n  - - Hive on Spark\n---\n\n情况如下：\n\n本来hive on spark是正常的状态，增删改查都没有问题，直到一天虚拟机因为笔记本非正常关机后重启完虚拟机仍在工作，\n\n![image-20241005192256436](../../../img/image-20241005192256436.png)\n\n然后在关闭集群时，node02，03正常关闭，node01死活关闭不了（关闭虚拟机后再次打开node01还是运行状态），直接任务管理器强退相关任务和服务后才关闭，最后导致metastore数据库异常，hive执行插入操作异常，报错：\n\n`[2024-10-05 18:28:26] [42000][30041] Error while processing statement: FAILED: Execution Error, return code 30041 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. Failed to create Spark client for Spark session d8f9be8d-27ba-464b-af51-94b5e3275c47`\n\n解决方法:\n\n~~删除metastore重新创建并初始化元数据库:~~\n\n~~`mysql> drop database metastore;`~~\n\n~~`mysql> create database metastore;`~~\n\n~~`[root@node01 conf]$ schematool -initSchema -dbType mysql -verbose`~~\n\n~~然后重启hive和集群执行插入数据测试：~~\n\n~~![image-20241005193544038](../../../img/image-20241005193544038.png)~~\n\n~~成功~~\n\n十几分钟后再次使用又遇到同样的报错，\n\n![spark-standalone](../../../img/image-20241006090215878.png)\n\n然后发现spark用的是大学上课装的老版本，而我的/etc/profile配置文件里是这样的：\n\n![profile](../../../img/image-20241006123230093.png)\n\n（即使/etc/profile.d里配置的SPARK_HOME是正确的路径）\n\n删除老版本Spark再修改profile文件Spark_home位置后，Hive on Spark再次复活。\n\n![复活证明](../../../img/image-20241006172059609.png)\n","slug":"BigData/Hive on Spark/新建 文本文档","published":1,"date":"2024-10-02T12:12:45.704Z","updated":"2024-10-06T09:21:10.909Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cmgu7uyem0019jkv229auelal","content":"<p>情况如下：</p>\n<p>本来hive on spark是正常的状态，增删改查都没有问题，直到一天虚拟机因为笔记本非正常关机后重启完虚拟机仍在工作，</p>\n<p><img src=\"/../../../img/image-20241005192256436.png\" alt=\"image-20241005192256436\"></p>\n<p>然后在关闭集群时，node02，03正常关闭，node01死活关闭不了（关闭虚拟机后再次打开node01还是运行状态），直接任务管理器强退相关任务和服务后才关闭，最后导致metastore数据库异常，hive执行插入操作异常，报错：</p>\n<p><code>[2024-10-05 18:28:26] [42000][30041] Error while processing statement: FAILED: Execution Error, return code 30041 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. Failed to create Spark client for Spark session d8f9be8d-27ba-464b-af51-94b5e3275c47</code></p>\n<p>解决方法:</p>\n<p><del>删除metastore重新创建并初始化元数据库:</del></p>\n<p><del><code>mysql&gt; drop database metastore;</code></del></p>\n<p><del><code>mysql&gt; create database metastore;</code></del></p>\n<p><del><code>[root@node01 conf]$ schematool -initSchema -dbType mysql -verbose</code></del></p>\n<p><del>然后重启hive和集群执行插入数据测试：</del></p>\n<p><del><img src=\"/../../../img/image-20241005193544038.png\" alt=\"image-20241005193544038\"></del></p>\n<p><del>成功</del></p>\n<p>十几分钟后再次使用又遇到同样的报错，</p>\n<p><img src=\"/../../../img/image-20241006090215878.png\" alt=\"spark-standalone\"></p>\n<p>然后发现spark用的是大学上课装的老版本，而我的&#x2F;etc&#x2F;profile配置文件里是这样的：</p>\n<p><img src=\"/../../../img/image-20241006123230093.png\" alt=\"profile\"></p>\n<p>（即使&#x2F;etc&#x2F;profile.d里配置的SPARK_HOME是正确的路径）</p>\n<p>删除老版本Spark再修改profile文件Spark_home位置后，Hive on Spark再次复活。</p>\n<p><img src=\"/../../../img/image-20241006172059609.png\" alt=\"复活证明\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>情况如下：</p>\n<p>本来hive on spark是正常的状态，增删改查都没有问题，直到一天虚拟机因为笔记本非正常关机后重启完虚拟机仍在工作，</p>\n<p><img src=\"/../../../img/image-20241005192256436.png\" alt=\"image-20241005192256436\"></p>\n<p>然后在关闭集群时，node02，03正常关闭，node01死活关闭不了（关闭虚拟机后再次打开node01还是运行状态），直接任务管理器强退相关任务和服务后才关闭，最后导致metastore数据库异常，hive执行插入操作异常，报错：</p>\n<p><code>[2024-10-05 18:28:26] [42000][30041] Error while processing statement: FAILED: Execution Error, return code 30041 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. Failed to create Spark client for Spark session d8f9be8d-27ba-464b-af51-94b5e3275c47</code></p>\n<p>解决方法:</p>\n<p><del>删除metastore重新创建并初始化元数据库:</del></p>\n<p><del><code>mysql&gt; drop database metastore;</code></del></p>\n<p><del><code>mysql&gt; create database metastore;</code></del></p>\n<p><del><code>[root@node01 conf]$ schematool -initSchema -dbType mysql -verbose</code></del></p>\n<p><del>然后重启hive和集群执行插入数据测试：</del></p>\n<p><del><img src=\"/../../../img/image-20241005193544038.png\" alt=\"image-20241005193544038\"></del></p>\n<p><del>成功</del></p>\n<p>十几分钟后再次使用又遇到同样的报错，</p>\n<p><img src=\"/../../../img/image-20241006090215878.png\" alt=\"spark-standalone\"></p>\n<p>然后发现spark用的是大学上课装的老版本，而我的&#x2F;etc&#x2F;profile配置文件里是这样的：</p>\n<p><img src=\"/../../../img/image-20241006123230093.png\" alt=\"profile\"></p>\n<p>（即使&#x2F;etc&#x2F;profile.d里配置的SPARK_HOME是正确的路径）</p>\n<p>删除老版本Spark再修改profile文件Spark_home位置后，Hive on Spark再次复活。</p>\n<p><img src=\"/../../../img/image-20241006172059609.png\" alt=\"复活证明\"></p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cmgu7uye70004jkv2a0jo4hev","category_id":"cmgu7uyea0008jkv2gxz83zml","_id":"cmgu7uyee000fjkv22zsac603"},{"post_id":"cmgu7uye80005jkv2dsbl5v1u","category_id":"cmgu7uyea0008jkv2gxz83zml","_id":"cmgu7uyef000kjkv28gn75d65"},{"post_id":"cmgu7uye90007jkv21xkw3noq","category_id":"cmgu7uyea0008jkv2gxz83zml","_id":"cmgu7uyeg000njkv2eqcz7vqc"},{"post_id":"cmgu7uyea0009jkv2b4ox8k9r","category_id":"cmgu7uyef000ijkv21h31hmsd","_id":"cmgu7uyeg000pjkv2652le92p"},{"post_id":"cmgu7uyeb000ajkv256t6d5ub","category_id":"cmgu7uyef000ljkv2hn8mb5ky","_id":"cmgu7uyeg000rjkv26hvd23mm"},{"post_id":"cmgu7uyej0011jkv221vw072a","category_id":"cmgu7uyef000ijkv21h31hmsd","_id":"cmgu7uyem0018jkv26dkqbzgl"},{"post_id":"cmgu7uyej0013jkv2fpghbq4m","category_id":"cmgu7uyef000ijkv21h31hmsd","_id":"cmgu7uyen001cjkv2b4xj8aye"},{"post_id":"cmgu7uyek0015jkv282ew4myn","category_id":"cmgu7uyef000ijkv21h31hmsd","_id":"cmgu7uyen001ejkv2btmbbkc0"},{"post_id":"cmgu7uyej0012jkv2656u6atx","category_id":"cmgu7uyel0016jkv2gl3bek43","_id":"cmgu7uyeo001hjkv29b7ueod8"},{"post_id":"cmgu7uyel0017jkv2hogldwaq","category_id":"cmgu7uyea0008jkv2gxz83zml","_id":"cmgu7uyeo001ljkv2f7l93g5d"},{"post_id":"cmgu7uyel0017jkv2hogldwaq","category_id":"cmgu7uyen001fjkv2g9hc9k0o","_id":"cmgu7uyeo001mjkv23fxhfnd7"},{"post_id":"cmgu7uyem0019jkv229auelal","category_id":"cmgu7uyea0008jkv2gxz83zml","_id":"cmgu7uyeo001njkv2c9fg2suy"},{"post_id":"cmgu7uyem0019jkv229auelal","category_id":"cmgu7uyeo001jjkv2ci2x8esp","_id":"cmgu7uyeo001ojkv20cxg0u9g"}],"PostTag":[{"post_id":"cmgu7uye70004jkv2a0jo4hev","tag_id":"cmgu7uye80006jkv2cfqrdva1","_id":"cmgu7uyef000gjkv27ocn2oqy"},{"post_id":"cmgu7uye70004jkv2a0jo4hev","tag_id":"cmgu7uyeb000bjkv26yft4xxh","_id":"cmgu7uyef000hjkv2a2q7dyvs"},{"post_id":"cmgu7uye80005jkv2dsbl5v1u","tag_id":"cmgu7uyeb000bjkv26yft4xxh","_id":"cmgu7uyeg000sjkv2glbt59pv"},{"post_id":"cmgu7uye80005jkv2dsbl5v1u","tag_id":"cmgu7uyef000jjkv24poc5kxb","_id":"cmgu7uyeg000tjkv2bu3hcuqi"},{"post_id":"cmgu7uye80005jkv2dsbl5v1u","tag_id":"cmgu7uyef000mjkv28dpz4092","_id":"cmgu7uyeh000vjkv2bm4o9meo"},{"post_id":"cmgu7uye80005jkv2dsbl5v1u","tag_id":"cmgu7uyeg000ojkv297r2buwl","_id":"cmgu7uyeh000wjkv2a59xhmve"},{"post_id":"cmgu7uye90007jkv21xkw3noq","tag_id":"cmgu7uyeg000ojkv297r2buwl","_id":"cmgu7uyeh000yjkv2akc09v07"},{"post_id":"cmgu7uyeb000ajkv256t6d5ub","tag_id":"cmgu7uyeg000ujkv26lcjaqs3","_id":"cmgu7uyeh000zjkv23z2efne7"},{"post_id":"cmgu7uyeb000ajkv256t6d5ub","tag_id":"cmgu7uyeh000xjkv2hoqmdn0m","_id":"cmgu7uyeh0010jkv26zqv6ojd"},{"post_id":"cmgu7uyej0012jkv2656u6atx","tag_id":"cmgu7uyeb000bjkv26yft4xxh","_id":"cmgu7uyen001bjkv20dj3hmo8"},{"post_id":"cmgu7uyej0012jkv2656u6atx","tag_id":"cmgu7uyek0014jkv21axr7g2j","_id":"cmgu7uyen001djkv20tumf16l"},{"post_id":"cmgu7uyel0017jkv2hogldwaq","tag_id":"cmgu7uyen001ajkv23cha0fth","_id":"cmgu7uyeo001ijkv2d1lo38v7"},{"post_id":"cmgu7uyem0019jkv229auelal","tag_id":"cmgu7uyen001gjkv2hr236toy","_id":"cmgu7uyeo001kjkv2ginl7vy4"}],"Tag":[{"name":"DataBase","_id":"cmgu7uye80006jkv2cfqrdva1"},{"name":"BigData","_id":"cmgu7uyeb000bjkv26yft4xxh"},{"name":"HDFS","_id":"cmgu7uyef000jjkv24poc5kxb"},{"name":"大数据","_id":"cmgu7uyef000mjkv28dpz4092"},{"name":"数据仓库","_id":"cmgu7uyeg000ojkv297r2buwl"},{"name":"hack","_id":"cmgu7uyeg000ujkv26lcjaqs3"},{"name":"apple music","_id":"cmgu7uyeh000xjkv2hoqmdn0m"},{"name":"电商数仓6.0","_id":"cmgu7uyek0014jkv21axr7g2j"},{"name":"DataX","_id":"cmgu7uyen001ajkv23cha0fth"},{"name":"spark","_id":"cmgu7uyen001gjkv2hr236toy"}]}}